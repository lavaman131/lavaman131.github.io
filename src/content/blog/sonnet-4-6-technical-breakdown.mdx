---
title: "Claude Sonnet 4.6: What Developers Actually Need to Know"
date: "2026-02-17"
authors: "Alex Lavaee, Norin Lavaee"
description: "Sonnet 4.6 scores within 1.2 points of Opus 4.6 on SWE-bench at one-fifth the cost. We break down the benchmarks, architecture changes, pricing math, developer reactions, and what it means for your agentic workflows."
---

import InteractiveMermaid from "../../components/blog/InteractiveMermaid.astro";
import BlogCTA from "../../components/blog/BlogCTA.astro";

## The Benchmark Reality

The numbers that matter for developers shipping code: Sonnet 4.6 nearly matches Opus 4.6 on the benchmarks enterprise teams actually use to evaluate models for agentic workflows.

| Benchmark | Sonnet 4.6 | Opus 4.6 | GPT-5.2 | Gap (Sonnet vs Opus) |
|---|---|---|---|---|
| SWE-bench Verified | 79.6% | 80.8% | Lower | 1.2 pts |
| OSWorld (Computer Use) | 72.5% | 72.7% | 38.2% | 0.2 pts |
| GDPval-AA Elo (Office Tasks) | 1633 | 1606 | — | **Sonnet wins** |
| Agentic Financial Analysis | 63.3% | 60.1% | 59.0% | **Sonnet wins** |
| ARC-AGI-2 (Novel Reasoning) | — | — | — | ~8 pt gap |
| BrowseComp (Agentic Search) | — | — | — | ~9 pt gap |
| Terminal-Bench 2.0 | — | — | — | ~6 pt gap |

The pattern: Sonnet 4.6 matches or beats Opus on production-oriented benchmarks (office tasks, financial analysis, coding) while Opus retains advantages on tasks requiring novel reasoning, complex terminal work, and deep agentic search. For most teams, the Sonnet-competitive tasks are the ones that show up in daily work.

One caveat on SWE-bench scores: Anthropic's model documentation page lists Sonnet 4.6 at 74.6% (10-trial average) and 80.2% with prompt modification. The 79.6% figure comes from the benchmark table in the announcement post, likely using conditions comparable to how Opus was evaluated. Evaluation methodology matters — always check whether scores use the same conditions before comparing across models.

---

## Architecture: Three Features That Change How You Build

Sonnet 4.6 introduces three architectural capabilities that directly affect how you structure agentic applications.

<InteractiveMermaid chart={`
flowchart LR
    subgraph "Sonnet 4.6 Architecture"
        AT["Adaptive Thinking<br/><i>4 effort levels</i><br/><i>Model decides when<br/>to reason deeply</i>"]
        CC["Context Compaction<br/><i>Auto-summarizes<br/>older context</i><br/><i>Effectively infinite<br/>conversations</i>"]
        MW["1M Token Context<br/><i>Beta</i><br/><i>Entire codebases<br/>in one request</i>"]
    end
    AT -->|"Reduces unnecessary<br/>thinking tokens"| COST["Lower Cost<br/>per Request"]
    CC -->|"Prevents context<br/>window overflow"| LONG["Long-Running<br/>Agent Sessions"]
    MW -->|"No chunking<br/>needed"| CODE["Full Codebase<br/>Reasoning"]
    classDef feature fill:#89b4fa,stroke:#74c7ec,color:#1e1e2e
    classDef benefit fill:#a6e3a1,stroke:#94e2d5,color:#1e1e2e
    class AT,CC,MW feature
    class COST,LONG,CODE benefit
`} />

### 1. Adaptive Thinking

Previously, extended thinking required setting a fixed `budget_tokens` — you had to guess how much reasoning the model needed. Adaptive thinking replaces this with a `type: "adaptive"` mode where the model decides when and how much to think, guided by an `effort` parameter.

Four effort levels:

| Level | Behavior | Use Case |
|---|---|---|
| `max` | Always thinks, no depth limit (Opus 4.6 only) | Hardest problems requiring maximum reasoning |
| `high` (default) | Always thinks, deep reasoning | Complex coding, multi-step analysis |
| `medium` | Moderate thinking, may skip for simple queries | Balanced latency and quality |
| `low` | Minimal thinking, skips for simple tasks | High-throughput, latency-sensitive |

This matters because adaptive thinking also automatically enables **interleaved thinking** — the model can reason between tool calls in agentic loops, not just at the start of a response. For multi-step coding workflows where the agent calls tools, reads results, and decides next steps, this is a meaningful improvement.

Here's how to use it with the Python SDK (from [Anthropic's docs](https://platform.claude.com/docs/en/build-with-claude/adaptive-thinking)):

```python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-6",
    max_tokens=16000,
    thinking={"type": "adaptive"},
    output_config={"effort": "medium"},
    messages=[
        {
            "role": "user",
            "content": "Refactor this module to use the repository pattern.",
        }
    ],
)

for block in response.content:
    if block.type == "thinking":
        print(f"Thinking: {block.thinking}")
    elif block.type == "text":
        print(f"Response: {block.text}")
```

Or in TypeScript:

```typescript
import Anthropic from "@anthropic-ai/sdk";

const client = new Anthropic();

const response = await client.messages.create({
  model: "claude-sonnet-4-6",
  max_tokens: 16000,
  thinking: { type: "adaptive" },
  output_config: { effort: "medium" },
  messages: [{
    role: "user",
    content: "Refactor this module to use the repository pattern."
  }]
});

for (const block of response.content) {
  if (block.type === "thinking") {
    console.log(`Thinking: ${block.thinking}`);
  } else if (block.type === "text") {
    console.log(`Response: ${block.text}`);
  }
}
```

Note: `thinking.type: "enabled"` with `budget_tokens` is deprecated on Sonnet 4.6 and Opus 4.6. It still works, but Anthropic recommends migrating to adaptive mode.

### 2. Context Compaction

For long-running agent sessions, context compaction (beta) automatically summarizes older conversation context when approaching the window limit. Instead of your agent session crashing when it hits 200K tokens, the API compresses the history and keeps going. This is particularly relevant for agentic coding workflows where tool-use loops can burn through context fast.

How it works: when input tokens exceed a configurable trigger threshold (minimum 50K tokens, default 150K), the API generates a summary of the conversation so far. On subsequent requests, all message blocks prior to the compaction summary are automatically dropped, and the conversation continues from the compressed state. You can also provide custom summarization instructions to preserve domain-specific context like variable names or architectural decisions.

See the [compaction docs](https://platform.claude.com/docs/en/build-with-claude/compaction) for API usage and code examples.

### 3. 1M Token Context Window (Beta)

Both Sonnet 4.6 and Opus 4.6 support a 1M token context window via a beta header. This is roughly 750K words — enough to fit entire codebases, lengthy contracts, or dozens of research papers in a single request.

```python
from anthropic import Anthropic

client = Anthropic()

response = client.beta.messages.create(
    model="claude-sonnet-4-6",
    max_tokens=1024,
    messages=[{"role": "user", "content": "Analyze this codebase..."}],
    betas=["context-1m-2025-08-07"],
)
```

Important constraints:
- **Tier requirement**: Available to organizations in usage tier 4 and above
- **Pricing**: Requests exceeding 200K tokens are charged at 2x input and 1.5x output rates
- **Rate limits**: Long context requests have dedicated (lower) rate limits

---

## The Pricing Math

This is where the release gets consequential for teams running parallel agent sessions at scale.

| Model | Input (per MTok) | Output (per MTok) | SWE-bench | Cost Ratio |
|---|---|---|---|---|
| **Claude Sonnet 4.6** | \$3 | \$15 | 79.6% | **1x** |
| Claude Opus 4.6 | \$15 | \$75 | 80.8% | 5x |
| GPT-5.2 | \$1.75 | \$14 | Lower | ~0.9x |
| GLM-5 (open-source) | ~\$0.15 | — | 77.8% | ~0.05x |

The comparison that matters most: an agent processing 10M tokens/day costs roughly \$45 in Sonnet 4.6 input vs \$150 in Opus 4.6 input. Over a month, that's ~\$1,350 vs ~\$4,500 per agent — and most teams run multiple agents in parallel.

GPT-5.2 is slightly cheaper on input (\$1.75 vs \$3) but trails significantly on computer use (38.2% vs 72.5% OSWorld) and agentic capabilities. GLM-5 is dramatically cheaper but lacks computer use, 1M token context, and — according to practitioners — the consistent instruction following needed for production agentic workflows.

<InteractiveMermaid chart={`
graph TD
    subgraph "SWE-bench Score per Dollar (Higher = Better)"
        S46["Sonnet 4.6<br/>79.6% @ \\$3/MTok<br/><b>26.5 pts/\\$</b>"]
        O46["Opus 4.6<br/>80.8% @ \\$15/MTok<br/><b>5.4 pts/\\$</b>"]
        GPT["GPT-5.2<br/>Lower @ \\$1.75/MTok"]
        GLM["GLM-5<br/>77.8% @ ~\\$0.15/MTok<br/><b>~519 pts/\\$</b>"]
    end
    classDef best fill:#a6e3a1,stroke:#94e2d5,color:#1e1e2e
    classDef good fill:#89b4fa,stroke:#74c7ec,color:#1e1e2e
    classDef open fill:#f9e2af,stroke:#fab387,color:#1e1e2e
    classDef other fill:#cdd6f4,stroke:#bac2de,color:#1e1e2e
    class S46 best
    class O46 good
    class GLM open
    class GPT other
`} />

---

## Computer Use: 14.9% → 72.5% in 16 Months

The progression on OSWorld-Verified tells a story about compound improvement:

| Model | Date | OSWorld Score |
|---|---|---|
| Sonnet 3.5 | Oct 2024 | 14.9% |
| Sonnet 3.7 | Feb 2025 | 28.0% |
| Sonnet 4 | Jun 2025 | 42.2% |
| Sonnet 4.5 | Oct 2025 | 61.4% |
| **Sonnet 4.6** | **Feb 2026** | **72.5%** |

That's nearly a 5x improvement in 16 months. GPT-5.2 sits at 38.2% — roughly where Sonnet 4 was eight months ago.

Why this matters for developers: computer use is the capability that unlocks automation of legacy systems that lack APIs. Insurance portals, government databases, ERP systems, hospital scheduling tools — any software with a GUI but no programmatic interface becomes automatable. Jamie Cuffe, CEO of Pace, reported Sonnet 4.6 hit **94% accuracy** on their complex insurance computer use benchmark: "It reasons through failures and self-corrects in ways we haven't seen before."

Anthropic also notes significant improvements in **prompt injection resistance** for computer use — critical when your agent browses the web and interacts with external systems where malicious actors can embed instructions.

---

## What Developers Are Actually Saying

Developer reactions have been notably specific about cost-performance tradeoffs rather than generic praise. Here's a sampling from early testers:

**Moving to Sonnet 4.6 from Opus:**

- **Caitlin Colgrove, CTO of Hex Technologies**: "We're moving the majority of our traffic to Sonnet 4.6. With adaptive thinking and high effort, we see Opus-level performance on all but our hardest analytical tasks."
- **Leo Tchourakov, Factory AI**: "We are transitioning our Sonnet traffic over to this model."
- **Brendan Falk, CEO of Hercules**: "Opus 4.6 level accuracy, instruction following, and UI, all for a meaningfully lower cost."

**Coding-specific feedback:**

- **David Loker, VP of AI at CodeRabbit**: "Punches way above its weight class for the vast majority of real-world PRs."
- **Joe Binder, VP of Product at GitHub**: "Already excelling at complex code fixes, especially when searching across large codebases is essential."
- **Ben Kus, CTO of Box**: Outperformed Sonnet 4.5 in heavy reasoning Q&A by **15 percentage points** across real enterprise documents.
- **Ryan Wiggins, Mercury Banking**: "Faster, cheaper, and more likely to nail things on the first try."

**The dissenting view:**

Boris Cherny, Claude Code's creator, still prefers Opus for all coding work. His reasoning: the bottleneck isn't token cost, it's human time spent correcting AI mistakes. When a 1.2-point SWE-bench gap translates to even slightly more errors on hard problems, the time cost of debugging outweighs the savings. This is a valid perspective for solo developers or small teams where one person is deeply reviewing every output. For teams running dozens of parallel agent sessions where aggregate throughput matters more than peak accuracy, the calculus favors Sonnet.

---

## Model Specs at a Glance

Quick reference for integrating Sonnet 4.6 into your stack:

| Spec | Sonnet 4.6 | Opus 4.6 |
|---|---|---|
| **API Model ID** | `claude-sonnet-4-6` | `claude-opus-4-6` |
| **AWS Bedrock ID** | `anthropic.claude-sonnet-4-6` | `anthropic.claude-opus-4-6-v1` |
| **GCP Vertex AI ID** | `claude-sonnet-4-6` | `claude-opus-4-6` |
| **Context Window** | 200K (standard) / 1M (beta) | 200K (standard) / 1M (beta) |
| **Max Output** | 64K tokens | 128K tokens |
| **Training Data Cutoff** | Jan 2026 | Aug 2025 |
| **Adaptive Thinking** | Yes | Yes |
| **Extended Thinking** | Yes | Yes |
| **Latency** | Fast | Moderate |

---

## Where Open-Source Stands

GLM-5 deserves mention because it represents where the open-source frontier is heading. At 77.8% SWE-bench and roughly \$0.15/MTok input (20x cheaper than Sonnet 4.6), the raw coding capability gap is narrowing fast.

But capability convergence isn't the full story. No open model currently offers:
- Computer use (GUI automation)
- 1M token context windows
- Context compaction for effectively infinite sessions
- Adaptive thinking with effort controls
- Consistent instruction following across production-scale agentic workflows

Model capability is converging. The moat is increasingly in infrastructure, tooling, and the ecosystem around the model — not the model weights themselves.

---

## Bottom Line

For teams running parallel agent sessions where cost per autonomous hour matters more than peak capability on the hardest 5% of problems, the 5x Opus premium is hard to justify. Sonnet 4.6 is the obvious default for the majority of production agentic workloads.

For solo developers or teams where one expert is closely reviewing every output and working on novel, hard problems — Opus still has an edge worth paying for, particularly on complex terminal work, novel reasoning, and deep agentic search.

The practical migration: start with `claude-sonnet-4-6` using adaptive thinking at `high` effort. If you notice quality drops on your specific workload, bump to Opus selectively for those tasks. Most teams will find they rarely need to.

---

## Key Takeaways

- **SWE-bench Verified**: 79.6% vs Opus 4.6's 80.8% — a 1.2 point gap at 5x lower cost (\$3/\$15 vs \$15/\$75 per MTok)
- **Computer use**: OSWorld went from 14.9% to 72.5% in 16 months. Sonnet 4.6 is 0.2 points behind Opus here
- **New architecture features**: Adaptive thinking (4 effort levels), context compaction (beta), and a 1M token context window (beta)
- **Developer preference**: 70% preferred over Sonnet 4.5 in Claude Code; 59% preferred over Opus 4.5
- **Open-source pressure**: GLM-5 hits 77.8% SWE-bench at ~\$0.15/MTok input — 20x cheaper, but lacks computer use and 1M context

---

**References:**

- [Anthropic Sonnet 4.6 Announcement](https://www.anthropic.com/news/claude-sonnet-4-6)
- [Anthropic Adaptive Thinking Docs](https://platform.claude.com/docs/en/build-with-claude/adaptive-thinking)
- [Anthropic Context Compaction Docs](https://platform.claude.com/docs/en/build-with-claude/compaction)
- [Anthropic 1M Context Window Docs](https://platform.claude.com/docs/en/build-with-claude/context-windows#1m-token-context-window)
- [Anthropic Model Comparison](https://platform.claude.com/docs/en/docs/about-claude/models)
- [VentureBeat: Anthropic's Sonnet 4.6 matches flagship AI performance at one-fifth the cost](https://venturebeat.com/technology/anthropics-sonnet-4-6-matches-flagship-ai-performance-at-one-fifth-the-cost)
