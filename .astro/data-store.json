[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.7.10","content-config-digest","45eedef4b1b80dfd","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"http://localhost:3000\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":3000,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[]},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":\"prism\",\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[null],\"rehypePlugins\":[null],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false},\"legacy\":{\"collections\":false}}","projects",["Map",11,12,24,25,36,37,48,49,60,61,72,73,84,85,96,97,108,109,120,121,132,133,145,146],"cartoonify",{"id":11,"data":13,"body":17,"filePath":18,"assetImports":19,"digest":21,"legacyId":22,"deferredRender":23},{"title":14,"cover":15,"githubUrl":16},"Cartoonify: Cartoon Style Transfer + Generation","__ASTRO_IMAGE_../../assets/images/projects/cartoonify/thumbnail.png","https://github.com/lavaman131/cartoonify","import { Image } from \"astro:assets\";\nimport loss from \"../../assets/images/projects/cartoonify/report/loss.png\";\nimport eval1 from \"../../assets/images/projects/cartoonify/eval/1.png\";\nimport eval2 from \"../../assets/images/projects/cartoonify/eval/2.png\";\nimport eval3 from \"../../assets/images/projects/cartoonify/eval/3.png\";\nimport eval4 from \"../../assets/images/projects/cartoonify/eval/4.png\";\nimport eval5 from \"../../assets/images/projects/cartoonify/eval/5.png\";\nimport eval6 from \"../../assets/images/projects/cartoonify/eval/6.png\";\nimport eval7 from \"../../assets/images/projects/cartoonify/eval/7.png\";\nimport eval8 from \"../../assets/images/projects/cartoonify/eval/8.png\";\nimport dataCollection from \"../../assets/images/projects/cartoonify/report/data_collection.png\";\nimport results from \"../../assets/images/projects/cartoonify/report/results.png\";\n\n## Introduction\n\nThe main objective was to apply style transfer in the context of diffusion models to generate more convincing “Disney”-style images from source images of people. To achieve this, we needed a deep understanding of the diffusion model’s architecture. The particular diffusion model employed in our project integrates a Variational Autoencoder (VAE), a U-Net architecture, and the CLIP (Contrastive Language-Image Pretraining) model.\n\n## Methodology\n\nWe used the latent diffusion model: stable-diffusion-v1.5. First, we collected the images of modern Disney characters, animals/objects, and landscapes. We followed DreamBooth, a training technique that updates the entire stable diffusion model with a few curated images. Since we chose to train the model with DreamBooth technique, we did not need a large dataset. In total, we collected around 200 images from Youtube, Google, and Pinterest. They were resized to 512 x 512. For higher quality, the background of images had to be white or black.\n\n\u003CImage src={dataCollection} alt=\"dataCollection\" />\n\nAfter collecting the data, we tried to train the model and text encoder with prior preservation. Prior preservation uses additional images of the same class we are trying to train as part of the fine-tuning process (regularization). Before training the model, we set approximately 40 parameters in advance. The specifics of the training parameters are as follows:\n\n* Pre-trained Model: We utilized the 'stable-diffusion-v1-5' model from RunwayML as our starting point, with the 'main' revision indicating the use of the latest model variant.\n* Variation and Precision: The model variant 'fp16' was selected for training, which uses 16-bit floating-point precision. This choice was instrumental in optimizing the training speed and memory usage without significantly compromising the model's performance.\n* Validation Parameters: The validation prompt was set to \"a person, Disney style\" to guide the model in generating images that match this theme. Four images were used for validation purposes to ensure that the style was being captured accurately.\n* Training Data: The training dataset located in the '/data/disney' directory was complemented with a class-specific dataset from '/data/style_ddim', intended to imbue the model with a deeper understanding of the distinctive style features.\n* Image Configuration: A total of 1,000 class images were provided to the model, with a prior loss weight of 1.0 to ensure that the original style attributes were preserved. The resolution was fixed at 512 pixels, and center cropping was employed to maintain focus on the central elements of the images.\n* Training Procedure: To encode the text prompts effectively, the model's text encoder was included in the training loop. The training was conducted in batches of size four over a single epoch, capped at a maximum of 4,000 steps to prevent overfitting.\n* Optimizer: We implemented an 8-bit Adam optimizer to conserve memory while maintaining a learning rate of 1e-6, constant throughout the training. Beta coefficients for the Adam optimizer were set to 0.9 and 0.999, with a weight decay of 1e-2 and epsilon value of 1e-8 for numerical stability.\n* Regularization and Gradient Control: The maximum norm for gradients was limited to 1.0 during backpropagation to prevent exploding gradients.\n* Tracking and Precision: Model checkpoints were created every 500 steps, and training logs were recorded in the 'logs' directory. Mixed precision training was enforced with 'fp16' precision for both the model's prior generation and validation steps to align with the selected training variant.\n* Schedulers: A 'PNDMScheduler' was used for validation, ensuring that learning rates were adjusted appropriately at each step of the validation process.\n* Reproducibility: A random seed value of 42 was used to initialize the training process, which ensures reproducibility of results across different training sessions.\n\nEach of these parameters was chosen with careful consideration of both the computational efficiency and the specific stylistic goals of the project. The train script can be found [here](https://github.com/lavaman131/cartoonify/blob/main/tools/train_dreambooth.sh).\n\n## Results\n\nSince the outputs of the model are images, valid quantitative evaluations are ambiguous. For quantitative evaluations, we decided to rate the images from each checkpoint and see if there has been improvement of image generation quality. The ratings range from 1 (poor quality) to 10 (great quality). Note, in the third set of images, an entirely black image is triggered by the Stable Diffusion image safety checker model. Therefore, we did not evaluate the particular image.\n\n\u003CImage src={eval1} alt=\"eval1\" />\n\u003CImage src={eval2} alt=\"eval2\" />\n\u003CImage src={eval3} alt=\"eval3\" />\n\u003CImage src={eval4} alt=\"eval4\" />\n\u003CImage src={eval5} alt=\"eval5\" />\n\u003CImage src={eval6} alt=\"eval6\" />\n\u003CImage src={eval7} alt=\"eval7\" />\n\u003CImage src={eval8} alt=\"eval8\" />\n\nThroughout the training process, we evaluated the generated images at intervals of 500 steps to assess their quality. The average scores at these intervals were calculated to track the progression of image quality. At 500 steps, the initial images received an average rating of 3. Subsequent assessments showed improvement, with average ratings rising to 3.5, then peaking at 8.625 after 3500 steps, which marked the highest image quality during our experiments. Notably, after 3500 steps, a slight regression was observed, culminating in an average rating of 5.375 by the end of 4000 steps. We also include hand-picked images from the final checkpoint in the [Supplementary Material](#supplementary-material) section.\n\nThese observations suggest that while there was an improvement in the quality of the generated images throughout most of the training, a decrease in quality was noted towards the end. This trend underscores the complex relationship between training steps and image quality, indicating a possible overfitting or a need for adjustment in the training parameters beyond the 3500-step mark.\n\nAnother metric that we check is the cross entropy loss during the training process. As you can see below, the loss fluctuates quite a bit which may be expected for such a small dataset on the order of hundreds of samples.\n\n\u003CImage src={loss} alt=\"loss\" />\n\n## Conclusion\n\nThe exploration of DreamBooth's training capabilities revealed a number of critical insights, particularly relating to its sensitivity to hyperparameters and the potential for overfitting. It was discovered that a low learning rate was paramount for stable convergence, while the training duration needed to be proportional to the size of the dataset to avoid overfitting while still capturing sufficient details. The incorporation of prior preservation, especially when training on nuanced subjects like faces, proved to be crucial for maintaining the integrity of generated images. The refinement of the text encoder emerged as a significant factor that influenced the final output quality. This fine-tuning, when carried out with the adjustments to the U-Net model, led to substantial improvements in the fidelity of the generated images. Dedicated prompt engineering was another critical element, substantially impacting the relevance and specificity of the generated images. Careful crafting of prompts ensured that the model's outputs more accurately reflected the desired themes and styles. Furthermore, experimentation with various noise schedulers like Euler A highlighted the importance of these components in the image synthesis process. In our specific case, the Euler A noise scheduler was particularly effective, enhancing the model's ability to produce high-quality images.\n\nIn conclusion, the process of training DreamBooth requires a delicate balance between a multitude of parameters and training nuances. Our findings underscore the importance of a methodical approach to training parameter configuration, which  can lead to remarkable results that push the boundaries of generative models when executed with precision.\n\n## References\n\n* \"Training Stable Diffusion with Dreambooth using 🧨 Diffusers.\" https://huggingface.co/blog/dreambooth \n* Mishira, Onkar. \"Stable Diffusion Explained.\" https://medium.com/@onkarmishra/stable-diffusion-explained-1f101284484d \n* Ruiz, Nataniel et al. \"DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation.\" https://arxiv.org/pdf/2208.12242\n\n## Supplementary Material\n\n\u003CImage src={results} alt=\"results\" />\n\n## 👥 Collaborators\n\n* Yewon Lee (Boston University)","src/content/projects/cartoonify.mdx",[20],"../../assets/images/projects/cartoonify/thumbnail.png","9a99aeeed29fbadc","cartoonify.mdx",true,"ev-range-pred",{"id":24,"data":26,"body":30,"filePath":31,"assetImports":32,"digest":34,"legacyId":35,"deferredRender":23},{"title":27,"cover":28,"githubUrl":29},"Electric Vehicle Range Prediction","__ASTRO_IMAGE_../../assets/images/projects/ev-range-pred/thumbnail.gif","https://github.com/lavaman131/EV_range_prediction/tree/main","import { Image } from \"astro:assets\";\nimport outliers from \"../../assets/images/projects/ev-range-pred/report/outliers.png\";\nimport outliersResidual from \"../../assets/images/projects/ev-range-pred/report/outliers_residual.png\";\nimport quantile from \"../../assets/images/projects/ev-range-pred/report/quantile.png\";\nimport transforms from \"../../assets/images/projects/ev-range-pred/report/transforms.png\";\nimport corr from \"../../assets/images/projects/ev-range-pred/report/corr.png\";\n\n## Introduction\n\nIn recent years, more and more people are buying Electrical Vehicles\n(EVs) for environmental, aesthetic, and financial reasons. The number of\ncar companies inventing EVs for their brand are increasing. Companies\nsuch as Tesla, Ford, and Rivian are taking advantage of this move toward\nEVs. The goal of our project is to examine just how much the range of\nEVs changes due to other factors such as battery pack. In this paper we\nanalyze the relation between the range and other variables such as\nacceleration, top speed, battery pack, efficiency, fast charge and price\nin order to give us a deeper understanding of what element influences\nthe range of EVs the most. Additionally, we can also see what car\nmanufacturers should do to improve their EVs, and provide useful data\nfor car companies.\n\n\n## Background\n\n### Dataset\n\nDivyanshu Gupta, Kaggle (2021), Cars Dataset with Battery Capacity Data File. Retrieved from [Kaggle](https://www.kaggle.com/datasets/divyanshugupta95/cars-dataset-with-battery-pack-capacity).\n\n### Data Collection Method\n\nThe data was collected from different companies such as Tesla, Porsche,\nBMW. The data set also gives us the specific make and model of the cars.\nThe dataset contains 14 explanatory variables with 1 response variable\nand a total of 102 data points.\n\n\n## Preliminary Analysis\n\n### Hypothesized Variables That Impact Electric Vehicle Range\n\nThe main covariate that we believe will have the largest impact on the\nrange of an electric vehicle is battery packs in kilowatts per hour. The\nother variables that we are looking at would be acceleration, top speed,\nefficiency, how fast the car charges, and price. Qualitative variables\nwould be the plug style, number of seats, power train (all wheel drive\nvs. four wheel drive), and type of car. This mix of qualitative and\nquantitative variables will allow us to give a clearer understanding of\nhow different factors affect the range of EVs.\n\n\n\u003Cdiv className=\"table-auto\" style={{ overflowX: 'auto' }}>\n  \u003Ctable style={{ minWidth: '100%', fontSize: '0.9em' }}>\n    \u003Cthead>\n      \u003Ctr>\n        \u003Cth>\u003C/th>\n        \u003Cth>Brand\u003C/th>\n        \u003Cth>Model\u003C/th>\n        \u003Cth>AccelSec\u003C/th>\n        \u003Cth>TopSpeed_KmH\u003C/th>\n        \u003Cth>Range_Km\u003C/th>\n        \u003Cth>Battery_Pack Kwh\u003C/th>\n        \u003Cth>Efficiency_WhKm\u003C/th>\n        \u003Cth>FastCharge_KmH\u003C/th>\n        \u003Cth>RapidCharge\u003C/th>\n        \u003Cth>PowerTrain\u003C/th>\n        \u003Cth>PlugType\u003C/th>\n        \u003Cth>BodyStyle\u003C/th>\n        \u003Cth>Segment\u003C/th>\n        \u003Cth>Seats\u003C/th>\n        \u003Cth>PriceEuro\u003C/th>\n      \u003C/tr>\n    \u003C/thead>\n    \u003Ctbody>\n      \u003Ctr>\n        \u003Cth>0\u003C/th>\n        \u003Ctd>Tesla\u003C/td>\n        \u003Ctd>Model 3 Long Range Dual Motor\u003C/td>\n        \u003Ctd>4.6\u003C/td>\n        \u003Ctd>233\u003C/td>\n        \u003Ctd>460\u003C/td>\n        \u003Ctd>70.0\u003C/td>\n        \u003Ctd>161\u003C/td>\n        \u003Ctd>940\u003C/td>\n        \u003Ctd>Yes\u003C/td>\n        \u003Ctd>AWD\u003C/td>\n        \u003Ctd>Type 2 CCS\u003C/td>\n        \u003Ctd>Sedan\u003C/td>\n        \u003Ctd>D\u003C/td>\n        \u003Ctd>5\u003C/td>\n        \u003Ctd>55480\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Cth>1\u003C/th>\n        \u003Ctd>Volkswagen\u003C/td>\n        \u003Ctd>ID.3 Pure\u003C/td>\n        \u003Ctd>10.0\u003C/td>\n        \u003Ctd>160\u003C/td>\n        \u003Ctd>270\u003C/td>\n        \u003Ctd>45.0\u003C/td>\n        \u003Ctd>167\u003C/td>\n        \u003Ctd>250\u003C/td>\n        \u003Ctd>Yes\u003C/td>\n        \u003Ctd>RWD\u003C/td>\n        \u003Ctd>Type 2 CCS\u003C/td>\n        \u003Ctd>Hatchback\u003C/td>\n        \u003Ctd>C\u003C/td>\n        \u003Ctd>5\u003C/td>\n        \u003Ctd>30000\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Cth>2\u003C/th>\n        \u003Ctd>Polestar\u003C/td>\n        \u003Ctd>2\u003C/td>\n        \u003Ctd>4.7\u003C/td>\n        \u003Ctd>210\u003C/td>\n        \u003Ctd>400\u003C/td>\n        \u003Ctd>75.0\u003C/td>\n        \u003Ctd>181\u003C/td>\n        \u003Ctd>620\u003C/td>\n        \u003Ctd>Yes\u003C/td>\n        \u003Ctd>AWD\u003C/td>\n        \u003Ctd>Type 2 CCS\u003C/td>\n        \u003Ctd>Liftback\u003C/td>\n        \u003Ctd>D\u003C/td>\n        \u003Ctd>5\u003C/td>\n        \u003Ctd>56440\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Cth>3\u003C/th>\n        \u003Ctd>BMW\u003C/td>\n        \u003Ctd>iX3\u003C/td>\n        \u003Ctd>6.8\u003C/td>\n        \u003Ctd>180\u003C/td>\n        \u003Ctd>360\u003C/td>\n        \u003Ctd>74.0\u003C/td>\n        \u003Ctd>206\u003C/td>\n        \u003Ctd>560\u003C/td>\n        \u003Ctd>Yes\u003C/td>\n        \u003Ctd>RWD\u003C/td>\n        \u003Ctd>Type 2 CCS\u003C/td>\n        \u003Ctd>SUV\u003C/td>\n        \u003Ctd>D\u003C/td>\n        \u003Ctd>5\u003C/td>\n        \u003Ctd>68040\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Cth>4\u003C/th>\n        \u003Ctd>Honda\u003C/td>\n        \u003Ctd>e\u003C/td>\n        \u003Ctd>9.5\u003C/td>\n        \u003Ctd>145\u003C/td>\n        \u003Ctd>170\u003C/td>\n        \u003Ctd>28.5\u003C/td>\n        \u003Ctd>168\u003C/td>\n        \u003Ctd>190\u003C/td>\n        \u003Ctd>Yes\u003C/td>\n        \u003Ctd>RWD\u003C/td>\n        \u003Ctd>Type 2 CCS\u003C/td>\n        \u003Ctd>Hatchback\u003C/td>\n        \u003Ctd>B\u003C/td>\n        \u003Ctd>4\u003C/td>\n        \u003Ctd>32997\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Cth>...\u003C/th>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Cth>97\u003C/th>\n        \u003Ctd>Nissan\u003C/td>\n        \u003Ctd>Ariya 63kWh\u003C/td>\n        \u003Ctd>7.5\u003C/td>\n        \u003Ctd>160\u003C/td>\n        \u003Ctd>330\u003C/td>\n        \u003Ctd>63.0\u003C/td>\n        \u003Ctd>191\u003C/td>\n        \u003Ctd>440\u003C/td>\n        \u003Ctd>Yes\u003C/td>\n        \u003Ctd>FWD\u003C/td>\n        \u003Ctd>Type 2 CCS\u003C/td>\n        \u003Ctd>Hatchback\u003C/td>\n        \u003Ctd>C\u003C/td>\n        \u003Ctd>5\u003C/td>\n        \u003Ctd>45000\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Cth>98\u003C/th>\n        \u003Ctd>Audi\u003C/td>\n        \u003Ctd>e-tron S Sportback 55 quattro\u003C/td>\n        \u003Ctd>4.5\u003C/td>\n        \u003Ctd>210\u003C/td>\n        \u003Ctd>335\u003C/td>\n        \u003Ctd>86.5\u003C/td>\n        \u003Ctd>258\u003C/td>\n        \u003Ctd>540\u003C/td>\n        \u003Ctd>Yes\u003C/td>\n        \u003Ctd>AWD\u003C/td>\n        \u003Ctd>Type 2 CCS\u003C/td>\n        \u003Ctd>SUV\u003C/td>\n        \u003Ctd>E\u003C/td>\n        \u003Ctd>5\u003C/td>\n        \u003Ctd>96050\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Cth>99\u003C/th>\n        \u003Ctd>Nissan\u003C/td>\n        \u003Ctd>Ariya e-4ORCE 63kWh\u003C/td>\n        \u003Ctd>5.9\u003C/td>\n        \u003Ctd>200\u003C/td>\n        \u003Ctd>325\u003C/td>\n        \u003Ctd>63.0\u003C/td>\n        \u003Ctd>194\u003C/td>\n        \u003Ctd>440\u003C/td>\n        \u003Ctd>Yes\u003C/td>\n        \u003Ctd>AWD\u003C/td>\n        \u003Ctd>Type 2 CCS\u003C/td>\n        \u003Ctd>Hatchback\u003C/td>\n        \u003Ctd>C\u003C/td>\n        \u003Ctd>5\u003C/td>\n        \u003Ctd>50000\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Cth>100\u003C/th>\n        \u003Ctd>Nissan\u003C/td>\n        \u003Ctd>Ariya e-4ORCE 87kWh Performance\u003C/td>\n        \u003Ctd>5.1\u003C/td>\n        \u003Ctd>200\u003C/td>\n        \u003Ctd>375\u003C/td>\n        \u003Ctd>87.0\u003C/td>\n        \u003Ctd>232\u003C/td>\n        \u003Ctd>450\u003C/td>\n        \u003Ctd>Yes\u003C/td>\n        \u003Ctd>AWD\u003C/td>\n        \u003Ctd>Type 2 CCS\u003C/td>\n        \u003Ctd>Hatchback\u003C/td>\n        \u003Ctd>C\u003C/td>\n        \u003Ctd>5\u003C/td>\n        \u003Ctd>65000\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Cth>101\u003C/th>\n        \u003Ctd>Byton\u003C/td>\n        \u003Ctd>M-Byte 95 kWh 2WD\u003C/td>\n        \u003Ctd>7.5\u003C/td>\n        \u003Ctd>190\u003C/td>\n        \u003Ctd>400\u003C/td>\n        \u003Ctd>95.0\u003C/td>\n        \u003Ctd>238\u003C/td>\n        \u003Ctd>480\u003C/td>\n        \u003Ctd>Yes\u003C/td>\n        \u003Ctd>AWD\u003C/td>\n        \u003Ctd>Type 2 CCS\u003C/td>\n        \u003Ctd>SUV\u003C/td>\n        \u003Ctd>E\u003C/td>\n        \u003Ctd>5\u003C/td>\n        \u003Ctd>62000\u003C/td>\n      \u003C/tr>\n    \u003C/tbody>\n  \u003C/table>\n\u003C/div>\n\n\n### Exploring Influence of Outliers\n\nWe tested different outlier removal methods such as taking away data\nthat was two and three standard deviations away from mean. Figure 1\ncompares the linear relationship of covariates against Range_Km with and\nwithout outliers respectively:\n\n\u003CImage src={outliers} alt=\"outliers\" />\n\nAfter analyzing the best-fit lines with and without outliers we went on\nto analyze the corresponding residual plots. Figure 2 compares the\nresidual plots of covariates against Range_Km with and without outliers\nrespectively:\n\n\u003CImage src={outliersResidual} alt=\"outliersResidual\" />\n\nWe found a minimal difference in the strength of the linear relationship\n(measured with $R^2$) between the covariates and response variable when\nfiltering outliers. Additionally, our dataset is relatively small with\n102 samples. For these reasons, we decided not to remove outliers for\nfurther stages of our analysis.\n\n\u003Cfigure>\n  \u003Ctable className=\"table-auto\">\n    \u003Cthead>\n      \u003Ctr>\n        \u003Cth>Covariate\u003C/th>\n        \u003Cth>R² Before\u003C/th>\n        \u003Cth>R² After\u003C/th>\n      \u003C/tr>\n    \u003C/thead>\n    \u003Ctbody>\n      \u003Ctr>\n        \u003Ctd>AccelSec\u003C/td>\n        \u003Ctd>0.46\u003C/td>\n        \u003Ctd>0.522\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>TopSpeed_KmH\u003C/td>\n        \u003Ctd>0.56\u003C/td>\n        \u003Ctd>0.463\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>Battery_Pack Kwh\u003C/td>\n        \u003Ctd>0.829\u003C/td>\n        \u003Ctd>0.753\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>Efficiency_WhKm\u003C/td>\n        \u003Ctd>0.098\u003C/td>\n        \u003Ctd>0.068\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>FastCharge_KmH\u003C/td>\n        \u003Ctd>0.569\u003C/td>\n        \u003Ctd>0.6\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>PriceEuro\u003C/td>\n        \u003Ctd>0.458\u003C/td>\n        \u003Ctd>0.411\u003C/td>\n      \u003C/tr>\n    \u003C/tbody>\n  \u003C/table>\n  \u003Cfigcaption className=\"text-center mt-2 text-sm italic\">\n    Table 1\n  \u003C/figcaption>\n  \u003C/figure>\n\n### Normal Quantile Plot\n\nIn order to verify one of the central assumptions of linear regression,\nnormally distributed residuals, we created a Q-Q plot in Figure 3 for\nall of our covariates to observe that this assumption holds true for\nmost of our covariates.\n\n\u003CImage src={quantile} alt=\"normalQuantile\" />\n\n## Transformations\n\nAnalyzing most of the residuals we can see that most of them are random,\nnormally distributed, independent, and show homoscedasticity. However,\nthese conditions are not met for the covariates Battery pack and Price\nin Euro. For this reason, we decided that it is important to transform\nBattery pack and Price in Euro.\n\nWe tested two conventional transformations. First, we applied square\nroot transformations on Battery pack and Price in Euro. Next, we applied\nlog transformations to the two covariates. We determined that the log\ntransformations led to a stronger linear relationship ($R^2$) so our\nfinal transformation was a log transform on Battery Pack and Price.\nFigure 4 shows the improved residual plots after the log transformation:\n\n\u003CImage src={transforms} alt=\"transforms\" />\n\n\u003Cfigure>\n  \u003Ctable className=\"table-auto\">\n    \u003Cthead>\n      \u003Ctr>\n        \u003Cth>Covariate\u003C/th>\n        \u003Cth>R² Before\u003C/th>\n        \u003Cth>R² After\u003C/th>\n      \u003C/tr>\n    \u003C/thead>\n    \u003Ctbody>\n      \u003Ctr>\n        \u003Ctd>AccelSec\u003C/td>\n        \u003Ctd>0.46\u003C/td>\n        \u003Ctd>0.539\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>TopSpeed_KmH\u003C/td>\n        \u003Ctd>0.56\u003C/td>\n        \u003Ctd>0.552\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>Battery_Pack Kwh\u003C/td>\n        \u003Ctd>0.829\u003C/td>\n        \u003Ctd>0.768\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>Efficiency_WhKm\u003C/td>\n        \u003Ctd>0.098\u003C/td>\n        \u003Ctd>0.087\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>FastCharge_KmH\u003C/td>\n        \u003Ctd>0.569\u003C/td>\n        \u003Ctd>0.187\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>PriceEuro\u003C/td>\n        \u003Ctd>0.458\u003C/td>\n        \u003Ctd>0.529\u003C/td>\n      \u003C/tr>\n    \u003C/tbody>\n  \u003C/table>\n  \u003Cfigcaption className=\"text-center mt-2 text-sm italic\">\n    Table 2\n  \u003C/figcaption>\n\u003C/figure>\n\n## Main Results\n\n### Multicollinearity\n\nWe created a correlation coefficient matrix (Figure 5) in order to test\nfor multicollinearity within our covariates. Range was highly correlated\nwith all of our covariates which is a good sign that our model is\npredictive of range. We noticed that acceleration has a negative\ncorrelation with most of the other variables including range. The\nefficiency coefficient had the lowest correlation coefficient with range\nat 0.48. There is high correlation between price and our other\ncovariates, and we believe there is evidence of multicollinearity\nbecause of the high correlation coefficient. Removing price may reduce\nthe multicollinearity issues we encounter. It may be best to use a\nstepwise forward regression, to allow the model to remove covariates\nthat are causing multicollinearity.\n\n\u003CImage src={corr} alt=\"correlation\" />\n\nWith the covariates that remained after feature selection, we measured multicollinearity using the variance inflation factor (VIF) given by:\n\n$$\nVIF_i = \\frac{1}{1 - {R_i}^2}\n$$\n\nThe result (Table 3) shows that all of the selected covariates are below 5, which means that they will not have multicollinearity. The variable TopSpeed is less significant than the other covariates, but because it is significant at the 5% level and has a VIF level below 10, we believe it should still be included for analysis.\n\n\u003Cfigure>\n  \u003Ctable className=\"table-auto\">\n    \u003Cthead>\n      \u003Ctr>\n      \u003Cth>Term\u003C/th>\n      \u003Cth>Estimate\u003C/th>\n      \u003Cth>Std Error\u003C/th>\n      \u003Cth>t Ratio\u003C/th>\n      \u003Cth>Prob > |t|\u003C/th>\n      \u003Cth>VIF\u003C/th>\n    \u003C/tr>\n  \u003C/thead>\n  \u003Ctbody>\n    \u003Ctr>\n      \u003Ctd>Intercept\u003C/td>\n      \u003Ctd>-511.9363\u003C/td>\n      \u003Ctd>34.95776\u003C/td>\n      \u003Ctd>-14.64\u003C/td>\n      \u003Ctd>&lt; .0001*\u003C/td>\n      \u003Ctd>.\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>TopSpeed_KmH\u003C/td>\n      \u003Ctd>0.4494041\u003C/td>\n      \u003Ctd>0.175015\u003C/td>\n      \u003Ctd>2.57\u003C/td>\n      \u003Ctd>0.0120*\u003C/td>\n      \u003Ctd>1.9033032\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>Log(Battery_Pack KWh)\u003C/td>\n      \u003Ctd>235.09752\u003C/td>\n      \u003Ctd>14.19828\u003C/td>\n      \u003Ctd>16.56\u003C/td>\n      \u003Ctd>&lt; .0001*\u003C/td>\n      \u003Ctd>2.7759986\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>Efficiency_WhKm\u003C/td>\n      \u003Ctd>-1.013424\u003C/td>\n      \u003Ctd>0.184575\u003C/td>\n      \u003Ctd>-5.49\u003C/td>\n      \u003Ctd>&lt; .0001*\u003C/td>\n      \u003Ctd>1.8698509\u003C/td>\n    \u003C/tr>\n    \u003C/tbody>\n  \u003C/table>\n  \u003Cfigcaption className=\"text-center mt-2 text-sm italic\">\n    Table 3\n  \u003C/figcaption>\n\u003C/figure>\n\n### Variable Selection\n\n\n\u003Cfigure>\n  \u003Ctable className=\"table-auto\">\n    \u003Cthead>\n      \u003Ctr>\n        \u003Cth>Method\u003C/th>\n        \u003Cth>Prob to enter/leave\u003C/th>\n        \u003Cth>Equation\u003C/th>\n        \u003Cth>Adjusted R²\u003C/th>\n      \u003C/tr>\n    \u003C/thead>\n    \u003Ctbody>\n      \u003Ctr>\n        \u003Ctd>Forward Selection\u003C/td>\n        \u003Ctd>PE: 0.25\u003Cbr/>PL: 0.1\u003C/td>\n        \u003Ctd>$\\hat{y} = -909.39 + 0.29x_1 + 39.04x_2 - 2.08x_3 + 288.90x_4 - 28.80x_5 + 8.22x_6 - 45.13x_7 - 52.62x_8 + 70.71x_9 + 43.78x_{10}$\u003C/td>\n        \u003Ctd>0.941\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>Backward Elimination\u003C/td>\n        \u003Ctd>PE: 0.25\u003Cbr/>PL: 0.1\u003C/td>\n        \u003Ctd>$\\hat{y} = -909.39 + 0.29x_1 + 39.04x_2 - 2.08x_3 + 288.90x_4 - 28.80x_5 + 8.22x_6 - 45.13x_7 - 52.62x_8 + 70.71x_9 + 43.78x_{10}$\u003C/td>\n        \u003Ctd>0.941\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>Mixed\u003C/td>\n        \u003Ctd>PE: 0.25\u003Cbr/>PL:0.25\u003C/td>\n        \u003Ctd>$\\hat{y} = -909.39 + 0.29x_1 + 39.04x_2 - 2.08x_3 + 288.90x_4 - 28.80x_5 + 8.22x_6 - 45.13x_7 - 52.62x_8 + 70.71x_9 + 43.78x_{10}$\u003C/td>\n        \u003Ctd>0.941\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>AICC\u003C/td>\n        \u003Ctd>N/A\u003C/td>\n        \u003Ctd>$\\hat{y} = -909.39 + 0.29x_1 + 39.04x_2 - 2.08x_3 + 288.90x_4 - 28.80x_5 + 8.22x_6 - 45.13x_7 - 52.62x_8 + 70.71x_9 + 43.78x_{10}$\u003C/td>\n        \u003Ctd>0.941\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>BIC\u003C/td>\n        \u003Ctd>N/A\u003C/td>\n        \u003Ctd>$\\hat{y} = -909.39 + 0.29x_1 + 39.04x_2 - 2.08x_3 + 288.90x_4 - 28.80x_5 + 8.22x_6 - 45.13x_7 - 52.62x_8 + 70.71x_9 + 43.78x_{10}$\u003C/td>\n        \u003Ctd>0.941\u003C/td>\n      \u003C/tr>\n    \u003C/tbody>\n  \u003C/table>\n  \u003Cfigcaption className=\"text-center mt-2 text-sm italic\">\n    Table 4\n  \u003C/figcaption>\n\u003C/figure>\n\n\n### Model Fit\n\nThe transformations that we included in our model included the log of\nprice and log of battery pack. We achieved a perfect ${R_a}^2$ value and\nrealized that the car Model and brand covariate had a high cardinality\n(factor levels were equal to the number of data points in this case),\nhence, causing overfitting (over-generalization) of the fitted model.\nFor this reason, we removed the a Model and brand covariate and were\nleft with a much more applicable adjusted ${R_a}^2$ and low RMSE that\npredicts range.\n\n\u003Cfigure>\n  \u003Ctable className=\"table-auto\">\n    \u003Cthead>\n      \u003Ctr>\n        \u003Cth colSpan=\"2\">Summary of Fit\u003C/th>\n      \u003C/tr>\n    \u003C/thead>\n    \u003Ctbody>\n      \u003Ctr>\n        \u003Ctd>RSquare\u003C/td>\n        \u003Ctd>0.954078\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>RSquare Adj\u003C/td>\n        \u003Ctd>0.949031\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>Root Mean Square Error\u003C/td>\n        \u003Ctd>28.60422\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>Mean of Response\u003C/td>\n        \u003Ctd>338.6275\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>Observations (or Sum Wgts)\u003C/td>\n        \u003Ctd>102\u003C/td>\n      \u003C/tr>\n    \u003C/tbody>\n  \u003C/table>\n  \u003Cfigcaption className=\"text-center mt-2 text-sm italic\">\n    Table 5\n  \u003C/figcaption>\n\u003C/figure>\n\n\u003Cfigure>\n  \u003Ctable className=\"table-auto\">\n  \u003Cthead>\n    \u003Ctr>\n      \u003Cth>Source\u003C/th>\n      \u003Cth>DF\u003C/th>\n      \u003Cth>Sum of Squares\u003C/th>\n      \u003Cth>Mean Square\u003C/th>\n      \u003Cth>F Ratio\u003C/th>\n    \u003C/tr>\n  \u003C/thead>\n  \u003Ctbody>\n    \u003Ctr>\n      \u003Ctd>Model\u003C/td>\n      \u003Ctd>10\u003C/td>\n      \u003Ctd>1546901.5\u003C/td>\n      \u003Ctd>154690\u003C/td>\n      \u003Ctd>189.0612\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>Error\u003C/td>\n      \u003Ctd>91\u003C/td>\n      \u003Ctd>74456.3\u003C/td>\n      \u003Ctd>818\u003C/td>\n      \u003Ctd>Prob > F\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>C. Total\u003C/td>\n      \u003Ctd>101\u003C/td>\n      \u003Ctd>1621357.8\u003C/td>\n      \u003Ctd>\u003C/td>\n      \u003Ctd>&lt;.0001*\u003C/td>\n    \u003C/tr>\n    \u003C/tbody>\n  \u003C/table>\n  \u003Cfigcaption className=\"text-center mt-2 text-sm italic\">\n    Table 6\n  \u003C/figcaption>\n\u003C/figure>\n\n\u003Cfigure>\n  \u003Ctable className=\"table-auto\">\n    \u003Ctr>\n      \u003Cth>Term\u003C/th>\n      \u003Cth>Estimate\u003C/th>\n      \u003Cth>Std Error\u003C/th>\n      \u003Cth>t Ratio\u003C/th>\n      \u003Cth>Prob &gt; |t|\u003C/th>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Cth>Intercept\u003C/th>\n      \u003Ctd>-909.3919\u003C/td>\n      \u003Ctd>117.3835\u003C/td>\n      \u003Ctd>-7.75\u003C/td>\n      \u003Ctd>{'\u003C'} .0001\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Cth>TopSpeed_KmH\u003C/th>\n      \u003Ctd>0.2942874\u003C/td>\n      \u003Ctd>0.140142\u003C/td>\n      \u003Ctd>2.10\u003C/td>\n      \u003Ctd>0.0385\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Cth>RapidCharge[No]\u003C/th>\n      \u003Ctd>39.042448\u003C/td>\n      \u003Ctd>8.552891\u003C/td>\n      \u003Ctd>4.56\u003C/td>\n      \u003Ctd>{'\u003C'} .0001\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Cth>Efficiency_WhKm\u003C/th>\n      \u003Ctd>-2.078849\u003C/td>\n      \u003Ctd>0.161668\u003C/td>\n      \u003Ctd>-12.86\u003C/td>\n      \u003Ctd>{'\u003C'} .0001\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Cth>Log(Battery_Pack Kwh)\u003C/th>\n      \u003Ctd>288.89749\u003C/td>\n      \u003Ctd>14.30833\u003C/td>\n      \u003Ctd>20.19\u003C/td>\n      \u003Ctd>{'\u003C'} .0001*\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Cth>BodyStyle{'{'}SPV&Hatchback&MPV&SUV-Station&Sedan&Cabrio&Liftback&Pickup{'}'}\u003C/th>\n      \u003Ctd>-28.80209\u003C/td>\n      \u003Ctd>5.374477\u003C/td>\n      \u003Ctd>-5.36\u003C/td>\n      \u003Ctd>{'\u003C'} .0001*\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Cth>BodyStyle{'{'}SPV&Hatchback-MPV&SUV{'}'}\u003C/th>\n      \u003Ctd>8.2153396\u003C/td>\n      \u003Ctd>3.623798\u003C/td>\n      \u003Ctd>2.27\u003C/td>\n      \u003Ctd>0.0258\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Cth>BodyStyle{'{'}Station&Sedan&Cabrio&Liftback-Pickup{'}'}\u003C/th>\n      \u003Ctd>-45.12672\u003C/td>\n      \u003Ctd>12.9488\u003C/td>\n      \u003Ctd>-3.49\u003C/td>\n      \u003Ctd>0.0008*\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Cth>BodyStyle{'{'}Station&Sedan-Cabrio&Liftback{'}'}\u003C/th>\n      \u003Ctd>-52.61664\u003C/td>\n      \u003Ctd>6.97113\u003C/td>\n      \u003Ctd>-7.55\u003C/td>\n      \u003Ctd>{'\u003C'} .0001*\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Cth>BodyStyle{'{'}Cabriolet-Liftback{'}'}\u003C/th>\n      \u003Ctd>70.712853\u003C/td>\n      \u003Ctd>11.46589\u003C/td>\n      \u003Ctd>6.17\u003C/td>\n      \u003Ctd>{'\u003C'} .0001*\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Cth>Log(PriceEuro)\u003C/th>\n      \u003Ctd>43.782414\u003C/td>\n      \u003Ctd>14.52427\u003C/td>\n      \u003Ctd>3.01\u003C/td>\n      \u003Ctd>0.0033*\u003C/td>\n    \u003C/tr>\n  \u003C/table>\n  \u003Cfigcaption className=\"text-center mt-2 text-sm italic\">\n    Table 7\n  \u003C/figcaption>\n\u003C/figure>\n\n\n\nAfter we used forward selection as our variable selection method and set\np-value enter is 0.25 and p-value leave as 0.1, we determined our final\nmodel to be as follows:\n\n$$\n\\begin{aligned}\n\\hat{y} = & -909.39 + 0.29x_1 + 39.04x_2 - 2.08x_3 + 288.90x_4 \\\\\n& - 28.80x_5 + 8.22x_6 - 45.13x_7 - 52.62x_8 \\\\\n& + 70.71x_9 + 43.78x_{10}\n\\end{aligned}\n$$\n\nwhere $x_i$ are listed in the table below:\n\n\u003Cfigure>\n  \u003Ctable className=\"table-auto\">\n    \u003Ctbody>\n    \u003Ctr>\n      \u003Ctd>$x_1$\u003C/td>\n      \u003Ctd>Topspeed_KmH\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>$x_2$\u003C/td>\n      \u003Ctd>Rapid Charge\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>$x_3$\u003C/td>\n      \u003Ctd>Efficiency_WhKm\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>$x_4$\u003C/td>\n      \u003Ctd>log(Battery_Pack Kwh)\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>$x_5$\u003C/td>\n      \u003Ctd>BodyStyle{'{'}SPV&Hatchback&MPV&SUV–\u003Cbr/>Station&Sedan&Cabriolet&Lift back&Pickup{'}'}\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>$x_6$\u003C/td>\n      \u003Ctd>BodyStyle{'{'}SPV&Hatchback–MPV&SUV{'}'}\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>$x_7$\u003C/td>\n      \u003Ctd>BodyStyle{'{'}Station&Sedan&Cabriolet&Lift\u003Cbr/>back–Pickup{'}'}\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>$x_8$\u003C/td>\n      \u003Ctd>BodyStyle{'{'}Station&Sedan–Cabriolet&Lift\u003Cbr/>back{'}'}\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>$x_9$\u003C/td>\n      \u003Ctd>BodyStyle{'{'}Cabriolet–Lift back{'}'}\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>$x_{10}$\u003C/td>\n      \u003Ctd>log(PriceEuro)\u003C/td>\n      \u003C/tr>\n    \u003C/tbody>\n  \u003C/table>\n  \u003Cfigcaption className=\"text-center mt-2 text-sm italic\">\n    Table 8\n  \u003C/figcaption>\n\u003C/figure>\n\nWe found our final model to be adequate as it had a high ${R_a}^2$ and\nall of the covariates we found were significant at the $\\alpha = 0.05$\nlevel. The reason we used forward selection is to remove the issue of\nmulticollinearity and it was the simplest and easiest model. Figure 9\nshows the performance of our model. Some points to highlight are that\nour model's ${R_a}^2$ is 0.949, has a statistically significant $F$\nstatistic of 189.061 in the ANOVA test.\n\n### Model Interpretation\n\nThere is a natural interpretation of the covariates in that each one\nshows us how much they affect the range of EVs. The data we got shows\nthat the battery pack does affect the range the most. Other than the\npowertrain covariate, all of the data was significant at the 5% level of\nsignificance. We found how the different body styles of cars can affect\nthe range as well.\n\n\n## Conclusion\n\nBased on the results of our model we can see that there is a strong\npositive correlation between battery pack and range. This can help\nconsumers and car manufacturers when designing new EVs to create larger\nbattery packs and understand that with the combination of Bodystyle we\ncan see which are the best cars for range. For example, Cabrio or\nconvertibles and hatchbacks have the greatest impact on range with their\ncoefficient being 42. SUVs and pickup trucks for example have a negative\neffect on range.\n\nAnother interesting covariate we found was how rapid charge effects\nrange, with it being the third largest $\\beta$ that we found. This makes\nsense as charging cars will lead to more range for the car, it could\nmake the car more efficient. The price was also another factor that\nimpacts range, one that makes sense as price increases, the technology\nand range of the EV increases as well. Our model allows car\nmanufacturers, consumers, and investors to understand the factors of EVs\nthat most affect range allowing for better improvement of EVs for the\nfuture, improving the environment and lowering the market for gas\npowered vehicles.\n\n## 👥 Collaborators\n\n* Yuhang Du (Boston University)\n* Matthew George (Boston University)\n* Xiangru He (Boston University)\n* Yujie Yang (Boston University)","src/content/projects/ev-range-pred.mdx",[33],"../../assets/images/projects/ev-range-pred/thumbnail.gif","6d47af0de8ee5381","ev-range-pred.mdx","graphic-memoir-analysis",{"id":36,"data":38,"body":42,"filePath":43,"assetImports":44,"digest":46,"legacyId":47,"deferredRender":23},{"title":39,"cover":40,"githubUrl":41},"LLM-based Literature Analysis + Generation","__ASTRO_IMAGE_../../assets/images/projects/graphic-memoir-analysis/thumbnail.gif","https://github.com/lavaman131/graphic_memoir_analysis","import { Image } from \"astro:assets\";\nimport panel_detection from \"../../assets/images/projects/graphic-memoir-analysis/report/panel_detection.png\";\nimport avg_panels from \"../../assets/images/projects/graphic-memoir-analysis/report/avg_panels.png\";\nimport avg_words from \"../../assets/images/projects/graphic-memoir-analysis/report/avg_words.png\";\nimport word_cloud from \"../../assets/images/projects/graphic-memoir-analysis/report/word_cloud.png\";\nimport ratio from \"../../assets/images/projects/graphic-memoir-analysis/report/ratio.png\";\nimport sentiment from \"../../assets/images/projects/graphic-memoir-analysis/report/sentiment.png\";\nimport GPT2 from \"../../assets/images/projects/graphic-memoir-analysis/report/GPT2.gif\";\n\n## Introduction\n\nAccording to Anantrasirichai and Bull's \"Artificial Intelligence in the Creative Industries: a Review,\" artificial intelligence is described as an algorithmic process which allows \"a computer system to develop and emulate human-like behavior and hence make decisions similar to (or in some cases, better than) humans\" (591). A large branch of AI is machine learning (ML) which \"employs computational methods to 'learn' information directly from large amounts of example data without relying on a predetermined equation or model\" (591). Among the most recent developments in artificial intelligence is generative AI which utilizes algorithms to very accurately (measured as the algorithm's ability to effectively predict the next word/token given a previous sequence of words/tokens) create or generate text and image content. Artificial intelligence (AI) and statistical analysis is an exciting and rapidly evolving field, but its impact on literature has yet to be fully explored. In the first part of my research I will attempt to create a statistical tool to analyze graphic memoirs with Marjane Satrapi's graphic memoir, *The Complete Persepolis*, as a case study by closely examining the text and image content (features) of her memoir utilizing machine learning techniques. I will then follow my analysis with an experiment using generative AI to recreate text in the style of Satrapi in order to explore the capabilities of AI. While there are previous papers describing the influence of Satrapi's work as a medium to promote political and social activism against Iran's strict Islamic regime (Stromberg 91-119), none follow a statistical and machine learning approach to automatically identify unique features in Satrapi's memoir for analysis and directly assess the effects of generative AI for this matter. My work also provides a baseline as well as an open-source tool for statistical methods in comparing different graphic memoirs.\n\n## Methods\n\nThere are numerous features of a graphic memoir that allow it to convey different messages. For instance, the average number of words per page which can be extracted using optical character recognition algorithms presented in Vetulani, Zygmunt, et al. can reveal much about an author's style—short versus long sentences can have different effects on readers. Additionally, the number of panels in a page allows certain authors to underscore a particular scene in their memoir. Using object recognition or shape detection algorithms, it may be possible to count the number of panels per page in a given graphic memoir. Also, there are currently sentiment analysis algorithms in place that allow computers to recognize the connotation of certain words and phrases with the capabilities of large language models (LLMs) which are algorithms that have the ability to effectively interpret and generate natural language. Looking at the ratio of positive, negative, and neutral sentiment words may reveal important information in that regard. Another important metric would be to consider the average ratio of words to images in each page of the memoir. Additionally, whether a memoir is illustrated in black and white may reveal much about the work versus colored content. The author's background including race, age, nationality, gender, and any other personal information all could be indirectly related to the aforementioned statistics. Through statistical analysis of these features I can gain a greater context of the features that allow Satrapi's work to be such an effective medium for political and social protest as well as create a new form of computational literary analysis that can be embedded with traditional analysis of text.\n\n## Findings\n\n### Object Detection for Counting Panels\n\nUsing traditional object detection algorithms and appropriate shape thresholding it is possible to get an accurate count of the average number of panels per page in a given chapter of Satrapi's work. Through observing the number of panels per page, the reader can understand and compare the presence or lack-thereof of images in different memoirs. In the case of Satrapi's memoir, the strong presence of images aligns well with a study by psychologists Piotr Winkielman and Yekaterina Gogolushko on the \"Influence of Suboptimally and Optimally Presented Affective Pictures and Words on Consumption-Related Behavior\" which reveals that a single image elicits a stronger emotional response than a single word. In this particular sample page below we can observe the results of the object detection algorithm. In this specific case, the algorithm detects 6 panels denoted by the green outlines in the right-most image:\n\n\u003CImage src={panel_detection} class=\"w-full sm:w-2/3\" alt=\"Panel Detection\" />\n\n### Chapter vs Average Number of Panels Per Page\n\nFurthermore, digging deeper into the distribution of chapters versus the average number of panels per page one can see that Satrapi typically does not use more than seven panels on average per page. With this information, one can use the average number of panels per page to compare different memoirs.\n\n\u003CImage src={avg_panels} class=\"w-full sm:w-2/3\" alt=\"Average Panels\" />\n\n### Chapter vs Average Number of Words Per Page\n\nMoving on to the average number of words per page, due to the nature of *The Complete Persepolis* being a graphic memoir, it is much less dense in terms of average word (at most 200 words) count per page compared to novels which typically average to much more. The statistic of average number of words per page can be used to contrast the writing styles of different authors in graphic memoirs and weigh the advantages and disadvantages of denser texts. A highly debated topic in literature is finding this optimal word-count balance. By repeating this analysis for multiple memoirs it may be possible to find a general trend in word-count.\n\n\u003CImage src={avg_words} class=\"w-full sm:w-2/3\" alt=\"Average Words\" />\n\n### Words Frequencies\n\nThe frequency of certain words in a graphic memoir can also reveal more about the theme of a particular memoir. In the case of Satrapi's *The Complete Persepolis*, below is a visualization (word cloud) of the most common words in the entire memoir along with an accompanying table:\n\n\u003CImage src={word_cloud} alt=\"Word Cloud\" />\n\nAs seen in the table above, among the most common words is \"know\". The word \"know\" may appear frequently because it shows Satrapi's continual quest for knowledge of the world around her through her childhood in Iran, teenage years in Austria, and early adulthood in Iran. Arguably, this statistic reveals Satrapi's focus on knowledge acquired through interpersonal relationships with family, friends, and acquaintances. These relationships lead Satrapi to develop a knack for social and political activism in order to inform individuals with stereotypical and disillusioned perspectives of her Iranian nationality. This raises the ultimate question if the knowledge we gain from other people is as reliable as the research we conduct. The broader implications of Satrapi's concept of social disillusionment through her encounters with the global community while in school at Austria can be related to the effects of social media today and false narratives surrounding demographics of people.\n\n### Chapter vs Word to Image Ratio\n\nA highly discussed topic introduced by Scott McCloud in the chapter \"Show and Tell\" of his work, *Understanding Comics: The Invisible Art*, is finding the \"perfect\" balance between words and images. According to McCloud the norm in literature is that great works of art and literature are only possible when the words and images are kept separate. In Scott's perspective, people are often taught with picture books as children because they are \"easier\" and less nuanced than the more \"real\" novels read as adults. However, the art of comics is finding the right balance of words and images. Specifically, Satrapi's memoir utilizes a word and picture combination known as interdependent which involves words and pictures going hand in hand to convey an idea that neither could convey alone. The graph below shows the word to image ratios in every chapter of Marjane Satrapi's graphic memoir, *The Complete Persepolis*. For reference, the higher the word to image ratio, the greater the number of words relative to images there are. By repeating this analysis, it is possible to compare the interaction between words and images to study the ratio of each and their relative effectiveness in graphic memoirs.\n\n\u003CImage src={ratio} class=\"w-full sm:w-2/3\" alt=\"Average Ratio\" />\n\n### Colors Present in Memoir\n\nAnother undervalued property of graphic memoirs is their color palette. Scott McCloud describes the effect of color in the chapter \"A Word About Color\" of his work, *Understanding Comics: The Invisible Art*. According to McCloud, while color gives the author more ability to explore with the expression of their images and \"objectify their subjects\" (189), \"unfortunately, color is still an expensive option and has historically been in the hands of larger, more conservative publishers\" (191). This leads to most comic artists to be forced to experiment with shades of black and white. As explained by McCloud, \"the differences between black-and-white and color comics are vast and profound affecting every level of the reading experience.\" (192) McCloud states that in \"black and white, the ideas behind the art are communicated more directly [where] meaning transcends form [and] art approaches language.\" (192) Furthermore, McCloud states that flat colors \"take on more significance...and through more expressive colors comics can become an intoxicating environment of sensations that only color can give.\" (192) Thus, color plays an important role in the meaning of graphic memoirs, however, cost can be prohibitive. This could be a potential reason Satrapi uses a black and white color palette as well as to communicate a more direct meaning of her work:\n\n\n## Discussion\n\nOne interesting statistic to explore is the sentiment of a graphic memoir. Using state of the art language models it is possible to identify the connotation and tone of pages in a graphic memoir. As a good starting point, we can place the concept of sentiment into three broad categories: negative, neutral, and positive. Linguistically speaking, it can be possible to create datasets that roughly map common English words and their meaning to these categories. In the case of Marjane's work, it is not surprising to see that the majority of pages are detected as negative because the graphic memoir is centered around the difficulty of her life with the Iranian Revolution of 1979, Iran-Iraq War, separation from her family, loss of loved ones, and other trauma. However, this method is still lacking in understanding the nuance of combinations of multiple sentiments being expressed in a page. While it is possible to take the ratio of positive, neutral, and negative to aggregate a statistic of each, it leads to difficulty in effectively communicating my analysis. Hence, sentiment as a comparison metric of different memoirs has the potential to be further explored in future works. A graph of the highest sentiment score for each page of the memoir can be visualized in the bar graph below:\n\n\u003CImage src={sentiment} class=\"w-full sm:w-2/3\" alt=\"Sentiment\" />\n\n### AI Generated Text Using GPT2\n\nAs a final experiment, the capabilities of generative AI are explored through a transfer learning application on a LLM architecture known as GPT2 (Generative Pre-Trained Transformer). The model was fine-tuned (trained) on Satrapi's memoir and then tasked with sentence-completion of similar style text to her memoir. An example of the results are shown below. In this example, it can be seen that there are generally inconsistencies in the AI-generated text such as repetitive statements and logical fallacies. Through this initial case study, generative AI is still a long way from effectively and convincingly generating text. In fact, the simplest case of the algorithm involves probabilistically generating text based on the context of previous words (tokens) in the text. In most cases, similar types of algorithms are found to regurgitate information that they are trained on rather than generating unique information.\n\n\u003CImage src={GPT2} class=\"w-full sm:w-2/3\" alt=\"GPT2\" />\n\n## Conclusion\n\nThe space of AI and literature is an exciting and upcoming field. It is possible to explore human creativity and the effectiveness of literature by utilizing statistics and machine learning. My work sets a framework for future statistical tools that can be used to compare and contrast pieces of literature, especially graphic memoirs.\n\n\n## References\n\n* Anantrasirichai, Nantheera, and David Bull. \"Artificial Intelligence in the Creative Industries: a Review.\" *The Artificial Intelligence Review*, vol. 55, no. 1, 2022, pp. 589-656. \u003Ca href=\"https://doi.org/10.1007/s10462-021-10039-7\">https://doi.org/10.1007/s10462-021-10039-7\u003C/a>\n\n* McCloud, Scott. *Understanding Comics*. William Morrow Paperbacks, 1994.\n\n* Satrapi, Marjane. *The Complete Persepolis*. Pantheon Books, 2007.\n\n* Stromberg, Fredrik. \"Schemata in the Graphic Novel Persepolis.\" *European Comic Art*, vol. 13, no. 2, 2020, pp. 91-119. \u003Ca href=\"https://doi.org/10.3167/eca.2020.130205\">https://doi.org/10.3167/eca.2020.130205\u003C/a>\n\n\n* Vetulani, Zygmunt, et al. \"How to Improve Optical Character Recognition of Historical Finnish Newspapers Using Open Source Tesseract OCR Engine - Final Notes on Development and Evaluation.\" *Human Language Technology. Challenges for Computer Science and Linguistics*, vol. 12598, Springer International Publishing AG, Switzerland, 2020, pp. 17-30. Lecture Notes in Computer Science. \u003Ca href=\"https://doi.org/10.1007/978-3-030-66527-2_2\">https://doi.org/10.1007/978-3-030-66527-2_2\u003C/a>\n\n* Winkielman, Piotr, and Yekaterina Gogolushko. \"Influence of Suboptimally and Optimally Presented Affective Pictures and Words on Consumption-Related Behavior.\" *Frontiers in Psychology*, vol. 8, 2018, p. 2261. \u003Ca href=\"https://doi.org/10.3389/fpsyg.2017.02261\">https://doi.org/10.3389/fpsyg.2017.02261\u003C/a>\n\n## 👥 Collaborators\n\n* None","src/content/projects/graphic-memoir-analysis.mdx",[45],"../../assets/images/projects/graphic-memoir-analysis/thumbnail.gif","be215026983e8cce","graphic-memoir-analysis.mdx","grammar-ninja",{"id":48,"data":50,"body":54,"filePath":55,"assetImports":56,"digest":58,"legacyId":59,"deferredRender":23},{"title":51,"cover":52,"githubUrl":53},"Grammar Ninja: LLM-based Grammar Correction","__ASTRO_IMAGE_../../assets/images/projects/grammar-ninja/thumbnail.png","https://github.com/rvineet02/grammar_ninja","## Abstract\n\nImproving one’s English language writing is a significant challenge without access to proficient teachers that can provide valuable feedback. Given the recent rapid acceleration in generative models ability to understand language, we aim to develop a model/fine-tune a model to provide an interface that will generate feedback given text as an input. Our goal is provide quantitative benchmarks for language proficiency in six different areas: cohesion, syntax, vocabulary, phraseology, grammar, and conventions in a provided writing. Additionally, we also generate feedback at the inference layers to provide concrete feedback as to how the input text can be improved. To conclude, our project aims to apply the concepts learned in class to a real-world challenge, by providing a interface to acquire feedback on English writing. By focusing on key areas of language skills and providing model generated actionable feedback, we hope to contribute a somewhat practical tool. \n\n## Introduction\n\nThis project aims to develop a Natural Language Processing (NLP) based system that can automatically evaluate and provide feedback on student argumentative essays. The system will focus on several key aspects of writing, including the effectiveness of arguments, grammar, use of evidence, syntax, and tone. The feedback can be either quantitative, in the form of scores in various categories, or qualitative, as generated English feedback that offers specific guidance and suggestions for improvement.\n\nTo this end, we have decided to split our problem into three separate parts - predicting feedback scores based on the writing, identifying the different parts of argumentative writing (think Parts-Of-Speech tagging but for sentences as opposed to words), and finally constructing a generative model to produce the corrected version of a given input sentence.\n\nWe also provide instructions to setup the project [here](#Setup).\n\n## Methodology\n\nIn this section, we will discuss the various approaches we have taken to fine-tune the three models. \n\n### Feedback Scores (Bert-cased)\n\nWe used the English feedback dataset [here](https://www.kaggle.com/competitions/feedback-prize-english-language-learning). This was dataset we chose to use to identify scoring the dataset on the following features: cohesion, syntax, vocabulary, phraseology, grammar, conventions.\n\n\u003Ch4> Data Exploration\u003C/h4>\n\nGiven some input data `X`, we want to predict writing feedback scores `y` . The writing feedback scores range from 1 to 5 in 0.5 increments.\n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled.png)\n\nWe noticed a few things, most importantly that the target classes are imbalanced:\n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%201.png)\n\nFor example, `cohesion`:\n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%202.png)\n\n### Imbalanced Data - Applying Target Transformations\n\nSo, we now had to make a decision regarding how to re-balance the dataset, so the model can learn equally well on less represented data as well. Here, we considered some strategies:\n\n- Log Transformation: Useful for right-skewed distributions. However, it can't be applied to zero or negative values directly.\n- Square Root Transformation: Less aggressive than log transformation and can be used for right-skewed distributions.\n- Box-Cox Transformation: A more general form of transformation that includes log and square root as special cases. It can only be applied to positive data.\n- Yeo-Johnson Transformation: Similar to Box-Cox but can be applied to data with both positive and negative values.\n\nAlternate Strategies:\n\n- Under-sampling; Delete some data from rows of data from the majority classes. In this case, delete 2 rows resulting in label B and 4 rows resulting in label C.\n    - Limitation: This is hard to use when you don’t have a substantial (and relatively equal) amount of data from each target class.\n- Copy rows of data resulting minority labels. In this case, copy 4 rows with label A and 2 rows with label B to add a total of 6 new rows to the data set.\n    - Limitation: copying current data and you don’t really present anything new. You will get better models, though.\n- SMOTE - Synthetic Minority Oversampling Technique\n    - Synthetically generate new data based on implications of old data. Basically, instead of deleting or copying data, you use the current inputs to generate new input rows that are unique but will have a label based on what the original data implies. In the case above, one simple way to think of this idea would be to add 4 rows with label A to the data where the inputs represent total or partial similarities in values to current input features. Repeat this process for 2 rows of label B as well.\n    - Limitation: If two different class labels have common neighboring examples, it may be hard to generate accurate data representing what each unique label may look like from the input side and therefore SMOTE struggles with higher dimensionality data.\n\nAfter some experimentation on small sample sizes with SMOTE, Box-Cox Transformation and Log Transformation we came to the conclusion that target rebalancing with logs would be the most appropriate to reconstruct an appropriate distribution. \n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%203.png)\n\nWe have transformed the each target with a `log1` transformation, with `cohesion` as an example below:\n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%204.png)\n\nWe then compute the Shapiro-Wilk Test, Skewness, Kurtosis, giving rise to the following analysis: \n\nOur  transformation has correctly changed the distributions' characteristics, often reducing right skewness but introducing left skewness and altering the tailedness (kurtosis). After these changes, the data now more similarly  conforms to a normal distribution according to the Shapiro-Wilk test.  \n\n\u003Ch4> Model Architecture\u003C/h4>\n\nAfter this, we decided to choose a pre-trained generative model that understands semantics of language, and then further fine-tuning with our dataset. And additionally from above, we have, the following constraint - we want a model that is robust to non-normality. \n\nAfter some research and considering a few generative models GPT-3.5/4, Llama, we decided to use BERT instead. This is because BERT has both a cased and uncased version. So, we can experiment in more ways with tokenization. \n\nThe following is the model architecture:\n\n- We are using the BERT model. We then added a `Dropout` and `Linear Layer` as well. We add these layers to ensure the model is able to regularize and classify the data better.\n- In the forward loop, there are 2 outputs from the `BERT` model layer.\n- The output of this, `pooled_output` is passed through the `Dropout` layer and then the `Linear` layer.\n- We set the number of dimensions in the `Linear` layer to be equal to the number of classes we have in the dataset.\n\nThis gives is the following output:\n\n```\nBERT_Classifier(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (drop): Dropout(p=0.0, inplace=False)\n  (out): Linear(in_features=768, out_features=6, bias=True)\n)\n```\n\n\u003Ch4> Training Details\u003C/h4>\n\nAfter reading latest consensus, experiments on Huggingface and local experimentation, we came to the conclusion that the following hyper-parameters are best for this model and data:\n\n```python\nMAX_LEN = 200\nTRAIN_BATCH_SIZE = 4\nVALID_BATCH_SIZE = 4\nEPOCHS = 45\nLEARNING_RATE = 2e-05\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n```\n\n### **Essay Dissection (Longformer)**\n\nThis model automatically segments texts and classifies a range of argumentative and rhetorical elements. The primary elements identified by this model include:\n\n- Position: Identifying the main stance or thesis of the essay.\n- Lead: Recognizing introductory statements or premises.\n- Rebuttal: Detecting responses to counterarguments.\n- Claim: Identifying specific assertions or points made in support of the position.\n- Evidence: Recognizing data, quotations, or other factual information supporting claims.\n- Counterclaim: Detecting opposing viewpoints or arguments.\n- Concluding Statement: Identifying concluding remarks or summaries of the argument.\n\nThe training data used is taken from the writing feedback dataset [here](https://www.kaggle.com/competitions/feedback-prize-2021/data?select=train.csv).\n\n\u003Ch4> Data Exploration\u003C/h4>\n\nThe first step was to explore the initial two datasets. We found the smaller dataset mostly had duplicates of the first one, so we continued with just the larger dataset.\n\n![Screenshot 2023-12-21 at 2.18.54 PM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-21_at_2.18.54_PM.png)\n\nThe dataset was structured as shown above. There were overall about 15k documents/texts in the training dataset (which we also had access to). The split into the respective rhetorical parts is shown in the visual:\n\n![Screenshot 2023-12-21 at 10.42.09 PM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-21_at_10.42.09_PM.png)\n\nThis problem could have been treated as sequence classification problem or a token classification problem. However, we decided to structure it as a token classification similar to POS tagging.\n\n- Using the id, discourse type and prediction string columns we concatenated the texts on their id, each word was tagged with a discourse type.\n    \n    ![Screenshot 2023-12-21 at 4.57.24 PM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-21_at_4.57.24_PM.png)\n    \n- Now this text had to be converted to tokens suitable for our model. Since the first layer in our model was going to be a base pre-trained Longformer transformer, we went with the corresponding tokenizer.\n    \n    After converting the text to tokens, we needed to carefully align the tags with the tokenized text since some words would be split into multiple tokens.\n    \n    ![Screenshot 2023-12-21 at 5.02.41 PM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-21_at_5.02.41_PM.png)\n    \n    These tokens would eventually be encoded by the tokenizer along with the label before being passed into the model.\n    \n\u003Ch4> Model Architecture\u003C/h4>\n\nThe model relied on the pre-trained Longformer transformer, with a context-window of 4096 tokens. The Longformer is an advanced version of the Transformer model, optimized for processing long documents. It overcomes the limitations of traditional Transformer models like BERT or GPT, which struggle with lengthy texts due to their quadratic self-attention mechanism. The Longformer uses a \"sliding window\" attention mechanism, which reduces computational complexity from quadratic to linear by focusing each token's attention on a nearby window of tokens. This design allows the Longformer to efficiently handle texts much longer than standard models, making it ideal for tasks involving large documents, such as legal analysis, long-form summarization, and detailed document review.\n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%205.png)\n\n    The image above shows the difference in memory usage between full self-attention models and Longformer models. Notice the exponential increase in the memory usage of the full-self attention as the sequence length increases. Comparatively, Longformer performs much better. \n    \n    Our model used to classify long text documents such as essays consists of:\n    \n    - A **pre-trained Longformer** layer that can process text sequences of up to 4096 tokens, making it well-suited for lengthy essays.\n    - A **linear layer** (**`dense1`**) that reduces the dimensionality from the Longformer's output size to 256.\n    - A **ReLU activation** to introduce non-linearity, allowing the model to capture complex patterns.\n    - A **dropout layer** to mitigate overfitting by randomly zeroing some fraction of the output units during training.\n    - A final **linear layer** (**`dense2`**) that maps the reduced representation to the number of target labels (**`num_labels`**).\n    \nThe model outputs logits for each label, which can be converted into probabilities for essay classification. The **`forward`** method defines the data flow from input to output, utilizing **`input_ids`** and **`attention_mask`** to handle and process the input text.\n\n\u003Ch4> Training Details\u003C/h4>\n\nAfter extensive testing on smaller subsets of the data, we decided on the following hyper-parameters:\n\n```python\nnum_epochs = 10\nmax_seq_len = 800\nbatch_size = 16\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nloss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n\n# Scheduler to step learning rate\nscheduler = StepLR(optimizer, step_size=2, gamma=0.75)\n\n# Early stopping parameters\nbest_val_loss = float('inf')\nepochs_no_improve = 0\npatience = 2\n```\n    \nWe also employed mixed precision training and data parallelism to decrease training time. You can find the training loss and validation loss over epochs below.\n    \n\n### Grammar Correction (Mistral 7B)\n\n\u003Ch4> Data Exploration\u003C/h4>\n\nWe used the CoEdIT dataset from grammarly for our grammar correction and writing improvement which can be found [here](https://huggingface.co/datasets/grammarly/coedit).\n\nHere is how the dataset looks like:\n\n![Screenshot 2023-12-20 at 6.57.04 AM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-20_at_6.57.04_AM.png)\n\nFor this model, we have the following tasks: grammatical error correction, neutralize, simplification, paraphrase, coherence, and clarity improvement.\n\nBased on availability of the particular task and relevance for grammatical and writing improvement we chose to only train on neutralize, coherence, and grammatical error correction tasks. The distribution of grammar scenes is as follows:\n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%206.png)\n\nHere is an example of existing prompts in our dataset for different writing improvement tasks:\n\n![Screenshot 2023-12-20 at 7.16.41 AM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-20_at_7.16.41_AM.png)\n\nWe apply pre-processing to separate the dataset into instruction, input, and response format to prepare it for instruction fine-tuning. After preprocessing the dataset looks like this:\n\n![Screenshot 2023-12-20 at 7.19.44 AM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-20_at_7.19.44_AM.png)\n\nNext, to format the text for a generation pipeline we draw inspiration from the Alpaca Dataset formatting:\n\n```diff\nBelow is an instruction that describes a task, paired with an input \nthat provides further context. Write a response that appropriately \nthe request.\n\n### Instruction:\n{instruction}\n\n### Input:\n{sentence}\n\n### Response: \n{corrected_sentence}\n```\n\nBelow is an example of the prompt applied to an example sentence.\n\n```\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately the request.\n\n### Instruction:\nCorrect this sentence\n\n### Input:\nHellow there!\n\n### Response:\nHello there!\n```\n\nWe observed a few things in the data that allowed us to make a few decisions about the max input sequence to the model. Here, the figure displays the input token length distribution with the prompt applied to text in the dataset. The mean is depicted as a vertical red line at around 101 tokens:\n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%207.png)\n\nGiven the distribution above, we decided to standardize `token_size` of 200 based on distribution of input token lengths for sentences (model still takes up to `token_size` 4096 assuming enough GPU VRAM).\n\nHowever, during during inference, a `token_size` greater than 200 may not perform as well so a sliding window of 200 across each sentence may be necessary rather than predicting on the max token size of the particular LLM we are using.\n\n\u003Ch4> Model Architecture\u003C/h4>\n\nWe fine-tune the following base model -  **[Mistral-7B-v0.1](https://arxiv.org/abs/2310.06825)** (base model). For some context, the Mistral-7B-v0.1 Large Language Model (LLM) is a pretrained generative text model with 7 billion parameters. Mistral-7B-v0.1 outperforms Llama 2 13B on all benchmarks we tested.\n\nBelow, we describe some architectural details: It uses a similar architecture to Llama, but with improvements including: sliding window attention, rolling buffer cache, and pre-fill and chunking.\n\n![Screenshot 2023-12-20 at 9.28.10 AM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-20_at_9.28.10_AM.png)\n\n![Screenshot 2023-12-20 at 9.28.47 AM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-20_at_9.28.47_AM.png)\n\n![Screenshot 2023-12-20 at 9.30.31 AM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-20_at_9.30.31_AM.png)\n\nWe used the base model rather than the instruction-tuned model for chat since it makes more sense for our use-case to instruction-tune our model from the base model backbone (out-of-the-box only generates text, i.e., it does not follow instructions well).\n\n\u003Ch4> Training Details\u003C/h4>\n\nGiven the model is fairly large (though it is not on the small end for LLM), extensive hyperparameter tuning not possible due to compute restrictions (7B parameter model)\n\nWe reviewed accepted literature and guides online to make use of more common settings for hyperparameters:\n\n```\nper_device_train_batch_size=2  # Batch size,\ngradient_accumulation_steps=1  # No gradient accumulation steps,\nnum_train_epochs=1  # Fine-tune on entire dataset\nlearning_rate=2.5e-5  # Want a small lr for finetuning\nbf16=True  # Use mixed precision training with bfloat16,\noptim=\"paged_adamw_8bit\"  # Use 8-bit AdamW\nlogging_steps=25  # When to start reporting loss\nlogging_dir=LOGGING_DIR  # Directory for storing logs\nsave_strategy=\"steps\"  # Save the model checkpoint every logging step\nsave_steps=25  # Save checkpoints every 25 steps\nevaluation_strategy=\"steps\"  # Evaluate the model every logging step\neval_steps=25  # Evaluate and save checkpoints every 25 steps\n```\n\nWe fine-tune on a subset of the [CoEdIT](https://huggingface.co/datasets/grammarly/coedit) dataset (around 40000 samples). The backbone of the model was not changed during fine-tuning since the architecture is already very robust for generation tasks.\n\nWe used VM instance with 8 vCPUs, 40GB RAM, 24GB VRAM (NVIDIA RTX A5000) to fine-tune the model. We use techniques to significantly improve the efficiency of training with little to no performance sacrifice. The basic premise of the fine-tuning was the following - we utilize a parameter-efficient fine-tuning method (PEFT) called Quantized Low-Rank Adaptation (QLoRA). \n\nIn the past, fine-tuning large LLMs was a resource-intensive task, requiring substantial computational power and high-end GPU resources, thus limiting accessibility. Traditional methods involved fine-tuning the model on extensive datasets, followed by a step called 4-bit quantization for the model to function on consumer-grade GPUs after fine-tuning. This approach reduced resource usage but at the cost of the model's full capabilities, leading to compromised results.\n\nQLoRA emerges as a significant advancement within the realm of parameter-efficient fine-tuning (PEFT). PEFT aims to modify only a small fraction of a model's parameters, making fine-tuning more efficient and less resource-intensive. QLoRA aligns with this goal by allowing for the efficient fine-tuning of large language models using a single GPU. It maintains the high-level performance of a full 16-bit model even when reduced to 4-bit quantization. For perspective, while traditional methods required something like 780 gigabytes of VRAM to fine-tune a 65 billion parameter model, QLoRA achieves this with just a single 48 gigabyte VRAM GPU. This approach not only fits within the PEFT paradigm but also makes such advanced modeling far more attainable and practical.\n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%208.png)\n\n## Evaluation\n\n### **Feedback Scores (Bert-cased)**\n\n\u003Ch4> Train and Validation Performance\u003C/h4>\n\nThe train and validation loss of the model can be seen below: \n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%209.png)\n\nFrom this we can see that the model is performing quite well on the out-of-sample data, in-fact, it is performing better on the samples than on the training data itself. \n\nAdditionally, we tested two different `bert` models - `cased` and `uncased`. Testing on both models, it was clear that the `cased` model was better:\n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%2010.png)\n\nFrom the above, we concluded that the casing of a word, and by extension, how casing is applied within the context of a sentence alters its correctness on various metrics, and by using a uncased model and grouping these words together we were losing this signal. \n\nHence, we finally decided to stick with the `cased` model with the hyper-parameters discussed above.\n\n### **Essay Dissection (Longformer)**\n\n\u003Ch4> Train and Validation Performance\u003C/h4>\n\nFor this model, we compute the cross entropy loss. We compute the validation with the following formula: `number of correct predictions / total number of tokens`. Our model had an **accuracy of about 70%** on the testing data. Some of the performance curves can be seen below:\n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%2011.png)\n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%2012.png)\n\n### Grammar Correction (Mistral 7B)\n\n\u003Ch4> Train and Validation Performance\u003C/h4>\n\nThe train and validation loss of the model can be seen below: \n\n![train_loss (1).svg](../../assets/images/projects/grammar-ninja/report/train_loss_(1).svg)\n\n![val_loss (1).svg](../../assets/images/projects/grammar-ninja/report/val_loss_(1).svg)\n\nNote for the following before-after tests the final output is post-processed by cutting off the next sequence of conversations (prevent model from continuing indefinitely or until token limit)\n\n\u003Ch4> Benchmark Performance vs Fine-tune Performance\u003C/h4>\n\nWe compare the performance of the model before fine-tuning on the dataset (benchmark) and the fine-tuned model. In the figures below are the content after the `### Response` tag are generated:\n\n\u003Ch4> Before Fine-tuning\u003C/h4>\n\n```\n### Instruction:\nRemove grammar mistakes\n\n### Input:\nNLP, it stand for Natural Language Processing, is a field in computer science, where focus on how computers can understanding and interact with human language.\n\n### Response:\nNLP, it stands for Natural Language Processing, is a field in computer science, where focus on how computers can understand and interact with human language.\n```\n\n\u003Ch4> After Fine-tuning\u003C/h4>\n\n```\n### Instruction:\nRemove grammar mistakes\n\n### Input:\nNLP, it stand for Natural Language Processing, is a field in computer science, where focus on how computers can understanding and interact with human language.\n\n### Response:\nNLP, it stands for Natural Language Processing, is a field in computer science where the focus is on how computers can understand and interact with human language.\n```\n\nAs can be seen, fine-tuning improves grammar correction of LLM compared to out-of-the-box.\n\n## Presentation of Results\n\n### Feedback Scores (Bert-cased)\n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%2013.png)\n\nFor the feedback score model, we have the above frontend to display the results as percentages. We planned to combine all three models into this UI, but due to time constraints were not able to complete this. \n\n### Inference Scripts\n\nWe created inference scripts that can be used to test our trained models. They can be accessed on our repo by going to the tools folder and visiting one of the three models. More details are provided on the README of our GitHub repo for setup.\n\nHere is an example text that we use the test our trained models:\n\n```bash\ncat ../../examples/nlp.txt\n```\n\n```\nNLP, it stand for Natural Language Processing, is a field in computer science, where focus on how computers can understanding and interact with human language. It's goal is to make computers can understand and respond to text or voice data. But, it's hard because languages is very complex and have many rules that often not follow logic.\n\nIn field of NLP, machine learn algorithms is used for make computers can process and analyze large amounts of natural language data. The problems is that, even with advanced algorithms, computers often don't understand the nuances, like sarcasm or idioms, in human languages. So, many times, they makes errors when they tries to interpret what a human is saying or writing.\n```\n\nBelow our the outputs of our model for the example sentence.\n\n### Feedback Scores (Bert-cased)\n\n```bash\ncd feedback_scores\npython inference.py ../../examples/nlp.txt\n```\n\n![Screenshot 2023-12-22 at 5.28.39 PM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-22_at_5.28.39_PM.png)\n\n### Essay Dissection (Longformer)\n\n```bash\ncd essay_dissection\npython inference.py ../../examples/nlp.txt\n```\n\n![Screenshot 2023-12-22 at 5.25.07 PM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-22_at_5.25.07_PM.png)\n\n### Grammar Correction (Mistral 7B)\n\n- Note that this model takes a while to run if you are not using GPU\n    - since it involves multiple sliding windows if text is too long\n\n```bash\ncd grammar\npython inference.py ../../examples/nlp.txt\n```\n\n![Screenshot 2023-12-22 at 5.39.22 PM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-22_at_5.39.22_PM.png)\n\n## Future Work\n\nFor the BERT model specifically, we had some thoughts on how improve the model selection and the training process. Ideally, we would like to work with larger datasets and perform Hill Climbing because it can take lots of models and pick the best small subset of models. Additionally, we could add more specificity to the loss function - custom different loss rate per target; 2 stage pooling. First pool either words, sentences, or paragraphs and then also using different max_lens for training and inference. Also, it seems like there could be merits to doing some form of ensemble models though architecting this might be different with pre-trained large language models. \n\nIn the essay dissection model, we discussed utilizing a dual-Longformer setup that could potentially enhance our model's accuracy as seen [here](https://www.kaggle.com/code/abhishek/two-longformers-are-better-than-1). To counteract the skewed learning from imbalanced datasets, we also discussed implementing data rebalancing strategies such as SMOTE, which generates synthetic samples for minority classes, thus enhancing the training process. \n\nIn our grammar improvement LLM, we currently combine a set of prompts (clarity, coherence, grammatical error correction, and neutralize the sentence)\n\nWe decided to not fully utilize the entire CoEdIT dataset, since we thought that it wouldn’t directly relate to improving a sentence. These additional capabilities can be trained by including them in future fine-tuning runs and manifested in a separate UI that allows a user to paraphrase and/or simplify their writing.\n\nWe also used a barebones CLI inference script. In the future, given more time, we would package the three inference scripts as a [Gradio](https://github.com/gradio-app/gradio/) space to make our demo more interactive.\n\n## Setup\n\n### Environment Variables\n\n```bash\nexport HF_HUB_ENABLE_HF_TRANSFER=1\nexport HF_HOME=$HOME/.cache/huggingface/transformers\n```\n\n### Install Environment\n\n```bash\n# install dependencies\nconda env create --file config/${CONDA_ENV_FILE}\n# install grammar-ninja package\npip install -e .\n# download spacy model\npython -m spacy download en_core_web_md\n```\n\n## Demo\n\n```bash\ncd tools\n```\n\n### Essay Dissection (Longformer)\n\n```bash\ncd essay_dissection\npython inference.py ../../examples/nlp.txt\n```\n\n### Feedback Scores (Bert-cased)\n\n```bash\ncd feedback_scores\npython inference.py ../../examples/nlp.txt\n```\n\n### Grammar Correction (Mistral 7B)\n\n```bash\ncd grammar\npython inference.py ../../examples/nlp.txt\n```\n\n## 👥 Collaborators\n\n* Vineet Raju (Boston University)\n* Dhruv Chandwani (Boston University)","src/content/projects/grammar-ninja.mdx",[57],"../../assets/images/projects/grammar-ninja/thumbnail.png","d085b12fa5b39163","grammar-ninja.mdx","human-connectome",{"id":60,"data":62,"body":66,"filePath":67,"assetImports":68,"digest":70,"legacyId":71,"deferredRender":23},{"title":63,"cover":64,"githubUrl":65},"Detecting Brain Regions Associated with Autism Development","__ASTRO_IMAGE_../../assets/images/projects/human-connectome/thumbnail.gif","https://github.com/lavaman131/human_connectome","import { Image } from \"astro:assets\";\nimport connectome_diagram from \"../../assets/images/projects/human-connectome/report/connectome_diagram.svg\";\n\n\n## Inspiration\nBrain connectomics or brain network research has rapidly expanded using functional MRI (fMRI) and diffusion-weighted MRI (dwMRI). A common product of these varied analyses is a connectivity matrix (CM). A CM stores the connection strength between any two regions (\"nodes\") in a brain network. This format is useful for several reasons: \n1. it is highly distilled, with minimal data size and complexity\n2. graph theory can be applied to characterize the network's topology\n3. it retains sufficient information to capture individual differences such as age, gender, intelligence quotient (IQ), or disease state\n\nIn this project, the connectivity matrices of patients diagnosed with autism spectrum disorder (ASD) and typically developing (TD) autism are compared using graph statistical algorithms to determine regions of the brain associated with development of autism. \n\n## Statistical Analysis\nThis analysis relies on a simplified 2-complex based order d Forman–Ricci curvature [[1]](#1) of an edge $e = \\{u, v\\} \\in E$ is given by:\n\n$$\\begin{aligned} \\mathfrak {C}_G^{\\,2,d}(e) {\\mathop {=}\\limits ^{\\mathrm {def}}}\\mathfrak {C}_G^{\\,2,d}(u,v) = \\omega (e) \\left[ \\left( \\sum _{e \\sim f_d^2} \\frac{\\omega (e)}{\\omega (f_d^2)} \\,{+}\\, \\sum _{v \\sim e} \\frac{\\omega (v)}{\\omega (e)} \\right) \\ \\text {---}\\ \\sum _{e'||e,\\, e',e \\sim f_d^2} \\frac{ \\sqrt{\\omega (e) \\omega (e')} }{ \\omega (f_d^2)} \\right] \\end{aligned}$$\n\nThe difference of the curvatures is found between a pair of autism spectrum disorder (ASD) and typically developing (TD) autism connectivity matrices each constructed by taking the aggregated mean of all such similar samples corresponding to each diagnosis.\n\n## Results\n\n### Top 10 Statistically Significant Curvature Differences\nIt is important to note that some connections may occur in the same region of the brain, hence it may seem like a vertex is connected to itself which is not the case.\n\n#### 5 Most Negative Curvatures \n\n| Connected Region                                 | Curvature Difference |\n| ------------------------------------------------ | -------------------- |\n| Right Putamen, Right Insular Cortex              | -2331330.433152141   |\n| Right Insular Cortex, Right Insular Cortex       | -1994182.8538039196  |\n| Right Frontal Pole, Right Frontal Orbital Cortex | -1845748.8731592936  |\n| Right Frontal Pole, Right Frontal Pole           | -1716334.5267126393  |\n| Right Insular Cortex, Right Insular Cortex       | -1614932.5721171317  |\n\n#### 5 Most Positive Curvatures \n| Connected Region                                                                             | Curvature Difference |\n| -------------------------------------------------------------------------------------------- | -------------------- |\n| Left Central Opercular Cortex, Left Central Opercular Cortex                                 | 1934234.5367499162   |\n| Right Insular Cortex, Right Putamen                                                          | 1971224.916725241    |\n| Left Thalamus, Brain-Stem                                                                    | 2294358.294582237    |\n| Right Frontal Pole, Right Frontal Orbital Cortex                                             | 2565673.39796964     |\n| Left Middle Temporal Gyrus posterior division, Left Middle Temporal Gyrus posterior division | 3425965.9113210947   |\n\n## Discussion\nThe ten connections between the brain yielding statistically significant results (from most significant to least significant) towards the development of autism are:\n\n1. Left Middle Temporal Gyrus posterior division, Left Middle Temporal Gyrus posterior division \n2. Right Frontal Pole, Right Frontal Orbital Cortex\n3. Right Putamen, Right Insular Cortex   \n4. Left Thalamus, Brain-Stem     \n5. Right Insular Cortex, Right Insular Cortex   \n5. Right Insular Cortex, Right Putamen   \n6. Left Central Opercular Cortex, Left Central Opercular Cortex \n8. Right Frontal Pole, Right Frontal Orbital Cortex\n7. Right Frontal Pole, Right Frontal Pole\n9. Right Insular Cortex, Right Insular Cortex   \n\n### Connections Visualized\n\u003CImage src={connectome_diagram} alt=\"connectome_diagram\" />\n\n## Replicate Results\n\nVisit the [repo](https://github.com/lavaman131/human_connectome) for more information.\n\nAfter running the analysis, the output will be the top 5 most positive and top 5 most negative edges based on curvature difference:\n\n\u003Ctable className=\"table-auto\">\n  \u003Cthead>\n    \u003Ctr>\n      \u003Cth>Connected Regions\u003C/th>\n      \u003Cth>Curvature Difference\u003C/th>\n      \u003Cth>Type\u003C/th>\n    \u003C/tr>\n  \u003C/thead>\n  \u003Ctbody>\n    \u003Ctr>\n      \u003Ctd>Right Putamen ↔ Right Insular Cortex\u003C/td>\n      \u003Ctd>-2,331,330.43\u003C/td>\n      \u003Ctd>Negative\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>Right Insular Cortex ↔ Right Insular Cortex\u003C/td>\n      \u003Ctd>-1,994,182.85\u003C/td>\n      \u003Ctd>Negative\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>Right Frontal Pole ↔ Right Frontal Orbital Cortex\u003C/td>\n      \u003Ctd>-1,845,748.87\u003C/td>\n      \u003Ctd>Negative\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>Right Frontal Pole ↔ Right Frontal Pole\u003C/td>\n      \u003Ctd>-1,716,334.53\u003C/td>\n      \u003Ctd>Negative\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>Right Insular Cortex ↔ Right Insular Cortex\u003C/td>\n      \u003Ctd>-1,614,932.57\u003C/td>\n      \u003Ctd>Negative\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>Left Central Opercular Cortex ↔ Left Central Opercular Cortex\u003C/td>\n      \u003Ctd>1,934,234.54\u003C/td>\n      \u003Ctd>Positive\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>Right Insular Cortex ↔ Right Putamen\u003C/td>\n      \u003Ctd>1,971,224.92\u003C/td>\n      \u003Ctd>Positive\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>Left Thalamus ↔ Brain-Stem\u003C/td>\n      \u003Ctd>2,294,358.29\u003C/td>\n      \u003Ctd>Positive\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>Right Frontal Pole ↔ Right Frontal Orbital Cortex\u003C/td>\n      \u003Ctd>2,565,673.40\u003C/td>\n      \u003Ctd>Positive\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>Left Middle Temporal Gyrus posterior division ↔ Left Middle Temporal Gyrus posterior division\u003C/td>\n      \u003Ctd>3,425,965.91\u003C/td>\n      \u003Ctd>Positive\u003C/td>\n    \u003C/tr>\n  \u003C/tbody>\n\u003C/table>\n\n## References\n\u003Ca id=\"1\">[1]\u003C/a> Chatterjee, T., Albert, R., Thapliyal, S. *et al.* Detecting network anomalies using Forman–Ricci curvature and a case study for human brain networks. *Sci Rep* **11**, 8121 (2021). https://doi.org/10.1038/s41598-021-87587-z\n\n## 👥 Collaborators\n\n* Dr. Tanima Chatterjee (Boston University)","src/content/projects/human-connectome.mdx",[69],"../../assets/images/projects/human-connectome/thumbnail.gif","ae6a4fb23111403f","human-connectome.mdx","hand-gesture-recognition",{"id":72,"data":74,"body":78,"filePath":79,"assetImports":80,"digest":82,"legacyId":83,"deferredRender":23},{"title":75,"cover":76,"githubUrl":77},"🖖 Hand Gesture Recognition","__ASTRO_IMAGE_../../assets/images/projects/hand-gesture-recognition/thumbnail.gif","https://github.com/lavaman131/hand_gesture_recognition","import { Image } from \"astro:assets\";\nimport confusionMatrix from \"../../assets/images/projects/hand-gesture-recognition/reports/confusion_matrix.png\";\nimport gui from \"../../assets/images/projects/hand-gesture-recognition/reports/gui.png\";\nimport template1 from \"../../assets/images/projects/hand-gesture-recognition/binary_hands/1.png\";\nimport template2 from \"../../assets/images/projects/hand-gesture-recognition/binary_hands/2.png\";\nimport template3 from \"../../assets/images/projects/hand-gesture-recognition/binary_hands/3.png\";\nimport template4 from \"../../assets/images/projects/hand-gesture-recognition/binary_hands/4.png\";\nimport template5 from \"../../assets/images/projects/hand-gesture-recognition/binary_hands/5.png\";\nimport hand1 from \"../../assets/images/projects/hand-gesture-recognition/hands/1.png\";\nimport hand2 from \"../../assets/images/projects/hand-gesture-recognition/hands/2.png\";\nimport hand3 from \"../../assets/images/projects/hand-gesture-recognition/hands/3.png\";\nimport hand4 from \"../../assets/images/projects/hand-gesture-recognition/hands/4.png\";\nimport hand5 from \"../../assets/images/projects/hand-gesture-recognition/hands/5.png\";\n\n## 📚 Problem Definition\n\nThe problem is to recognize sign-language hand gestures from a video stream. This is useful because it can be used to create human computer interfaces that are more accessible to people with hearing disabilities. My analysis assumes that the background is relatively static and that the hand is the only moving object in the video stream.\n\nSome difficulties that I anticipate are:\n\n- The hand can be in different orientations and positions in the video stream.\n- The hand can be in different lighting conditions.\n- The hand can be occluded by other objects in the video stream.\n- The hand can be in motion.\n\nThe gestures are defined as follows:\n\n- **One**: The thumb is extended and the other fingers are closed.\n- **Two**: The thumb and the index finger are extended and the other fingers are closed.\n- **Three**: The thumb, index finger, and middle finger are extended and the other fingers are closed.\n- **Four**: The thumb, index finger, middle finger, and ring finger are extended and the little finger is closed.\n- **Five**: All fingers are extended.\n\n\u003Cdiv style=\"display: flex; justify-content: space-between; gap: 10px; margin: 20px 0;\">\n    \u003CImage src={hand1} alt=\"Hand 1\" style=\"flex: 1; max-width: 19%; height: auto;\" />\n    \u003CImage src={hand2} alt=\"Hand 2\" style=\"flex: 1; max-width: 19%; height: auto;\" />\n    \u003CImage src={hand3} alt=\"Hand 3\" style=\"flex: 1; max-width: 19%; height: auto;\" />\n    \u003CImage src={hand4} alt=\"Hand 4\" style=\"flex: 1; max-width: 19%; height: auto;\" />\n    \u003CImage src={hand5} alt=\"Hand 5\" style=\"flex: 1; max-width: 19%; height: auto;\" />\n\u003C/div>\n\n## 🛠️ Method and Implementation\n\n### Classical Computer Vision Algorithms\n\nI use binary image analysis followed by max contour detection for the segmentation of the hand. I also use template matching (with templates augmented via different scales and rotations to capture possible scales and orientations of the hand) with the maximum normalized correlation coefficient for classifying the hand movement as the digit 1, 2, 3, 4, or 5. More detailed analysis of the algorithms used is provided below.\n\n#### Binary Image Conversion Using Skin Color\n\nBy utilizing thresholding of RGB, HSV, and YCRB color profiles utilizing a combination of bitwise AND along with bitwise OR operators (which can be seen in the `color_model_binary_image_conversion` function I created [here](./a2/data/preprocessing.py) in my code) of an image as described in [this paper](https://arxiv.org/pdf/1708.02694.pdf), I was able to segment skin in an image with high accuracy. Using this mask, I was able to convert the original image to a binary image where the skin is black and the background is white. This binary image is then used for contour detection and template matching in the following steps.\n\n#### Contour Detection for Hand Segmentation\n\nAfter converting the image to a binary format, the next step is to segment the hand from the background. This is achieved through contour detection, a process used to find the outlines of objects within binary images. The algorithm I employ for this purpose is based on the principle of finding continuous curves that delineate object boundaries in an image. The specific steps are as follows:\n\n1. **Find Contours**: Utilize OpenCV's `findContours` function, which implements the Suzuki85 algorithm, to detect the contours in the binary image. This function returns a list of contours found, with each contour represented as a vector of points.\n\n2. **Select Max Contour**: Among all detected contours, the one with the maximum area is considered to represent the hand. This is based on the assumption that the hand is the largest skin-colored object in the image. The area of a contour is calculated using OpenCV's `contourArea` function.\n\n3. **Draw/Use Contour**: The maximal contour is then used either to create a mask for the hand or to extract the hand's outline for further processing. This step is crucial for isolating the hand from the rest of the image, ensuring that subsequent steps operate solely on the hand region.\n\nMathematically, contour detection can be seen as the process of identifying the boundaries $C$ of a region $R$ in the binary image where $C = \\partial R$. The Suzuki85 algorithm efficiently traces these boundaries by examining the connectivity of edge pixels in the image.\n\n#### Template Matching for Sign Language Digit Classification\n\nTemplate matching is a technique in computer vision used for finding areas of an image that match a template image. In the context of classifying hand movements as digits 1 through 5, I leverage template matching to compare the segmented hand region against a set of pre-defined templates corresponding to each digit. The process involves:\n\n1. **Template Preparation**: Generate a set of template images for each digit (1-5), capturing various scales and orientations to account for different hand sizes and positions. This augmentation ensures a comprehensive set of references for matching.\n\n\u003Cdiv style=\"display: flex; justify-content: space-between; gap: 10px; margin: 20px 0;\">\n    \u003CImage src={template1} alt=\"Template 1\" style=\"flex: 1; max-width: 19%; height: auto;\" />\n    \u003CImage src={template2} alt=\"Template 2\" style=\"flex: 1; max-width: 19%; height: auto;\" />\n    \u003CImage src={template3} alt=\"Template 3\" style=\"flex: 1; max-width: 19%; height: auto;\" />\n    \u003CImage src={template4} alt=\"Template 4\" style=\"flex: 1; max-width: 19%; height: auto;\" />\n    \u003CImage src={template5} alt=\"Template 5\" style=\"flex: 1; max-width: 19%; height: auto;\" />\n\u003C/div>\n\n2. **Normalized Correlation Coefficient**: For each template, calculate the match with the segmented hand using the normalized correlation coefficient. This metric measures the similarity between the template and portions of the target image, with a range from -1 (no match) to 1 (perfect match). Mathematically, it is defined as:\n\n$$\n\\begin{align*}\n&\\text{NCC}(T, I) = \\frac{\\sum_{x,y} [T(x,y) - \\bar{T}][I(x,y) - \\bar{I}]}{\\sqrt{\\sum_{x,y} [T(x,y) - \\bar{T}]^2 \\sum_{x,y} [I(x,y) - \\bar{I}]^2}} \\\\\n&\\text{where } T \\text{ is the template, }\\\\\n&I \\text{ is the image under examination, and }\\\\\n&\\bar{T}, \\bar{I} \\text{ are the mean values of } T \\text{ and } I \\text{ respectively.}\n\\end{align*}\n$$\n\n3. **Matching and Classification**: For each digit's set of templates, compute the NCC across the hand region. The digit whose template yields the highest NCC is determined to be the hand's sign. This approach effectively classifies the hand gesture by finding the most similar template in terms of shape and orientation.\n\nBy employing these algorithms, the system can segment the hand from the background and classify its gesture into one of the five digits, using the principles of binary image analysis, contour detection, and template matching.\n\n## 🔬 Experiments\n\nI conducted the following experiments to evaluate the performance of the hand gesture recognition system and logged the results in the `./experiments` directory with [hydra](https://hydra.cc/docs/intro/) configuration files. See more details about this in the [usage section](#-usage) of the README.\n\n### Different Region of Interest (ROI) Sizes\n\nI experimented with different regions of interest (ROI) to evaluate the performance of the hand gesture recognition system. The ROI is defined as the location in each frame where the processing happens (visually depicted by a green rectangle in my GUI). Intuitively, I saw the best performance when the ROI was centered around the hand. I decided to use an ROI width and height of 640 pixels and 790 pixels respectively. This is because using the full frame as the ROI would skew the results of the segmentation of the hand as the background would be included in the processing. Another practical implication is that the user can move their face outside of the ROI and not have it interfere with the processing of the hand gesture recognition system. Finally, the implementation of an ROI allows for a more intuitive and user-friendly experience for the user as they can see where the processing is happening.\n\n### Different Gamma Levels for Source Image\n\nI experimented with different gamma levels for the source image to evaluate the performance of the hand gesture recognition system. The gamma level is a parameter that controls the brightness of the source image. After experimenting with gamma levels from 0.1 to 1.2, I found that the best performance was achieved when the gamma level was set to 0.375. This is because it allowed there to be a greater contrast between the hand and the background. The gamma level can be changed based on the lighting conditions of the environment.\n\n### Template Matching\n\n#### Different Scales and Rotations\n\nTraditional template matching is very error prone because it relies on the exact match of the template with the image in terms of size and orientation. To overcome this, I experimented with different scales and rotations of the template images to evaluate the performance of the hand gesture recognition system. I found that the best performance was achieved when the template images were augmented with different scales and rotations. This is because it allowed the system to recognize the hand gesture even if the hand was in different orientations and positions in the video stream. The system was able to recognize the hand gesture even if the hand was occluded by other objects in the video stream.\n\nI ended up using the following scales (scale factor) and rotations (in degrees) for the template images:\n\n- **Scales**: [0.7, 0.8, 0.9, 1.0]\n- **Rotations**: [-20.0, -10.0, 0.0, 10.0, 20.0]\n\n#### Front and Back Camera Template Images\n\nAnother important experiment I conducted was to evaluate the performance of the system with template images taken with a front and back camera. I found that the system was able to perform better when the template image were taken with the same type of camera as the intended use. This is because the two different cameras lead to different orientations and shapes of the hand. I have included the template images taken with the front and back camera in the `./templates` directory. The template image can be specified by the user depending on whether they are using the front or back camera in the GUI (see the [advanced usage section](#advanced-usage) of the README for more details about this).\n\n#### Hand Template Image vs Finger Template Image\n\nI experimented with using a hand template image and a finger template image to evaluate the performance of the hand gesture recognition system. I found that the system was able to perform  much better with a hand template image. This is because the hand template image was able to capture the shape and orientation of the hand better than the finger template image.\n\n## 📈 Results\n\nI ran 20 trials with each finger to evaluate the performance of the hand gesture recognition system. For instance, I used the following command for the sign language digit 5 with 20 frames and a 5-second delay to start the camera and allow the user to prepare their hand for the gesture recognition system to capture the frames and process them for classification:\n\n```bash\npython main.py \\\nhydra.job.name=class_5 \\\nprocessing.ground_truth_label=5 \\\ncamera.num_frames=20 \\\ncamera.start_delay_seconds=5\n```\n\nThe following are the results of the experiments which I logged in the `./experiments` directory. Then I used the `./tools/evaluate.py` script to evaluate the performance of the hand gesture recognition system and logged the results in the `./reports` directory.\n\n### Confusion Matrix\n\n\u003CImage src={confusionMatrix} alt=\"Confusion Matrix\" />\n\n## 🗣️ Discussion\n\nThe performance of the system is impressive given the simplicity of the algorithms used. In the experiments I ran, the system was able to classify the sign-language digit 5 with 100% accuracy. However, the environment I ran my experiments in was relatively controlled with a static background and good lighting conditions. Using binary image analysis, max contour detection for the segmentation of the hand, and template matching proved to be a basic system to recognize sign-language hand gestures from a video stream.\n\n## 🏆 Conclusions\n\nWhile it is possible to recognize sign-language hand gestures from a video stream using classical computer vision algorithms, the performance of the hand gesture recognition system is far from perfect. Additionally, the amount of processing that occurs per frame makes the algorithm very slow (re-implementing things in C++ could help 😊). The system works best when the user tries to shape their hand to mimic the template images for the sign-language digit (1-5) that they are trying to automatically classify. The hand should be still and the background should be relatively static with not too much overexposure or underexposure in the camera. The system will not work well if the hand is in motion or if the background is not relatively static. Future work could involve using machine learning or deep learning algorithms to improve the performance of the hand gesture recognition system.\n\n## 🎬 Credits and Bibliography\n\n[Gamma Correction](https://docs.opencv.org/3.4/d3/dc1/tutorial_basic_linear_transform.html)\n\n[Gamma Correction 2](https://pyimagesearch.com/2015/10/05/opencv-gamma-correction/)\n\n[Skin Detection](https://arxiv.org/pdf/1708.02694.pdf)\n\n[Contour Approximation](https://pyimagesearch.com/2021/10/06/opencv-contour-approximation/)\n\n[Template Matching](https://pyimagesearch.com/2021/10/11/opencv-template-matching/)\n\n## 👥 Collaborators\n\n* None","src/content/projects/hand-gesture-recognition.mdx",[81],"../../assets/images/projects/hand-gesture-recognition/thumbnail.gif","143dbfc5bf64d156","hand-gesture-recognition.mdx","motion-gpt15",{"id":84,"data":86,"body":90,"filePath":91,"assetImports":92,"digest":94,"legacyId":95,"deferredRender":23},{"title":87,"cover":88,"githubUrl":89},"MotionGPT 1.5","__ASTRO_IMAGE_../../assets/images/projects/motion-gpt1.5/thumbnail.gif","https://github.com/lavaman131/motionGPTv1.5","import { Image } from \"astro:assets\";\nimport flowmdm_failure from \"../../assets/videos/projects/motion-gpt1.5/report/flowmdm_fail.mp4\";\nimport motiongpt_failure from \"../../assets/videos/projects/motion-gpt1.5/report/motiongpt_fail.mp4\";\nimport motiongptv15_pipeline from \"../../assets/images/projects/motion-gpt1.5/report/motiongptv1.5_pipeline.png\";\nimport motiongptv15_success from \"../../assets/videos/projects/motion-gpt1.5/report/motiongptv15_success.mp4\";\n\n\n## Introduction\n\nThe field of computer vision has witnessed significant advancements in the development of photorealistic avatars for various applications, including virtual reality, gaming, robotics, and medicine. However, generating realistic and seamless human motion compositions remains a challenging task due to the complex nature of human movement and the lack of datasets providing long motion sequences with diverse textual annotations.\n\nRecent works have explored different approaches to address this problem. FlowMDM, a pioneering diffusion-based model, introduced Blended Positional Encodings (BPE) and Pose-Centric Cross-Attention (PCCAT) to generate smooth and realistic motion compositions without the need for post-processing or additional denoising steps. The model achieves state-of-the-art results on the HumanML3D and Babel datasets. Additionally, FlowMDM introduces two new metrics, Peak Jerk (PJ) and Area Under the Jerk (AUJ), to better assess the smoothness and realism of generated motion compositions.\n\nAnother notable work is MotionGPT, a unified motion-language model that handles multiple motion-relevant tasks by treating human motion as a specific language. MotionGPT employs discrete vector quantization to transfer 3D motion into motion tokens, enabling language modeling on both motion and text in a unified manner. The model achieves state-of-the-art performances on tasks such as text-driven motion generation, motion captioning, motion prediction, and motion in-between.\n\nHowever, FlowMDM struggles with very complex descriptions and does not generalize well to out of distribution motions. This makes it hard to adapt to new motion generation applications. Additionally, FlowMDM is not powered by natural language descriptions, making it difficult to integrate in multimodal language models. Additionally, MotionGPT struggles to generate effective 3D motions via qualitative analysis, perhaps overfitting to the training dataset. By using a hybrid approach of combining aspects of FlowMDM and MotionGPT, we are able to improve generalizability of human composition models while providing greater accessibility to these systems via natural language-powered descriptions.\n\nBuilding upon the groundbreaking work of FlowMDM and MotionGPT, we introduce MotionGPTv1.5, a language-enhanced human motion composition model that pushes the boundaries of seamless and realistic motion generation. Our approach makes the first steps to leverage the motion generation pipeline of these models and incorporates natural language text prompts to improve performance on out-of-distribution data.\n\nBy utilizing a language model to effectively prompt the motion generation backbone, MotionGPTv1.5 can generate more diverse and coherent motion compositions, even when faced with novel or unseen scenarios. The integration of natural language prompts allows for a more intuitive and user-friendly interface, enabling users to guide the motion generation process with simple textual descriptions as well as better generalizability to extrapolated motions.\n\n## Method\n\nOur method works like the following (as summarized in the figure below).\n\n1. A user inputs a natural language prompt to interact with an off-the-shelf LLM (Llama 3 in our case). The user’s query is converted to an embedding and compares the similarity of the word embedding to relevant motions that FlowMDM was trained on which are stored in a vector database. The matching word embeddings in the vector database are then injected into the following system prompt for the LLM with a `{motion_dictionary}` placeholder key that allows the model to contextualize the actions that the model, FlowMDM, was conditioned on. The assumption here is that FlowMDM has been sufficiently trained on a reasonable set of “base actions” that can be combined to produce a more complex action.\n2. The LLM will generate an output a JSON object depending on the user’s request like the following (note that `\u003Cmotion>` and `\u003C/motion>` special tokens surround the JSON text to appropriately parse the following). In this stage, since the model has no prior knowledge of the required durations of each action, we omit the generation of action durations. This can optionally be included with a reasonable prompt as well, but we choose to fine-tune a custom motion duration prediction model with DistilBERT, yielding reasonable results. We have found that the model does not struggle at all to produce the expected format.\n\n```json\n{\n  \"text\": [\n    \"stand\",\n    \"lift right leg\",\n    \"move right leg away from left leg\",\n    \"lower right leg\"\n  ]\n}\n```\n\n3. As explained previously, LLMs provide arbitrary information when asked to guess the duration of each motion of an exercise, so we fine-tune DistilBERT with a lightweight LSTM output layer proceeding the final hidden layer of BERT. We then proceed with a token regression task to predict the duration for each motion in seconds. For more information please refer to the supplementary code and the [file](https://github.com/lavaman131/motionGPTv1.5/blob/main/src/mgpt/nlp/utils/duration_extraction.py) in the repo.\n4. Finally, the output of DistilBERT is post-processed back to the word resolution by averaging the durations of the same word id for a given token because of how tokenization is handled by BERT. The duration is then scaled by the frames per second of our video (in this case 30 fps) yielding a final output that looks like this:\n\n```json\n{\n  \"text\": [\n    \"stand\",\n    \"lift right leg\",\n    \"move right leg away from left leg\",\n    \"lower right leg\"\n  ],\n  \"lengths\": [67, 94, 176, 91]\n}\n```\n\n\u003CImage src={motiongptv15_pipeline} alt=\"MotionGPTv1.5 Pipeline\" />\n\n## Results\n\n### MotionGPTv1.5 Generated Motion\n\nHere is the same example for hip abduction on MotionGPTv1.5 using the user prompt: Can you generate the hip abduction motion?\n\n\u003Cdiv className=\"flex justify-center my-8\">\n  \u003Cvideo controls width=\"480\" height=\"240\">\n    \u003Csource src={motiongptv15_success} type=\"video/mp4\" />\n    Your browser does not support the video tag.\n  \u003C/video>\n\u003C/div>\n\n### Failure Cases of FlowMDM and MotionGPT\n\nFlowMDM produces a similar failure case as well for the same hip abduction exercise:\n\n```json\n{\n  \"text\": [\"hip abduction\"],\n  \"lengths\": [300] # number of frames to generate at 30fps\n}\n```\n\n\u003Cdiv className=\"flex justify-center my-8\">\n  \u003Cvideo controls width=\"480\" height=\"240\">\n    \u003Csource src={flowmdm_failure} type=\"video/mp4\" />\n    Your browser does not support the video tag.\n  \u003C/video>\n\u003C/div>\n\nFor instance hip abduction, a common physical therapy exercise is not well supported by MotionGPT as seen below:\n\n```markdown\n# Exercise: Hip Abduction\n\n# Prompt:\nStand straight, hold support. Lift one leg sideways, keep straight. Return leg. Alternate sides. Repeat.\n```\n\n\u003Cdiv className=\"flex justify-center my-8\">\n  \u003Cvideo controls width=\"480\" height=\"240\">\n    \u003Csource src={motiongpt_failure} type=\"video/mp4\" />\n    Your browser does not support the video tag.\n  \u003C/video>\n\u003C/div>\n\nThe results are obviously not ideal, but MotionGPTv1.5 is better than out-of-the-box for these models.\n\n## Applications\n\n### Assistive Physical Therapy Chatbot\n\nOne neat application is the ability of MotionGPTv1.5 to aid in the rehabilitation process of patients. Although very preliminary (not perfect), by improving motion generation and language models generalizability to new motions, we can naturally adapt them to represent unseen and more complex motions.\n\n## Conclusions\n\nIn conclusion, MotionGPTv1.5 represents a first step forward in the field of language-enhanced human motion composition. By building upon the existing frameworks of FlowMDM and MotionGPT, it offers improved performance on out-of-distribution data and more intuitive user interaction. While there remain challenges to overcome, particularly in the generation of complex and unseen motions, the progress made by MotionGPTv1.5 is promising. The potential applications, such as in physical therapy, highlight the value of further research and development in this area. We look forward to the continued evolution and refinement of this model.\n\n## 👥 Collaborators\n\n* None","src/content/projects/motion-gpt1.5.mdx",[93],"../../assets/images/projects/motion-gpt1.5/thumbnail.gif","47dfb0ddac24f333","motion-gpt1.5.mdx","object-tracking",{"id":96,"data":98,"body":102,"filePath":103,"assetImports":104,"digest":106,"legacyId":107,"deferredRender":23},{"title":99,"cover":100,"githubUrl":101},"👣 Single Object Tracking, Multi-Object Tracking, and Data Association","__ASTRO_IMAGE_../../assets/images/projects/object-tracking/thumbnail.gif","https://github.com/lavaman131/object_tracking","import { Image } from \"astro:assets\";\nimport mot from \"../../assets/images/projects/object-tracking/report/mot.gif\";\nimport sot from \"../../assets/images/projects/object-tracking/report/sot.gif\";\n\n## 📚 Problem Definition\n\n### Single Object Tracking\n\nIn the realm of computer vision and video processing, tracking the movement of objects across a sequence of frames is a fundamental challenge. One specific aspect of this challenge is Single Object Tracking (SOT), where the goal is to follow the trajectory of a single entity, such as a vehicle, across multiple frames.\n\nThe core task is to process detected 2D locations (x, y coordinates) of a vehicle as it moves across various frames of a video. These detections are inherently noisy due to various factors like camera movement, changes in lighting, and occlusions. The objective is to implement a filtering technique that can generate a smooth and continuous 2D track of the vehicle, mitigating the noise and inaccuracies in the observed locations. The choice of an alpha-beta filter or a Kalman filter allows for a balance between simplicity and effectiveness, leveraging predictions and measurements to estimate the vehicle's current and future states with a higher degree of accuracy.\n\n### Multi-Object Tracking and Data Association\n\nExpanding the challenge to Multi-Object Tracking (MOT), this project addresses the complexity of tracking multiple objects simultaneously. In real-world scenarios, a video may contain several objects of interest, each requiring tracking and identification over time. The task involves processing bounding boxes for multiple detected objects across video frames and efficiently tracking them by assigning a unique ID to each object. This ID must persist as long as the object is present and detected in the video.\n\nThe critical challenge in MOT is the Data Association problem, which involves correctly associating the detected bounding boxes with the corresponding tracks (or IDs) over time, especially in scenarios where objects may intersect, occlude each other, or leave and re-enter the frame. This requires sophisticated algorithms that can manage uncertainties and ambiguities in object detection and identification, ensuring accurate and consistent tracking of multiple objects throughout the video sequence.\n\nThis project aims to develop and implement solutions for both SOT and MOT, leveraging filtering techniques and data association strategies to achieve robust and reliable object tracking in video data.\n\n## 🛠 Methods\n\n### Alpha-Beta Filtering in 1D and 2D\n\nTo smooth out the noisy detections and predict future positions of the tracked objects, we implemented the alpha-beta filter, a simplified form of the Kalman filter. The alpha-beta filter is well-suited for real-time applications due to its computational efficiency and ease of implementation.\n\n#### 1D Alpha-Beta Filter\n\nIn the 1D case, the alpha-beta filter estimates the position and velocity along a single axis. The filter operates in two main steps: prediction and update.\n\n**Prediction Step:**\n\n- **Position Prediction:** $\\hat{x}_{k|k-1} = \\hat{x}_{k-1|k-1} + \\hat{v}_{k-1|k-1} \\Delta t$\n- **Velocity Prediction:** $\\hat{v}_{k|k-1} = \\hat{v}_{k-1|k-1}$\n\n**Update Step:**\n\n- **Measurement Residual:** $r_k = z_k - \\hat{x}_{k|k-1}$\n- **Position Update:** $\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + \\alpha r_k$\n- **Velocity Update:** $\\hat{v}_{k|k} = \\hat{v}_{k|k-1} + \\left( \\frac{\\beta}{\\Delta t} \\right) r_k$\n\nWhere:\n- $\\hat{x}_{k|k}$ and $\\hat{v}_{k|k}$ are the updated estimates of position and velocity at time $k$.\n- $z_k$ is the measured position at time $k$.\n- $\\Delta t$ is the time interval between measurements.\n- $\\alpha$ and $\\beta$ are the filter gains controlling the responsiveness to new measurements.\n\n#### 2D Extension\n\nFor 2D tracking, the alpha-beta filter is applied separately to the x and y coordinates. This assumes that the motion in each dimension is independent, which is acceptable in many tracking scenarios.\n\n**Prediction Step:**\n\n- **Position Prediction:** $\\hat{\\mathbf{x}}_{k|k-1} = \\hat{\\mathbf{x}}_{k-1|k-1} + \\hat{\\mathbf{v}}_{k-1|k-1} \\Delta t$\n- **Velocity Prediction:** $\\hat{\\mathbf{v}}_{k|k-1} = \\hat{\\mathbf{v}}_{k-1|k-1}$\n\n**Update Step:**\n\n- **Measurement Residual:** $\\mathbf{r}_k = \\mathbf{z}_k - \\hat{\\mathbf{x}}_{k|k-1}$\n- **Position Update:** $\\hat{\\mathbf{x}}_{k|k} = \\hat{\\mathbf{x}}_{k|k-1} + \\alpha \\mathbf{r}_k$\n- **Velocity Update:** $\\hat{\\mathbf{v}}_{k|k} = \\hat{\\mathbf{v}}_{k|k-1} + \\left( \\frac{\\beta}{\\Delta t} \\right) \\mathbf{r}_k$\n\nWhere:\n- $\\hat{\\mathbf{x}}_{k|k}$ and $\\hat{\\mathbf{v}}_{k|k}$ are the updated estimates of position and velocity vectors.\n- $\\mathbf{z}_k$ is the measured position vector at time $k$.\n\n#### Selection of Filter Gains\n\nThe values of $\\alpha$ and $\\beta$ determine the filter's performance:\n\n- **High $\\alpha$ and $\\beta$:** More responsive to new measurements but less smooth trajectories.\n- **Low $\\alpha$ and $\\beta$:** Smoother trajectories but slower to react to changes.\n\nThese gains are typically tuned empirically based on the characteristics of the motion and the measurement noise.\n\n### Data Association Using the Hungarian Algorithm\n\nIn multi-object tracking, correctly associating detections to existing tracks is crucial. We implemented the Hungarian matching algorithm as our data association approach due to its ability to find the optimal assignment between detections and tracks efficiently.\n\n#### Cost Matrix Construction\n\nWe construct a cost matrix $C$ where each element $c_{ij}$ represents the cost of assigning detection $i$ to track $j$. The cost is computed based on a distance metric, such as the Euclidean distance between the predicted position of the track and the detected position:\n\n$$c_{ij} = \\| \\hat{\\mathbf{x}}_{j|k-1} - \\mathbf{z}_i \\| $$\n\n#### Hungarian Algorithm Steps\n\n1. **Initialization:** Start with the cost matrix $C$.\n2. **Row Reduction:** Subtract the minimum value in each row from all the elements of that row.\n3. **Column Reduction:** Subtract the minimum value in each column from all the elements of that column.\n4. **Assignment:** Cover all zeros in the matrix using the minimum number of horizontal and vertical lines. If the number of lines equals the number of tracks, an optimal assignment is found.\n5. **Adjustment:** If not all zeros are covered, find the smallest uncovered value, subtract it from all uncovered elements, and add it to elements at the intersections of the covering lines. Repeat the assignment step.\n\n#### Updating Tracks\n\nAfter obtaining the optimal assignment:\n\n- **Matched Pairs:** Update the state estimates of the tracks using the associated detections.\n- **Unmatched Detections:** Initialize new tracks for these detections.\n- **Unmatched Tracks:** Increase the missed detection count; if it exceeds a threshold, the track is terminated.\n\n#### Advantages\n\n- **Optimal Matching:** Ensures the minimum total cost, reducing the likelihood of incorrect associations.\n- **Scalability:** Efficient for a reasonable number of objects, making it suitable for real-time applications.\n- **Flexibility:** Can incorporate additional factors like appearance similarity by adjusting the cost function.\n\n## 📈 Results\n\n### Single Object Tracking\n\n\u003CImage src={sot} alt=\"Single Object Tracking\" />\n\nThe alpha-beta filter effectively smoothed out the noise in the detected positions, resulting in a more accurate and stable trajectory of the vehicle. By adjusting the filter gains, we achieved a balance between responsiveness and smoothness, accommodating both steady motion and sudden maneuvers.\n\n### Multi-Object Tracking\n\n\u003CImage src={mot} alt=\"Multi-Object Tracking\" />\n\nThe implementation of the Hungarian algorithm for data association enabled reliable tracking of multiple vehicles simultaneously. The algorithm efficiently matched detections to existing tracks, even in challenging scenarios with occlusions and crossing paths. The combination of alpha-beta filtering for state estimation and optimal assignment through the Hungarian algorithm resulted in robust multi-object tracking performance.\n\n## 👥 Collaborators\n\n* None","src/content/projects/object-tracking.mdx",[105],"../../assets/images/projects/object-tracking/thumbnail.gif","f920658c3d6b92f6","object-tracking.mdx","plantify-dr",{"id":108,"data":110,"body":114,"filePath":115,"assetImports":116,"digest":118,"legacyId":119,"deferredRender":23},{"title":111,"cover":112,"githubUrl":113},"Plant Disease Detection App","__ASTRO_IMAGE_../../assets/images/projects/plantify-dr/thumbnail.png","https://github.com/lavaman131/PlantifyDr","import { Image } from \"astro:assets\";\n\nimport Resources from \"../../assets/images/projects/plantify-dr/report/Resources.png\";\nimport Flowchart from \"../../assets/images/projects/plantify-dr/report/Flowchart.jpg\";\nimport ResNet50 from \"../../assets/images/projects/plantify-dr/report/ResNet50.png\";\nimport SkipConnection from \"../../assets/images/projects/plantify-dr/report/SkipConnection.png\";\nimport CosineAnnealing from \"../../assets/images/projects/plantify-dr/report/CosineAnnealing.png\";\nimport TomatoTable from \"../../assets/images/projects/plantify-dr/report/Tomato_Table.png\";\nimport TomatoGraph from \"../../assets/images/projects/plantify-dr/report/Tomato_Graph.png\";\nimport TomatoConfusionMatrix from \"../../assets/images/projects/plantify-dr/report/Tomato_Confusion_Matrix.png\";\nimport TomatoResults from \"../../assets/images/projects/plantify-dr/report/Tomato_Results.png\";\n\n## Video Explanation & Demo\n\n\u003Ciframe \n  width=\"750\" \n  height=\"422\"\n  src=\"https://www.youtube.com/embed/uRbDXtMIHRk\" \n  title=\"PlantifyDr Demo\"\n  frameborder=\"0\" \n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" \n  allowfullscreen\n/>\n\n## Built With\n\n- [fastai](https://docs.fast.ai/) - A deep learning library written in Python based off of PyTorch\n- [Flask](https://flask.palletsprojects.com/en/1.1.x/) - Used as web framework for deployment\n- [Docker Compose](https://docs.docker.com/compose/) - Used to run application\n- [WSGI](https://wsgi.readthedocs.io/en/latest/what.html) - Web Server Gateway Interface to communicate with web application\n- [AWS](https://aws.amazon.com/) - For hosting ML models\n- [Swift](https://developer.apple.com/swift/) - For creating iOS app\n- [Postman](https://www.postman.com/) - For testing API\n\n\u003CImage src={Resources} alt=\"Resources\" />\n\n\n\n## Project Plan\n\n\u003CImage src={Flowchart} alt=\"Flowchart\" />\n\n## Model architecture\n\n- The power of convolutional neural networks (CNNs), specifically, `ResNet-50` allows computer to detect patterns in the pixels of images that humans cannot see.\n- As opposed to other ResNet variants, ResNet-50 provides compromise of additional model layers (50) while keeping model storage relatively low (~100mb).\n\n  \u003CImage src={ResNet50} alt=\"ResNet50\" />\n\n- Finally, skip connections helps mitigate vanishing gradient and model performance will perform at least as good as previous layer.\n\n  \u003CImage src={SkipConnection} alt=\"SkipConnection\" />\n\n## Deep Learning techniques used\n\n1. **Model training** with `Learning rate (LR) scheduler` using `cosine annealing` as opposed to more traditional LR scheduling from `lr_max/div` to `lr_max` where `div` is a number (100000.0 in my case & default for fastai library) for optimal learning rate for better training results\n\n2. **Fine tuning model** with `freeze` for `freeze_epochs` (transfer learning) then with `unfreeze` from epochs using `discriminative LR` (lower LR for earlier layers and greater LR for later layers)\n\n\u003CImage src={CosineAnnealing} alt=\"CosineAnnealing\" />\n\n## Results\n\nMy final models each achieved a validation `accuracy of >= 99.2%`.\n\n#### Here are my results for tomatos:\n\n\u003CImage src={TomatoTable} alt=\"TomatoTable\" />\n\u003CImage src={TomatoGraph} alt=\"TomatoGraph\" />\n\u003CImage src={TomatoConfusionMatrix} alt=\"TomatoConfusionMatrix\" />\n\u003CImage src={TomatoResults} alt=\"TomatoResults\" />\n\n#### Statistical Analysis:\n\n- The `Matthews correlation coefficient (MCC)` is in essence a correlation coefficient value between -1 and +1 commonly used in machine learning that considers class imbalances. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. It takes into consideration true and false positives and negatives. My model achieved a `MCC of 0.991`.\n- The `F1 score` can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. This is also helpful in considering class imbalances. My model achieved an `F1 score of 0.992`.\n- As shown in the train loss and valid loss columns and the Epochs vs Training and Validation graph my model has a `good fit` after `12 epochs` of training\n  - A `good fit` is identified by a training and validation loss that decreases to a point of stability with a minimal gap between the two final loss values.\n- Therefore, based on the previous statistics it can be inferred that the `validation accuracy` from my results is reliable to diagnose and treat plant diseases.\n\n## Experiment for yourself:\n\nIn your terminal run:\n\n```bash\npip install -r requirements.txt\n```\n\nGet the data from:\nhttps://www.kaggle.com/lavaman151/plantifydr-dataset \n\n## Points of Improvement:\n\n- In the future, I would like to add more plant types and diseases to my dataset.\n- Additionally, I want to add more features like plant nutritional deficiency recognition which can help provide insight into pesticide free and biological treatment of plants.\n- Something else I might have done differently was build my app in React Native or Flutter as it would have allowed me to bundle my iOS and Android app together which would be easier to do as the only developer.\n- Finally, I would like to implement and experiment with more state-of-the art models including XResNet and its variants for transfer learning on different datasets.\n\n## Final Words\n\nI hope my experiences building this app can shed some light into someone who is looking to get started on a similar ML project. This project was my first experience with deep learning and sparked my passion for computer vision. I loved every bit of the four months I spent on this project.\n\n## Get my app\n\nMy app was available previously on the App Store and Google Play Store, however, it is no longer available since I wasn't able to afford to renew my subscription and the cost of maintaining the services for the app was too high at the time.\n\n## 👥 Collaborators\n\n* None","src/content/projects/plantify-dr.mdx",[117],"../../assets/images/projects/plantify-dr/thumbnail.png","9957ddd46584ebe7","plantify-dr.mdx","responsible-ai-audit",{"id":120,"data":122,"body":126,"filePath":127,"assetImports":128,"digest":130,"legacyId":131,"deferredRender":23},{"title":123,"cover":124,"githubUrl":125},"Birdy: Responsible AI Audit of Twitter","__ASTRO_IMAGE_../../assets/images/projects/responsible-ai-audit/thumbnail.png","https://github.com/lavaman131/responsible-ai-audit","import { Image } from \"astro:assets\";\nimport fig1 from \"../../assets/images/projects/responsible-ai-audit/report/fig1.png\";\nimport fig2 from \"../../assets/images/projects/responsible-ai-audit/report/fig2.png\";\nimport fig3 from \"../../assets/images/projects/responsible-ai-audit/report/fig3.png\";\n\n## Fictitious Case Study\n\n### Background\n\nBirdy, a major social media platform, where users can post text up to a certain number of characters. It has also long faced criticism for its handling of hate speech. To address this, the platform launched CleanTalk, an AI-driven system designed to identify and remove hate speech content. Recognizing the importance of fairness and accuracy in content moderation, Birdy aimed to make CleanTalk a model of responsible AI deployment.\n\nCleanTalk was designed to analyze posts in real-time, flagging content deemed as hate speech for review or automatic removal. The system's effectiveness relied heavily on its training data - a vast collection of social media posts manually labeled by a diverse group of annotators from across the globe. Birdy believed that by leveraging a broad annotator base, CleanTalk could minimize bias and accurately reflect the varied understandings of what constitutes hate speech.\n\n### Deployment\n\nThe development of CleanTalk involved an extensive data collection and annotation phase. Birdy engaged with a wide range of annotators, including freelancers, NGO partners, and in-house teams, to label posts as hate speech or non-hate speech. The platform sought to ensure diversity among annotators in terms of demographic backgrounds, believing this would help capture a wide spectrum of perspectives on hate speech.\n\nUpon deployment, CleanTalk was hailed as a pioneering effort in AI moderation, capable of discerning complex patterns of hate speech while accommodating the nuances of different cultures and languages. The system was trained on millions of posts, with the underlying assumption that a diverse annotator pool would mitigate biases.\n\n### Failure\n\nDespite Birdy's efforts, CleanTalk soon encountered challenges. Users began to notice inconsistent moderation outcomes, where posts of a particular category were flagged and hate speech removed, while other offensive posts of a different category remained untouched. Some investigators began looking in the training data used by CleanTalk and found that it was disproportionately trained with white annotators and their ideas about hate speech. \n\n### Legal Action\n\nThe findings sparked a public outcry, leading to a high-profile class action lawsuit brought by plaintiffs (a group of users of Birdy) who felt they had been wrongly censored by Birdy. The plaintiffs alleged that Birdy, in using CleanTalk’s biased system, were unduly abridging their right to free speech. As part of discovery, the court appointed a third-party neutral auditing agency to further investigate CleanTalk and the initial allegations by users. The results of this audit, though perhaps not enough to hold Birdy liable or not liable, could help push the result of the class action in a particular direction. \n\n### Conclusion\n\nThe Birdy case study underscores the nuanced challenge of training AI models for complex tasks like hate speech detection. It illustrates that diversity in data annotation, while crucial, is not a panacea for bias. Ensuring fairness and accuracy in AI systems requires ongoing evaluation, transparency in methodology, and a commitment to understanding and mitigating the potential biases introduced at every stage of the AI development lifecycle. This case study also highlights the careful balancing act that systems like CleanTalk must endeavor to navigate: how do we decide what is considered hate speech and how do we prioritize user safety while also ensuring free speech?\n\n## Audit Context\n\n### Objectives\n\nBirdy must ensure fairness and accuracy in its content moderation to ensure that all users feel safe using the platform. Given the case study where it was noted that offensive posts of one category were flagged when posts of another category were not flagged, it is critical that Birdy considers legal and technical recommendations to reduce bias in its system amongst different kinds of users. Birdy also needs to find a way to balance interests with its role as a social media platform where content should be moderated versus freedom of expression.\n\n### Prospective Auditors\n\nOur prospective auditor would be a court-appointed neutral auditor. This makes the most sense in a class action context given that this is a role already used in such cases.\n\n### What/When of Audit\n\nWe are auditing Birdy’s content moderation model, which takes in a post and outputs whether or not it is offensive. We seek to understand how the demographic background of data annotators influences Birdy’s hate speech detection model. We aim to identify any bias in the data and in model development, with certain choices that were made. \n\nThe audit is conducted during the discovery phase of the class action lawsuit.\n\n### Methods, Tools, Metrics, and Standards\n\n1. Legal\n\n    a. Debates about content moderation consistently center on the question of who gets to decide what warrants being taken down. There may be certain areas of general consensus (for example, CSAM), but it becomes much less clear in the realm of hate/offensive speech. \n\n    b. In order to measure fairness, we’ll be specifically looking at the standard deviation of TPR based on the annotator’s demographics. A high standard deviation would therefore suggest that there is wide variety in the TPR between the different groups of annotators. This, compounded with the fact that 80% of the dataset is made up of white annotators, would give us reason to pause and consider the implications of enforcing a system that is so heavily skewed toward one perspective. \n\n2. Technical: Data collection\n\n    a. Social bias frames dataset has text paired with Y/N offensiveness annotations (no, neutral, yes) along with the demographic information about annotators originally collected from various online communities from a University of Washington study\n\n    b. Neutral labels were preserved to avoid flagging a non-offensive tweet → however it leads the model to be more uncertain so we considered neutral as an incorrect classification\n\n        \u003Col type=\"i\">\n\n            \u003Cli>Neutral labels act as a way to “regularize” the models predictions, allowing us to control unfamiliar examples\u003C/li>\n\n            \u003Cli>Consequently neutral predictions can directly be analyzed by human annotators to revise the model performance\u003C/li>\n\n        \u003C/ol>\n\n3. Data preprocessing\n\n    a. Data entries with missing labels were dropped from the dataset\n\n    b. Train and validation sets provided were merged and then split into demographic subsets\n\n    c. Each demographic subset was then split into an 80-20 percent train-validation split\n\n    d. Test set was kept as is and treated as untouched holdout set for further performance analysis\n\n4. For each demographic subset:\n\n    a. Fine-tuned pretrained Distil-Bert model to predict if text is offensive for 450 training steps, measuring train and validation performance\n\n5. Measure discrepancy between each model in training, validation, and holdout set via metrics of FP, TP, FN, TN, precision, recall, ROC-curve analysis.\n\n## Case Study Analysis\n\n### Stakeholders\n\n* Social Media Companies: want to more consistently moderate their hate speech using this system, flaws in the system could backfire on the social media companies & create new problem\n* Individual Users: tension & differing interests between users who (a) posted harmful content and (b) were exposed to harmful content and feel like they’ve been harmed \n* Civil Rights/Advocacy Groups: Many groups, like the ACLU, have advocated for social media companies to implement more specific hate speech policies. However, they’ve also pointed out that social media companies tend to remove posts by certain demographics over others – even if the content is the same.\n* State Legislators: Several states, such as Texas, have proposed legislation that prohibit social media companies from discriminating in their content moderation on the basis of viewpoint. They would likely prefer a more hands-off approach to content moderation where most things, unless truly dangerous or advocating for violence, are permitted. \n* EU: While this algorithm is based in the United States and will be subject to its laws, the EU might have an interest in encouraging Birdy to take more stringent measures to protect its online users. \n* Brands/Advertisers: Advertisers want to appeal to a broad audience and may not (for the sake of their brand) want to place ads on a platform they feel is hosting hate speech. One example of this is X. After Elon Musk endorsed an anti-semitic conspiracy theory on X, several major brands paused their activity on X.\n\n### Human Values\n\n* Freedom of Expression/Freedom of Speech\n\n    * In the United States (particularly when compared to other jurisdictions like the EU), we have a uniquely strong tradition of free speech. As seen in past court decisions (such as *Snyder v. Phelps*), we tend lean toward allowing speech even if it is blatantly offensive or harmful to certain groups. While the freedom of speech of users vis-a-vis the social media company isn’t a legal standard since these companies are private entities, it’s still a significant human value to consider. \n\n      * It is also incredibly difficult to create one general definition of hate speech. Each person will have their own unique interpretation of the term. Hence, freedom of expression also cautions us from over-moderating content and forcing one group’s idea of hate speech onto others. \n\n    * Content moderation decisions have also been interpreted as the social media’s editorial discretion. One current example of this is a recent decision by the Eleventh Circuit Court of Appeals in *NetChoice, LLC, et al. v. Attorney General* that struck down portions of a Florida law requiring social media platforms to moderate content neutrally. Hence, while bias may be found via this audit, the question still remains unclear regarding what can be done from a legal standpoint. \n\n* Fairness \n\n    * Content moderation systems inevitably have to decide what their definition of hate speech is. We care about fairness in content moderation regarding (a) the way that the rules are consistently/inconsistently applied and (b) who gets to decide what is hate speech. This audit will focus primarily on (b).\n  \n* Transparency\n    * This human value takes a bit of a backseat in the context of this audit, but it is nonetheless still important. Birdy’s users have a right to know how they are being moderated and Birdy has a responsibility to make this information accessible somewhere on its platform. This responsibility also extends to making users aware of CleanTalk and how CleanTalk works. While Birdy may have its rules of conduct explicitly written out, ensuring transparency also means explaining how CleanTalk enforces these rules and how it was trained to do so. This audit seeks to expose this and help make it known to the plaintiffs in the class action.\n  \n### Life Cycle of the System\n\nThe Machine Learning Life Cycle of CleanTalk involves the following procedures:\n\nThe system is deployed on Birdy’s social media platform via an API developed by CleanTalk that parses the text of users tweets using a machine learning model and returns the sentiment of the text.\n\nThe system is both monitored and controlled by parties by Birdy and CleanTalk. For purposes of transparency, Birdy makes information about their use of CleanTalk known in their terms and conditions as well as in their rules and policies. A team at Birdy occasionally monitors and reviews the posts that are flagged as toxic. Birdy ultimately has the final say on the system’s actual application to its users. CleanTalk developed the algorithm through a process of data collection and annotation. CleanTalk will oversee the actual training (and subsequent retraining) of its system. It has broader control over what text will be flagged as toxic and the sensitivity of its algorithm. \n\nThe system will be periodically retrained every six months by CleanTalk offline to stay up to date with new terminology, trends, or slang. This retraining process involves the following like a typical ML DevOps pipeline:\n\nA preprocessing algorithm will be applied to tweets that happened during the new period on Birdy’s platform. Additionally, to introduce diversity in data, a small portion of text will be scraped from openly accessible social media platforms. A sample of this processed data that is deemed to have a reasonable quality via a downstream algorithm will be assigned to annotators to identify the sentiment of the text.\n\nMultiple diverse annorators will label the same text for sentiment and this will be added to a retraining dataset. The diversity of both the annotators and the dataset is crucial to ensuring fairness. The existing model will be fine-tuned using state-of-the-art techniques in natural language understanding and standard evaluation metrics such as precision, recall, accuracy, etc. Part of the model’s training will involve supervised learning with the annotator’s data and ground truth label of the majority annotation.\n\nThe new ML model’s performance will be compared to the old model.\nAfter ensuring the new model’s performance is better than the old model according to previously stated evaluation metrics, the system will be accessible as an API endpoint to Birdy’s engineering team.\n\n## Audit Question / Issue\n\n### Question\n\nIs Birdy’s use of CleanTalk resulting in unfair and biased content moderation that unfairly abridged certain users’ freedom of expression?\n\n### Why This Matters\n\nThis question is important from a desire to protect the user’s freedom of expression while still maintaining a fair and safe online environment. Any content moderation system will inevitably run into the issue of users feeling like they were unfairly censored; there will never be one single definition of what constitutes hate speech, nor what the necessary response to hate speech should be. Thus, while we shouldn’t expect a perfect system, we should expect it to have the cultural nuance to balance the two aforementioned interests as best as possible. If we find wide discrepancies based on the annotator’s demographics, we have reason to be concerned.\n\n### Expected Findings\n\nFrom a preliminary assessment of the data used to train the system, we can already see a significant bias. 80% of the annotators are white. While this is probably reflective of other biases in the tech workforce rather than any intentional design choice, we expect that the audit will expose issues associated with this.  These discrepancies, notably discrepancies in the TPR rates between different groups of annotators, will point to an unfairness in the content moderation system that we should be concerned about.\n\n## Methods\n\n### Normative Question\n\nIs Birdy’s use of CleanTalk resulting in unfair and biased content moderation that unduly abridges certain group’s freedom of expression? (Also: Who gets to define hate speech? Is a super robust/strict content moderation system desirable?)\n\n* We need to contextualize this audit in a value-based framework centered on free speech, user safety and content moderation.\n    * Content moderation decisions by social media companies are interpreted, to some degree, by courts as them exercising their editorial discretion under the First Amendment. So, even if the audit does find bias or issues with their data collection, it’s not immediately obvious what can be done to rectify this or if anything should be done. (example: Florida law regulating content moderation on the basis of viewpoint was recently partly struck down by the Court of Appeals.)\n    * Bias is undesirable and we want a consistent content moderation system. However, we also have to balance this interest with the US’s strong free speech tradition. Thus, while high TPR’s demonstrate the accuracy of the system, users still may not agree that what is considered “toxic” is in fact toxic. While this issue may never be fully avoided (people will always disagree with what is considered toxic), at the very least the impact can be lessened by consistent TPR across different groups of annotators.\n\n### Technical Methods\n\n* Identify how model performance varies based on the annotator demographic that it is trained on → does this vary among demographics?\n* False positive rate vs true positive rate (ROC curve) to visualize model performance and choose threshold for considering a model biased\n* Analyze false positives (model is too strict on certain posts) and false negatives (model is not strict enough in enforcing moderation) and if it varies among demographics on the same test set\n* Based on above analysis, determine threshold for classifying model as unfair\n\n## Findings & Conclusions\n\n### Model Bias and Dataset Composition Analysis\n\n\u003Ch4>Definitions\u003C/h4>\n\n\u003Ch5>Model Bias ($\\beta$)\u003C/h5>\n\nFor our evaluation, we define model bias ($\\beta$) as the standard deviation of the true positive rate (TPR) of the model concerning a certain demographic. For example, the model shown in Table 1 was trained on a random subset of data and validated against separate test sets, with a $\\beta = 0.12$. \n\n\u003Cfigure>\n\u003Cdiv className=\"table-auto\" style={{ overflowX: 'auto' }}>\n  \u003Ctable style={{ minWidth: '100%', fontSize: '0.9em' }}>\n  \u003Ctr>\n    \u003Cth>Label\u003C/th>\n    \u003Cth>white_male_conservative\u003C/th>\n    \u003Cth>white_male_liberal\u003C/th>\n    \u003Cth>white_female_liberal\u003C/th>\n    \u003Cth>black_female_moderate_liberal\u003C/th>\n    \u003Cth>white_female_conservative\u003C/th>\n    \u003Cth>all\u003C/th>\n  \u003C/tr>\n  \u003Ctr>\n    \u003Ctd>TPR\u003C/td>\n    \u003Ctd>0.896\u003C/td>\n    \u003Ctd>0.915\u003C/td>\n    \u003Ctd>0.816\u003C/td>\n    \u003Ctd>0.584\u003C/td>\n    \u003Ctd>0.661\u003C/td>\n    \u003Ctd>0.912\u003C/td>\n  \u003C/tr>\n  \u003Ctr>\n    \u003Ctd>FNR\u003C/td>\n    \u003Ctd>0.104\u003C/td>\n    \u003Ctd>0.085\u003C/td>\n    \u003Ctd>0.184\u003C/td>\n    \u003Ctd>0.416\u003C/td>\n    \u003Ctd>0.339\u003C/td>\n    \u003Ctd>0.087\u003C/td>\n  \u003C/tr>\n  \u003Ctr>\n    \u003Ctd>TNR\u003C/td>\n    \u003Ctd>0.491\u003C/td>\n    \u003Ctd>0.489\u003C/td>\n    \u003Ctd>0.617\u003C/td>\n    \u003Ctd>0.747\u003C/td>\n    \u003Ctd>0.587\u003C/td>\n    \u003Ctd>0.478\u003C/td>\n  \u003C/tr>\n  \u003Ctr>\n    \u003Ctd>FPR\u003C/td>\n    \u003Ctd>0.509\u003C/td>\n    \u003Ctd>0.511\u003C/td>\n    \u003Ctd>0.383\u003C/td>\n    \u003Ctd>0.253\u003C/td>\n    \u003Ctd>0.413\u003C/td>\n    \u003Ctd>0.522\u003C/td>\n  \u003C/tr>\n\u003C/table>\n\u003Cfigcaption className=\"text-center mt-2 text-sm italic\">\n    Table 1\n\u003C/figcaption>\n\u003C/div>\n\u003C/figure>\n\n\u003Ch5>Fairness Metrics\u003C/h5>\n\nFairness is a comparative metric of bias, where:\n- Models with lower $\\beta$ are considered *more fair*\n- Models with higher $\\beta$ are considered *less fair*\n\n**Note**: Models are not considered 'fair' in absolute terms, rather, they are comparatively 'more fair' or 'less fair' than other models.\n\n\u003Ch4> Dataset Composition ($C$) \u003C/h4>\nWe define composition ($C$) as the percent makeup of a dataset regarding a certain demographic. For our dataset, the racial composition is:\n\n$$\nC = \\begin{bmatrix}\n\\text{White}: 83\\\\\n\\text{Asian}: 6\\\\\n\\text{Black}: 4\\\\\n\\text{Hispanic}: 4\\\\\n\\text{Native American}: 0\\\\\n\\text{Other}: 0\n\\end{bmatrix}\n$$\n\nThe dataset bias ($B$) is calculated as the standard deviation of $C$, giving us $B = 28.12$.\n\n\u003Ch4>Training Data Analysis\u003C/h4>\n\nFor our white male conservative demographic:\n$$\nC_{\\text{white\\_male\\_conservative}} = \\begin{bmatrix}\n\\text{White}: 100\\\\\n\\text{Asian}: 0\\\\\n\\text{Black}: 0\\\\\n\\text{Hispanic}: 0\\\\\n\\text{Native American}: 0\\\\\n\\text{Other}: 0\n\\end{bmatrix}\n$$\n\nThis results in $B = 37.27$.\n\n\u003CImage \n  src={fig1} \n  alt=\"Model performance across demographics\" \n  class=\"mx-auto\"\n/>\n\n\u003Cdiv className=\"flex flex-row gap-8 justify-center\">\n  \u003CImage \n    src={fig2} \n    alt=\"Validation annotator race\"\n    width={350}\n    height={150}\n  />\n\n  \u003CImage \n    src={fig3} \n    alt=\"Validation annotator political standing\" \n    width={350}\n    height={150}\n  />\n\u003C/div>\n\n### Performance Evaluation\n\nThe model performance shows:\n\n$$\\beta_{\\text{white\\_male\\_liberal}} > \\beta_{\\text{white\\_female\\_liberal}} > \\beta_{\\text{black\\_female\\_moderate\\_liberal}}$$\n\n\u003Ch4>Key Findings\u003C/h4>\n\n1. **Model Performance**: Models trained on white male data demonstrate superior performance compared to those trained on white female or black female data.\n\n2. **Dataset Bias Impact**: The high $B$ value in our dataset explains the poor performance of models trained on minority racial groups, as the dataset is skewed towards white people.\n\n### Content Moderation Implications\n\n\u003Ch4>Observed Biases\u003C/h4>\n\n- White male annotators show higher TPR rates\n- 80% of annotator dataset comprised of white males\n- Significant TPR disparities between demographics\n\n\u003Ch4>Challenges\u003C/h4>\n\n1. One-size-fits-all moderation standards prove problematic\n2. Risk of:\n   - Inaccurate content flagging\n   - Potential freedom of speech impediments\n   - System inconsistency\n\n\u003Ch4>Impact on User Engagement\u003C/h4>\n\n- Both strict and lenient moderation can create chilling effects\n- Marginalized groups face disproportionate online hostility\n- Risk of reduced participation in digital discourse\n\n## Recommendations\n\n### Section 230 (Policy Recommendations)\n\nSection 230 of the Communications Decency Act of 1996 is a fundamental law that shaped the Internet. It is a legal provision that protects Internet service providers from being sued for content published by third parties on their sites. It states: \"No provider or user of an interactive computer service shall be considered the publisher or speaker of any information provided by another information content provider\". This provision was introduced to encourage online companies to moderate their content without fear of being held responsible for anything posted by users. Article 230 also allows social platforms to moderate their content by deleting messages that do not comply with the platform's ethical standards, provided they act in \"good faith\".\nClearly defining what constitutes 'good faith' in the context of Section 230 is crucial. The notion of good faith is often open to interpretation and can be exploited in different ways. For example, a company that deliberately moderates a population category may be considered discriminatory. In another case, if an individual A is moderated for similar content for which an individual B is not, if the case is isolated, this could be attributed to an error or lack of attention. We could therefore assume that the company has not acted in \"bad faith\". However, if suspicious moderation is observed within the same platform, and the latter invokes vague arguments such as a lack of technical, financial or human resources allocated to moderation services, it would be essential to establish a clear audit process to remove any ambiguity about the notion of good faith. This could include financial audits to verify the real existence of a lack of resources, and ethical or internal audits to assess whether moderation is truly intentional. Defining this term more precisely would help Birdy comprehend the legal requirements around moderating content on its platform. Having an explicit understanding of what qualifies as acting in 'good faith' would allow Birdy to reduce the risk of lawsuits by verifying that its moderation practices conform to legal criteria.\n\n### Exploring Further Cultural Nuance (Birdy-specific recommendation)\n\nThe conclusion of this audit still has several gaps. For instance, we focus specifically on English text and say nothing about other languages. Second, the demographics are limited to race, gender, and political orientation but there is nothing about nationality. However, defining objectionable content is highly nuanced and can vary significantly across countries/cultures, even among English-speaking individuals. What may be considered an inoffensive joke in South Africa could be viewed as harassment in the United States. It is therefore recommended that Birdy conduct a comprehensive audit to align and assess how their use of CleanTalk seeks to balance these varying cultural nuances. \nSpecifically, the audit should articulate granular guidelines for discerning when constructive criticism crosses into a personal attack, when humor becomes a form of harassment or hate speech, and when repeated offensive behavior constitutes a violation of community standards. These threshold definitions should draw from established legal precedents as well as consultations with subject matter experts - including legal scholars, psychologists, and human rights specialists - to ensure the guidelines are fair, contextually appropriate, and consistent with ethical principles for the online environment. With well-defined policies aligned through a rigorous auditing process, platforms like Birdy can more effectively safeguard their user communities while upholding open and respectful dialogue.\n\n### Human-in-the-Loop / Procedural Justice\n\nIn tandem with a more balanced and diversified content moderation process, Birdy should implement a stronger human-in-the-loop, beyond just having someone occasionally review the posts that CleanTalk flags as toxic. Along with providing users the reasoning behind their content removal/flagging and the policies that have been violated, users should have access to clear-cut tools to be able to appeal the decision should they disagree with the decision. Many social media platforms, such as Instagram and more recently Snapchat, have integrated this function as a way to correct any possible shortcomings of the content moderation system. Moreover, the appeals process upholds the fundamental principle of free speech by allowing individuals to challenge censorship and have their voices heard in the digital public sphere. Doing so might also mitigate recidivism as offering an appeals process ensures procedural justice. Furthermore, integrating a stronger human-in-the-loop aspect not only enhances transparency and accountability but also ensures a more nuanced understanding of context, cultural sensitivities, and intent behind user-generated content. By offering users the opportunity to appeal decisions, platforms foster trust and empower users to participate in refining community standards, ultimately leading to a more inclusive and balanced online environment.\n\n### Technical\n\n\u003Ch4>Model Monitoring\u003C/h4>\n\n* Continuously monitor performance of model in production\n  * Introduce logging for number of posts blocked for demographic groups\n  * Measure user satisfaction of system via surveys\n  * Measure number of likes and dislikes for system\n* Recommend using multi-modal LMs to filter out text, image, and audio\n* Diversify training data\n  * Original training data was heavily imbalanced, with 80% of the annotators being white\n  * Any imbalance in training data directly affects performance of model → it is critical for Birdy to take proactive steps to ensure training data for content moderation is balanced among different annotator demographics\n  \n\u003Ch4>System Design\u003C/h4>\n\n1. When user clicks on a Birdy post, they can either label it as offensive or do nothing and Birdy can use this to further train the model → reinforcement learning human feedback\n   a. Periodically, ask user if this filter was effective.\n2. Demographic-based filtering, enabled by user through profile page\n3. Simple keyword filtering (non-machine learning approach for more fine-grained control of filtering by users) where users can specify certain content they do not want to see\n\n## Summary\n\nThe audit of CleanTalk reveals biased training data leads to biased models and moderation outcomes, while also highlighting the concerns about content moderation and freedom of speech. Exacerbated by an unbalanced dataset of predominantly white annotators, models trained on biased datasets perform worse in fairness metrics. The audit revealed that models trained on white male data perform much better than those trained on white female or black female data, highlighting a detrimental instance of data imbalance. Imbalance present in the training data and testing data reveals that a one-size-fits-all approach to content moderation poses a serious risk of flawed and inconsistent results. In light of this audit, we encourage CleanTalk to further diversify its training to data and implement a human-in-the-loop for appeals. Policymakers may also consider Section 230 of the Communications Decency Act and what exactly “good faith” content moderation looks like.\n\n## 👥 Collaborators\n\n* Amudha Sairam (UC Berkeley)\n* Arthur Clenet (UC Berkeley)\n* Rena Lu (UC Berkeley)\n* Safa Basravi (UC Berkeley)\n* Sam Wu (Boston University)","src/content/projects/responsible-ai-audit.mdx",[129],"../../assets/images/projects/responsible-ai-audit/thumbnail.png","63aa40b2f64fa324","responsible-ai-audit.mdx","sketch-n-solve",{"id":132,"data":134,"body":139,"filePath":140,"assetImports":141,"digest":143,"legacyId":144,"deferredRender":23},{"title":135,"cover":136,"githubUrl":137,"paperUrl":138},"Sketch-n-Solve","__ASTRO_IMAGE_../../assets/images/projects/sketch-n-solve/thumbnail.png","https://github.com/lavaman131/sketch-n-solve","https://arxiv.org/abs/2409.14309","## Abstract\n\nWe present Sketch 'n Solve, an open-source Python package that implements efficient randomized numerical linear algebra (RandNLA) techniques for solving large-scale least squares problems. While sketch-and-solve algorithms have demonstrated theoretical promise, their practical adoption has been limited by the lack of robust, user-friendly implementations. Our package addresses this gap by providing an optimized implementation built on NumPy and SciPy, featuring both dense and sparse sketching operators with a clean API. Through extensive benchmarking, we demonstrate that our implementation achieves up to 50x speedup over traditional LSQR while maintaining high accuracy, even for ill-conditioned matrices. The package shows particular promise for applications in machine learning optimization, signal processing, and scientific computing.","src/content/projects/sketch-n-solve.mdx",[142],"../../assets/images/projects/sketch-n-solve/thumbnail.png","0cef516c27c33d97","sketch-n-solve.mdx","reasoning-slms",{"id":145,"data":147,"body":151,"filePath":152,"assetImports":153,"digest":155,"legacyId":156,"deferredRender":23},{"title":148,"cover":149,"githubUrl":150},"DAPO: A Case Study with Small Language Models","__ASTRO_IMAGE_../../assets/images/projects/reasoning-slms/thumbnail.png","https://github.com/lavaman131/reasoning-slms","import { Image } from \"astro:assets\";\nimport all_ablations from \"../../assets/images/projects/reasoning-slms/report/all_ablations.png\";\nimport check_numbers from \"../../assets/images/projects/reasoning-slms/report/check_numbers.png\";\nimport check_answer from \"../../assets/images/projects/reasoning-slms/report/check_answer.png\";\n\n\n## Introduction\n\nOver the past year, we’ve seen reinforcement-learning-based fine-tuning methods supercharge large language models (LLMs), giving them the ability to “think through” problems step by step. Proximal Policy Optimization (PPO) got us started, then DeepSeek’s Group Relative Policy Optimization (GRPO) showed how you can skip a heavyweight critic network by comparing groups of sampled outputs. Building on that, **Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO)** took things further with:\n\n* **Clip-Higher Strategy**: separate clipping thresholds for “too small” and “too large” policy updates\n* **Dynamic Sampling**: drop prompts that give no learning signal (all-correct or all-wrong)\n* **Token-Level Loss**: compute gradients per token to avoid bias against long, detailed answers\n* **Overlong Filtering & Reward Shaping**: handle truncated or overly verbose responses more gracefully\n\nThose tricks unlocked better reasoning benchmarks on huge 30-50B-parameter models—so we asked ourselves: **can a 0.5B-parameter model learn the same tricks?** In this experiment, we applied DAPO to the lightweight [Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct) and ran ablations to see which pieces really matter when using a smaller language model. For the purposes of this exploration we considered small language models to be less than one billion parameters.\n\n---\n\n## Background\n\n### Proximal Policy Optimization (PPO)\n\nPPO is a workhorse policy-gradient algorithm that strikes a sweet spot between stability and simplicity. At each step, it:\n\n1. Samples a batch of trajectories\n2. Optimizes a *clipped* surrogate objective to keep policy changes small\n3. Uses only first-order gradients—no expensive second-order trust region calculations\n\nThis gives you reliable, DRL-style training without wrestling with hard constraints. PPO’s proved itself on robotics and Atari; we’re now using it to steer LLMs toward better answers.\n\n\u003Cdetails>\n\u003Csummary>🤓 Math highlight (clipped objective)\u003C/summary>\n\n$$\n\\mathcal{L}_{\\mathrm{PPO}}(\\theta) = \\mathbb{E}_t\\bigl[ \\min\\bigl(r_t(\\theta)\\hat A_t,\\;\\text{clip}(r_t(\\theta),1-\\epsilon,1+\\epsilon)\\hat A_t \\bigr)\\bigr],\n$$\n\nwhere $r_t(\\theta)=\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\rm old}}(a_t|s_t)}$.\n\n\u003C/details>\n\n---\n\n### Group Relative Policy Optimization (GRPO)\n\nThe DeepSeek paper introduces an innovative approach for enhancing reasoning capabilities in large language models using reinforcement learning, specifically focusing on Group Relative Policy Optimization (GRPO). Unlike conventional RL methods that rely on separate critic models for evaluating policy performance, GRPO reduces computational costs by estimating performance baselines directly from sampled group outputs.\n\nGRPO works by initially sampling multiple outputs from the existing policy model for each input query. The policy is then optimized by maximizing an objective function that compares the current policy outputs against these sampled outputs. This optimization process uses a clipped policy ratio to stabilize training, closely related to PPO, but notably avoids using an additional large-scale critic model. The optimization reward signals in GRPO are derived from both the accuracy and the correctness of the format, encouraging the model to provide correct answers and adhere to structured response formats.\n\n🔍 GRPO loss (high-level)\n\n1. Generate $G$ answers $\\{o_i\\}$ for each query\n2. Compute a “pseudo-advantage” $\\hat A_{i,t}$ by comparing each token’s reward to the group\n3. Apply a clipped update just like PPO, but **no separate critic** required\n\n\u003Cdetails>\n\u003Csummary>🤓 Math highlight GRPO Loss function\u003C/summary>\n\n$$\n\\begin{align*}\n\\mathcal{L}_{\\mathrm{GRPO}}(\\theta)\n&= -\\frac{1}{\\sum_{i=1}^G \\lvert o_i\\rvert}\n    \\sum_{i=1}^G \\sum_{t=1}^{\\lvert o_i\\rvert}\n    \\Biggl[\\,\n      \\min\\Bigl(\n        \\frac{\\pi_\\theta\\bigl(o_{i,t}\\mid q,\\,o_{i,\u003Ct}\\bigr)}\n             {\\pi_{\\theta_{\\mathrm{old}}}\\bigl(o_{i,t}\\mid q,\\,o_{i,\u003Ct}\\bigr)}\n        \\,\\hat{A}_{i,t}, \n        \\notag\\\\[-0.5ex]\n&\\quad\\quad\\quad\n        \\mathrm{clip}\\Bigl(\n          \\frac{\\pi_\\theta\\bigl(o_{i,t}\\mid q,\\,o_{i,\u003Ct}\\bigr)}\n               {\\pi_{\\theta_{\\mathrm{old}}}\\bigl(o_{i,t}\\mid q,\\,o_{i,\u003Ct}\\bigr)},\n          1-\\epsilon,\\;1+\\epsilon\n        \\Bigr)\\,\\hat{A}_{i,t}\n      \\Bigr)\n      \\;-\\;\\beta\\,D_{\\mathrm{KL}}\\!\\bigl[\\pi_\\theta\\;\\big\\|\\;\\pi_{\\mathrm{ref}}\\bigr]\n    \\Biggr]\n\\end{align*}\n$$\n\n\u003C/details>\n\nCompared to previous methods, GRPO demonstrates superior efficiency by substantially reducing the requirements and complexity of training resources. DeepSeek’s experimental results show significant improvements in reasoning tasks, outperforming conventional supervised fine-tuning methods. Models fine-tuned with GRPO exhibit strong reasoning capabilities, achieving benchmark performance comparable to industry-leading models like OpenAI's o1 series. Thus, GRPO represents a highly effective and resource-efficient approach for training advanced reasoning capabilities into LLMs.\n\n---\n\n### Decoupled Clip & Dynamic Sampling Policy Optimization (DAPO)\n\nDAPO keeps the GRPO spirit but adds four critical tricks for **stability** and **diversity**:\n\n1. **Clip-Higher Strategy**\n\n   * Use two epsilons ($\\epsilon_{\\rm low},\\epsilon_{\\rm high}$) so you don’t squash exploration too aggressively.\n\n2. **Dynamic Sampling**\n\n   * Skip prompts where all $G$ answers are identical (all right or all wrong). Pull fresh samples so gradients aren’t wasted.\n\n3. **Token-Level Loss**\n\n   * Instead of averaging at the sequence level (which underweights long, detailed answers), sum up per-token contributions.\n\n4. **Overlong Filtering & Reward Shaping**\n\n   * Mask or softly penalize answers truncated by token limits, and apply a gentle length penalty to discourage rambling without crushing good content.\n\n\u003Cdetails>\n\u003Csummary>🤓 Math highlight DAPO Loss function\u003C/summary>\n\n$$\n\\begin{aligned}\n\\mathcal{L}_{\\text{DAPO}}(\\theta) &= \\mathbb{E}_{(q,a) \\sim \\mathcal{D}, \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(\\cdot|q)} \\\\\n&\\quad \\left[ \\frac{1}{\\sum_{i=1}^G |o_i|} \\sum_{i=1}^G \\sum_{t=1}^{|o_i|} \\min \\left( r_{i,t}(\\theta) \\hat{A}_{i,t}, \\text{clip}(r_{i,t}(\\theta), 1 - \\epsilon_{\\text{low}}, 1 + \\epsilon_{\\text{high}}) \\hat{A}_{i,t} \\right) \\right] \\\\\n&\\text{s.t. } 0 \u003C |\\{o_i \\mid \\text{is\\_equivalent}(a, o_i)\\}| \u003C G,\n\\end{aligned}\n$$\n\n\u003C/details>\n\nTogether, these tweaks gave DAPO a clear win on the AIME 2024 benchmark, beating GRPO on a 32B-parameter model in half the steps. We wanted to see if those benefits survive down-sizing.\n\n---\n\n### Dr. GRPO – A “Done Right” Revision\n\nDr. GRPO refines GRPO by removing two subtle biases:\n\n* **Length bias**: penalizing longer correct answers\n* **Difficulty bias**: skew from question hardness\n\nIt replaces group-normalized rewards with a simple \"subtract the group mean\" advantage:\n\n$$\n\\tilde A_{i,t} = R(q,o_i) \\;-\\; \\frac{1}{G}\\sum_{j=1}^G R(q,o_j).\n$$\n\nThis brings GRPO closer to vanilla PPO principles—but still without a standalone critic.\n\n---\n\n## Our Setup\n\n### Integrating DAPO into TRL\n\nWe used the [TRL library](https://github.com/lvwerra/trl), the de-facto RL-for-LLM toolkit, and added DAPO’s algorithms as new methods. Since there was no proper implementation of DAPO on TRL, we implemented a basic version of DAPO in the library. This let us spin up ablations by toggling each feature in a few lines of code.\n\n### Dataset: GSM8K\n\nWe fine-tuned on the [OpenAI Grade School Math 8K (GSM8K)](https://huggingface.co/datasets/openai/gsm8k) dataset. It is a dataset with 8.5K diverse word problems requiring multi-step reasoning. Below is an example of the structure of an instance in the dataset.\n\n```json\n{\n    \"question\": \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\",\n    \"answer\": \"Natalia sold 48/2 = $\u003C\u003C48/2=24>>24$ clips in May. Natalia sold 48+24 = $\u003C\u003C48+24=72>>72$ clips altogether in April and May. 72\"\n}\n```\n\n### Reward Functions\n\nWe combined:\n\n* **Exact-answer reward** (big bonus if final answer matches)\n* **Format reward** (smaller bonus for correct JSON/LaTeX-style structure)\n* **Partial credits** for close or partially correct formats\n* **Soft length penalties** if needed (overlong shaping)\n\n---\n\n## Ablation Studies\n\nWe ran 250-step experiments on Qwen2.5-0.5B. Starting from a **base GRPO** setup, we flipped one switch at a time. The experiments can be viewed [here](https://api.wandb.ai/links/artificial-intelligence-research/3nc1wr0u):\n\n| Method                      | Reward |\n| --------------------------- | :----: |\n| Base GRPO                   |  7.63  |\n| + Overlong Filtering        |  8.36  |\n| + Clip-Higher               |  7.19  |\n| + Soft Overlong Punishment  |  5.03  |\n| + DAPO Token-Level Loss     |  5.45  |\n| + Dr. GRPO Token-Level Loss |  8.00  |\n| + Disable Reward Scaling    |  8.13  |\n| + Remove KL Divergence      |  9.16  |\n| + Dynamic Sampling          |  4.58  |\n\n### DAPO (BNPO) Token-Level Loss\n\nAfter establishing a baseline, we ran ablation experiments one by one with each of our methods. The first method implemented was DAPO Token-Level loss. While GRPO's loss is sequence-level, this loss is token-level. Token-level policy loss calculates gradients per token across a batch, unlike sample-level loss which averages per response and can underweight tokens in longer sequences. This approach ensures fairer gradient contributions regardless of response length, improving training stability and providing healthier control over output length dynamics in long-CoT scenarios.\n\n\n###  Dr. GRPO Token-Level Loss\n\nIn this experiment we changed the loss function to the Dr. GRPO Loss function. This approach removes the length and biases inherent in the original GRPO objective, providing unbiased policy gradients and improving token efficiency by mitigating artificial response length increases.\n\n### Disable Reward Scaling\n\nIn this experiment we disabled the scaling of rewards by the reward standard deviation, as it has shown to introduce bias. This method was suggested in the Dr. GRPO implementation. The Dr. GRPO paper integrates this with their loss function but we are testing them separately.\n\n\n### Clip-Higher Strategy\n\nThe first method implemented was the Clip-Higher strategy. Inside the GRPO loss function, the clip function's epsilon value is decoupled into epsilon-low and epsilon-high. If epsilon-high is increased, then this increases the probability for low-probability token samples to be selected. This results in the increased diversity of samples.\n\n### Overlong Filtering\n\nFor this experiment we implement Overlong Filtering. The rewards for all truncated responses are masked. This prevents confusing the model by penalizing a sound reasoning process because the response is too long.\n\n### Soft Overlong Punishment\n\nFor this experiment we implement a Soft Overlong Punishment to the reward. Consider a punishment threshold $L_{cache}$. This threshold determines how close to the maximum completion length (number of tokens) a response can be before it is penalized with a soft punishment. Once the response length exceeds the maximum completion length, a hard reward penalty of -1 is applied.\n\n### Dynamic Sampling\n\nFor this experiment we implement Dynamic Sampling as it is defined in the DAPO paper. If all of a response's tokens are the same value, then they are filtered out and replaced with a new sample. This helps filter out responses where the model is overconfident on or completely lost on. These responses are replaced with responses where the model can learn more constructively.\n\n### Remove KL Divergence Penalty\n\nFor this experiment we removed the KL Divergence penalty term. During training of reasoning models, the model distribution may diverge significantly from the initial model, and thus the term may not be necessary.\n\n> **Key takeaways**:\n>\n> * **Dr. GRPO loss**, **Overlong Filtering**, **no KL penalty**, and **no reward scaling** all boosted rewards.\n> * DAPO's clip-higher, soft overlong punishment, token-level loss, and dynamic sampling actually hurt on this tiny model.\n\n\u003Cfigure>\n  \u003CImage src={all_ablations} alt=\"Reward curves for all ablations\" />\n  \u003Cfigcaption>\u003Csmall>Reward curves after 250 steps: baseline GRPO vs. each ablation.\u003C/small>\u003C/figcaption>\n\u003C/figure>\n\n\n---\n\n## Final Configuration vs. Baseline\n\nOur **final recipe** for Qwen2.5-0.5B:\n\n* **Loss:** Dr. GRPO token-level\n* **Techniques enabled:** Overlong Filtering\n* **Techniques disabled:** Reward Scaling, KL penalty, Clip-Higher, Dynamic Sampling, Soft Overlong Punishment\n\nCompared to vanilla GRPO:\n\n\u003CImage src={check_numbers} alt=\"Numerical correctness rewards\" />\n\u003CImage src={check_answer} alt=\"Format correctness rewards\" />\n\n* **Numerical accuracy** improved steadily.\n* **Format correctness** saw an even bigger lift—a sign our tweaks help the model stay \"on-script\" with answer formatting.\n\n---\n\n## Results\n\nHere is a test question:\n\n```\nQuestion: John runs a telethon to raise money. For the first 12 hours,\nhe generates $5000 per hour. The remaining 14 hours,\nhe generates 20% more per hour. How much total money does he make?\n\nAnswer: 144000\n```\n\n\nHere is the full reasoning-style answer output by the model:\n\n```\nResponse:\n\u003Cstart_working_out>\n    John generates $5000 per hour for the first 12 hours,\n    so that’s $5000 × 12 = $60000.\n    For the remaining 14 hours, 20% more per hour means\n    $5000 × 20% = $1000 extra, so $5000 + $1000 = $6000 per hour.\n    Over 14 hours that’s $6000 × 14 = $84000.\n    In total: $60000 + $84000 = $144000.\n\u003Cend_working_out>\n\u003CSOLUTION>144000\u003C/SOLUTION>\n```\n\n## Discussion\n\n1. **Dr. GRPO is surprisingly powerful** on small models—removing that sequence-length bias really matters.\n2. **Overlong Filtering** prevents punishing good reasoning just because you hit a token cap.\n3. DAPO’s fancy token-level loss and dynamic sampling might be overkill (or need retuning) when parameters are limited.\n4. We suspect that tiny models benefit more from bias-fixing and stability tweaks than from aggressive exploration strategies.\n\n### Instruction Tuning Is Crucial\n\nWe found that the quality of the base instruction-tuned model shapes everything downstream. For instance, when we tried Google’s Gemma 3 1B (with a lighter instruction-tuning regimen), fine-tuned outputs often derailed into gibberish by the end of the reasoning trace. In contrast, the Qwen family—already robustly instruction-tuned on reasoning datasets—delivered crisp, human-readable step-by-step answers right out of the gate. This suggests tiny-model DAPO is only as good as your starting point.\n\n### Precision & Attention Kernels\n\nWe also observed that mixed-precision (bfloat16 vs. fp16) and attention implementation (SDPA vs. FlashAttention2) can make or break training stability—worth tuning alongside your RL tricks.\n\n---\n\n## Conclusion & Future Work\n\n* **Small-scale DAPO** isn’t just a “big-model only” party. With the right subset of techniques (Dr. GRPO, overlong filtering, no KL/reg scaling), you can teach a 0.5B-parameter model to reason effectively.\n* **Next steps**: compare against [Group Policy Gradient (GPG)](https://arxiv.org/abs/2504.02546), a minimal RL objective for LLMs, and explore curriculum scheduling on prompt difficulty.\n\n---\n\nFeel free to clone, tinker, and let us know what works—or doesn’t—at small scale!\n\n\n## 👥 Collaborators\n\n* Zach Gentile (Boston University)","src/content/projects/reasoning-slms.mdx",[154],"../../assets/images/projects/reasoning-slms/thumbnail.png","d2850f02fc0b400a","reasoning-slms.mdx"]