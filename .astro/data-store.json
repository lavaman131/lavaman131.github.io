[["Map",1,2,9,10,169,170],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.15.1","content-config-digest","777926a0dabe1ee9","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"http://localhost:3000\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":3000,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":false,\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[null],\"rehypePlugins\":[null,[null,{\"themes\":[\"catppuccin-latte\",\"catppuccin-mocha\"],\"useDarkModeMediaQuery\":true,\"styleOverrides\":{}}]],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false},\"legacy\":{\"collections\":false}}","projects",["Map",11,12,24,25,36,37,48,49,60,61,72,73,84,85,96,97,108,109,120,121,132,133,144,145,156,157],"cartoonify",{"id":11,"data":13,"body":17,"filePath":18,"assetImports":19,"digest":21,"legacyId":22,"deferredRender":23},{"title":14,"cover":15,"githubUrl":16},"Cartoonify: Cartoon Style Transfer + Generation","__ASTRO_IMAGE_../../assets/images/projects/cartoonify/thumbnail.png","https://github.com/lavaman131/cartoonify","import { Image } from \"astro:assets\";\nimport loss from \"../../assets/images/projects/cartoonify/report/loss.png\";\nimport eval1 from \"../../assets/images/projects/cartoonify/eval/1.png\";\nimport eval2 from \"../../assets/images/projects/cartoonify/eval/2.png\";\nimport eval3 from \"../../assets/images/projects/cartoonify/eval/3.png\";\nimport eval4 from \"../../assets/images/projects/cartoonify/eval/4.png\";\nimport eval5 from \"../../assets/images/projects/cartoonify/eval/5.png\";\nimport eval6 from \"../../assets/images/projects/cartoonify/eval/6.png\";\nimport eval7 from \"../../assets/images/projects/cartoonify/eval/7.png\";\nimport eval8 from \"../../assets/images/projects/cartoonify/eval/8.png\";\nimport dataCollection from \"../../assets/images/projects/cartoonify/report/data_collection.png\";\nimport results from \"../../assets/images/projects/cartoonify/report/results.png\";\n\n## Introduction\n\nThe main objective was to apply style transfer in the context of diffusion models to generate more convincing â€œDisneyâ€-style images from source images of people. To achieve this, we needed a deep understanding of the diffusion modelâ€™s architecture. The particular diffusion model employed in our project integrates a Variational Autoencoder (VAE), a U-Net architecture, and the CLIP (Contrastive Language-Image Pretraining) model.\n\n## Methodology\n\nWe used the latent diffusion model: stable-diffusion-v1.5. First, we collected the images of modern Disney characters, animals/objects, and landscapes. We followed DreamBooth, a training technique that updates the entire stable diffusion model with a few curated images. Since we chose to train the model with DreamBooth technique, we did not need a large dataset. In total, we collected around 200 images from Youtube, Google, and Pinterest. They were resized to 512 x 512. For higher quality, the background of images had to be white or black.\n\n\u003CImage src={dataCollection} alt=\"dataCollection\" />\n\nAfter collecting the data, we tried to train the model and text encoder with prior preservation. Prior preservation uses additional images of the same class we are trying to train as part of the fine-tuning process (regularization). Before training the model, we set approximately 40 parameters in advance. The specifics of the training parameters are as follows:\n\n* Pre-trained Model: We utilized the 'stable-diffusion-v1-5' model from RunwayML as our starting point, with the 'main' revision indicating the use of the latest model variant.\n* Variation and Precision: The model variant 'fp16' was selected for training, which uses 16-bit floating-point precision. This choice was instrumental in optimizing the training speed and memory usage without significantly compromising the model's performance.\n* Validation Parameters: The validation prompt was set to \"a person, Disney style\" to guide the model in generating images that match this theme. Four images were used for validation purposes to ensure that the style was being captured accurately.\n* Training Data: The training dataset located in the '/data/disney' directory was complemented with a class-specific dataset from '/data/style_ddim', intended to imbue the model with a deeper understanding of the distinctive style features.\n* Image Configuration: A total of 1,000 class images were provided to the model, with a prior loss weight of 1.0 to ensure that the original style attributes were preserved. The resolution was fixed at 512 pixels, and center cropping was employed to maintain focus on the central elements of the images.\n* Training Procedure: To encode the text prompts effectively, the model's text encoder was included in the training loop. The training was conducted in batches of size four over a single epoch, capped at a maximum of 4,000 steps to prevent overfitting.\n* Optimizer: We implemented an 8-bit Adam optimizer to conserve memory while maintaining a learning rate of 1e-6, constant throughout the training. Beta coefficients for the Adam optimizer were set to 0.9 and 0.999, with a weight decay of 1e-2 and epsilon value of 1e-8 for numerical stability.\n* Regularization and Gradient Control: The maximum norm for gradients was limited to 1.0 during backpropagation to prevent exploding gradients.\n* Tracking and Precision: Model checkpoints were created every 500 steps, and training logs were recorded in the 'logs' directory. Mixed precision training was enforced with 'fp16' precision for both the model's prior generation and validation steps to align with the selected training variant.\n* Schedulers: A 'PNDMScheduler' was used for validation, ensuring that learning rates were adjusted appropriately at each step of the validation process.\n* Reproducibility: A random seed value of 42 was used to initialize the training process, which ensures reproducibility of results across different training sessions.\n\nEach of these parameters was chosen with careful consideration of both the computational efficiency and the specific stylistic goals of the project. The train script can be found [here](https://github.com/lavaman131/cartoonify/blob/main/tools/train_dreambooth.sh).\n\n## Results\n\nSince the outputs of the model are images, valid quantitative evaluations are ambiguous. For quantitative evaluations, we decided to rate the images from each checkpoint and see if there has been improvement of image generation quality. The ratings range from 1 (poor quality) to 10 (great quality). Note, in the third set of images, an entirely black image is triggered by the Stable Diffusion image safety checker model. Therefore, we did not evaluate the particular image.\n\n\u003CImage src={eval1} alt=\"eval1\" />\n\u003CImage src={eval2} alt=\"eval2\" />\n\u003CImage src={eval3} alt=\"eval3\" />\n\u003CImage src={eval4} alt=\"eval4\" />\n\u003CImage src={eval5} alt=\"eval5\" />\n\u003CImage src={eval6} alt=\"eval6\" />\n\u003CImage src={eval7} alt=\"eval7\" />\n\u003CImage src={eval8} alt=\"eval8\" />\n\nThroughout the training process, we evaluated the generated images at intervals of 500 steps to assess their quality. The average scores at these intervals were calculated to track the progression of image quality. At 500 steps, the initial images received an average rating of 3. Subsequent assessments showed improvement, with average ratings rising to 3.5, then peaking at 8.625 after 3500 steps, which marked the highest image quality during our experiments. Notably, after 3500 steps, a slight regression was observed, culminating in an average rating of 5.375 by the end of 4000 steps. We also include hand-picked images from the final checkpoint in the [Supplementary Material](#supplementary-material) section.\n\nThese observations suggest that while there was an improvement in the quality of the generated images throughout most of the training, a decrease in quality was noted towards the end. This trend underscores the complex relationship between training steps and image quality, indicating a possible overfitting or a need for adjustment in the training parameters beyond the 3500-step mark.\n\nAnother metric that we check is the cross entropy loss during the training process. As you can see below, the loss fluctuates quite a bit which may be expected for such a small dataset on the order of hundreds of samples.\n\n\u003CImage src={loss} alt=\"loss\" />\n\n## Conclusion\n\nThe exploration of DreamBooth's training capabilities revealed a number of critical insights, particularly relating to its sensitivity to hyperparameters and the potential for overfitting. It was discovered that a low learning rate was paramount for stable convergence, while the training duration needed to be proportional to the size of the dataset to avoid overfitting while still capturing sufficient details. The incorporation of prior preservation, especially when training on nuanced subjects like faces, proved to be crucial for maintaining the integrity of generated images. The refinement of the text encoder emerged as a significant factor that influenced the final output quality. This fine-tuning, when carried out with the adjustments to the U-Net model, led to substantial improvements in the fidelity of the generated images. Dedicated prompt engineering was another critical element, substantially impacting the relevance and specificity of the generated images. Careful crafting of prompts ensured that the model's outputs more accurately reflected the desired themes and styles. Furthermore, experimentation with various noise schedulers like Euler A highlighted the importance of these components in the image synthesis process. In our specific case, the Euler A noise scheduler was particularly effective, enhancing the model's ability to produce high-quality images.\n\nIn conclusion, the process of training DreamBooth requires a delicate balance between a multitude of parameters and training nuances. Our findings underscore the importance of a methodical approach to training parameter configuration, which  can lead to remarkable results that push the boundaries of generative models when executed with precision.\n\n## References\n\n* \"Training Stable Diffusion with Dreambooth using ðŸ§¨ Diffusers.\" https://huggingface.co/blog/dreambooth \n* Mishira, Onkar. \"Stable Diffusion Explained.\" https://medium.com/@onkarmishra/stable-diffusion-explained-1f101284484d \n* Ruiz, Nataniel et al. \"DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation.\" https://arxiv.org/pdf/2208.12242\n\n## Supplementary Material\n\n\u003CImage src={results} alt=\"results\" />\n\n## ðŸ‘¥ Collaborators\n\n* Yewon Lee (Boston University)","src/content/projects/cartoonify.mdx",[20],"../../assets/images/projects/cartoonify/thumbnail.png","9a99aeeed29fbadc","cartoonify.mdx",true,"dinov2cpp",{"id":24,"data":26,"body":30,"filePath":31,"assetImports":32,"digest":34,"legacyId":35,"deferredRender":23},{"title":27,"cover":28,"githubUrl":29},"DINOv2.cpp: Accelerating Edge Inference with C++ and GGML","__ASTRO_IMAGE_../../assets/images/projects/dinov2cpp/thumbnail.jpg","https://github.com/lavaman131/dinov2.cpp","import { Image } from \"astro:assets\";\nimport tench from \"../../assets/images/projects/dinov2cpp/report/tench.jpg\";\nimport tench_pca_registers from \"../../assets/images/projects/dinov2cpp/report/tench-pca-registers.jpg\";\nimport tench_pca from \"../../assets/images/projects/dinov2cpp/report/tench_pca.jpg\";\n\n\n## Introduction\n\nEdge computing for AI involves the deployment of AI algorithms on local edge devices and sensors, which enables a wider range of AI use cases and provides substantial benefit. Edge computing allows for AI algorithms to be run locally on devices with limited hardware, ensuring devices do not need to be connected to the cloud for real-time inference. Some benefits provided include reduced memory and energy use, decreased latency, and faster compute. These also provide privacy benefits as data can be stored locally. Use cases include real-time processing for autonomous vehicles, analytics for security devices, and improved sensor understanding for robotics.\n\nThe Vision Transformer (ViT) architecture has achieved state-of-the art results on a wide range of traditional computer vision problems, pushing the capabilities of image analysis [Dosovitskiy et al., 2021](https://arxiv.org/abs/2010.11929). Among recent innovations, the DINOv2 family of vision transformer models achieves competitive results in classification, depth estimation, and semantic segmentation tasks. Developed by Meta, pretrained weights and model architectures have been made publicly available on GitHub [Oquab et al., 2024](https://arxiv.org/abs/2304.07193). Despite their improved performance, ViT models are more computationally expensive than convolutional neural networks. The ViT self-attention mechanism scales quadratically with input size, leading to increased memory usage and slower inference times. As a result, deployment of ViTs on systems with low resources can prove challenging and can fail to provide competitive speeds.\n\nWe introduce DINOv2.cpp, an inference engine for the DINOv2 family of models for use on edge devices. Written in C++ with minimal dependencies, our lightweight implementation achieves lower memory use and improved inference speeds on all models.\n\n## Related Work\n\n### vit.cpp [Taghadouini et al., 2024](https://github.com/staghado/vit.cpp)\n\nOur work is inspired by vit.cpp, a C++ implementation of the ViT inference engine using the ggml library.\n\n| Model | Max Mem (PyTorch) | Max Mem | Speed (PyTorch) | Speed   |\n| ----- | ----------------- | ------- | --------------- | ------- |\n| tiny  | ~780 MB           | ~20 MB  | 431 ms          | 120 ms  |\n| small | ~965 MB           | ~52 MB  | 780 ms          | 463 ms  |\n| base  | ~1.61 GB          | ~179 MB | 2393 ms         | 1441 ms |\n| large | ~3.86 GB          | ~597 MB | 8151 ms         | 4892 ms |\n\n*Table 1. Comparison of peak memory and inference speed across model sizes.*\n\n### DINOv2 [Oquab et al., 2024](https://arxiv.org/abs/2304.07193)\n\nDINOv2 is a self-supervised vision model that trains Vision Transformers on a massive 142 million-image dataset. It combines two key objectives: aligning global representations through class-token loss and fine-graining features via masked patch-token loss. The model also integrates techniques like Sinkhornâ€“Knopp for clustering and KoLeo regularization for uniform feature distribution. Additionally, DINOv2 includes a high-resolution adaptation stage and produces dense patch embeddings to capture local image semantics.\n\n## Approach\n\nWe build on the existing vit.cpp codebase, updating and refactoring the implementation as required to support DINOv2. Steps taken involved writing a script to convert the model to GGUF format, loading model weights into ggml, rewriting the forward pass to support new layers, and updating quantization support. In addition to these steps, we upgraded the GGML version for the base implementation of vit.cpp. Following the initial implementation of the inference pipeline, we added support for realtime visualization of extracted features, flash attention, and positional embedding interpolation.\n\n### GGUF Format [Gerganov et al., 2025](https://github.com/ggml-org/ggml)\n\nGGUF is a binary format that is optimized for quick loading and saving of models, making it highly efficient for inference purposes. GGUF is designed for use with GGML and other executors. A Python script is provided in vit.cpp to download a ViT model from HuggingFace and then convert it to a deprecated GGML format. We rewrote this script to account additional layers present in DINOv2, such as the register tokens, and to use the updated GGUF format.\n\n### Correcting for the DINOv2 Architecture\n\nAdapting the vit.cpp pipeline involved rewriting the model loading and forward pass to handle the modified DINOv2 architecture. Memory must be manually allocated for each tensor and account for the variable size of different models. The GGUF file format provides metadata on the loaded model architecture allowing the inference pipeline to account for different model sizes and architectures. The layer structure of the model is detailed in Figure 1.\n\n```text\nProcessing variable: dinov2.embeddings.cls_token with shape:  torch.Size([1, 1, 384])  and type:  torch.float32\n...\nProcessing variable: classifier.bias with shape:  torch.Size([1000])  and type:  torch.float32\n````\n\n*Figure 1. An example of DINOv2's model architecture information from our GGUF conversion script.*\n\n### Visualizing DINOv2's Feature Extraction\n\ndinov2.cpp supports parameters to enable classification or feature extraction and visualization. When visualizing features, the classification head is removed and the learned feature representation of the image will be output. To produce a visualization of features, principal component analysis (PCA) is run on the last layer of weights. OpenCV is used to convert and save the resulting image. Figure 2 shows an example of this visual feature representation.\n\n\u003Cdiv class=\"flex justify-between gap-x-4\">\n  \u003Cfigure class=\"flex-1\">\n    \u003CImage src={tench} alt=\"Original Image\" />\n    \u003Cfigcaption class=\"text-center\">\u003Csmall>Original Image\u003C/small>\u003C/figcaption>\n  \u003C/figure>\n\n  \u003Cfigure class=\"flex-1\">\n    \u003CImage src={tench_pca_registers} alt=\"PCA Features (No Registers)\" />\n    \u003Cfigcaption class=\"text-center\">\u003Csmall>PCA Features (No Registers)\u003C/small>\u003C/figcaption>\n  \u003C/figure>\n\n  \u003Cfigure class=\"flex-1\">\n    \u003CImage src={tench_pca} alt=\"PCA Features\" />\n    \u003Cfigcaption class=\"text-center\">\u003Csmall>PCA Features\u003C/small>\u003C/figcaption>\n  \u003C/figure>\n\u003C/div>\n\n*Figure 2. Left: Original. Middle: PCA Features without Register Tokens. Right: PCA Features with Register Tokens.*\n\n### Quantization\n\nWe adapted the ViT quantization script to support DINOv2. Table 2 details the different quantization formats supported with DINOv2.\n\n| Format | Description                                                         |\n| ------ | ------------------------------------------------------------------- |\n| 4\\_0   | 4-bit round-to-nearest quantization (q). Each block has 32 weights. |\n| 4\\_1   | 4-bit round-to-nearest quantization (q). Each block has 32 weights. |\n| 5\\_0   | 5-bit round-to-nearest quantization (q). Each block has 32 weights. |\n| 5\\_1   | 5-bit round-to-nearest quantization (q). Each block has 32 weights. |\n| 8\\_0   | 8-bit round-to-nearest quantization (q). Each block has 32 weights. |\n\n*Table 2. Supported quantization formats*\n\n### Additional Features\n\n**Flash Attention:**\nTo improve inference speed and reduce memory usage we implemented flash attention within the DINOv2 pipeline. Flash attention computes memory in a much more memory-efficient manner using kernel fusion techniques. This can be optionally toggled to maximize performance on CPU.\n\n**Positional Embedding Interpolation:**\nWhen training, Vision Transformers rely on a fixed size positional embedding. However during inference, input image size may vary, especially as models are implemented on different devices. The interpolation process involves resizing the learned positional embeddings to match the number of patches generated by the new input resolution. This enables the model to generalize to arbitrary input size.\n\n**Cross-Platform Support:**\nWe have tested dinov2.cpp on Windows, Linux, and Mac and have ensured that the inference pipeline works cross-platform.\n\n## Datasets\n\nThe original DINOv2 family of models was trained on the LVD-142M dataset, a curated set of 142 million images assembled from ImageNet-22k, Google Landmarks, and other fine-grained datasets. A self-supervised pipeline was developed for training, capturing features on the image and pixel level. The diverse dataset and self-supervised pipeline enables a wide degree of possible implementation in computer vision tasks without any finetuning. Existing tasks include classification, semantic segmentation, depth estimation, instance retrieval, and video understanding.\n\nDINOv2.cpp retains these capabilities. Publicly available weights can be converted to the gguf format without any loss of accuracy. The inference pipeline supports images up to sizes of 518 Ã— 518 pixels and automatically resizes input images, providing support for any image-based dataset. The diversity and size of the dataset used to train DINOv2 ensure that dinov2.cpp can support a wide range of real-time tasks in a variety of environments.\n\n## Evaluation Results\n\nWe evaluate our results on inference speed and memory usage. When compared to DINOv2â€™s PyTorch implementation, we aim to achieve faster inference speeds while decreasing memory usage. To compare our results, we ran four benchmarks. First, we have two benchmarks without quantization. For these benchmarks, we run the small, base, large, and giant models 100 times and measure the average inference speed and memory usage. This is done for the model variants with register tokens and without register tokens. We also ran two benchmarks for quantized models. For each model we benchmark the 4\\_0, 4\\_1, 5\\_0, 5\\_1, and 8\\_0 formats across 100 inference runs each. We run this for the model variants small, base, large, and giant, each with and without register tokens. Each experiment was run using Intel Core i9-14900HX, using 24 of its 32 threads for best results.\n\nOur benchmark results, detailed in Figure 3, show universal improvements in inference speed and memory use when compared to PyTorch. Our most notable improvement was with the small variant with register tokens: speed from 297 ms â†’ 64 ms, memory from 455 MB â†’ 110 MB.\n\nOur quantization benchmark results, detailed in Figure 4, show universal improvements in inference speed and memory use when compared to the non-quantized models run in our pipeline. Our most notable improvement can be seen with the giant model with register tokens: speed from 1995 ms â†’ 1281 ms, memory from 4400 MB â†’ 1281 MB with quantization type 4\\_0.\n\n### Benchmark Results\n\n#### Without Register Tokens\n\n| Model | Max Mem (PyTorch) | Max Mem  | Speed (PyTorch) | Speed   |\n| ----- | ----------------- | -------- | --------------- | ------- |\n| small | \\~455 MB          | \\~110 MB | 181 ms          | 62 ms   |\n| base  | \\~720 MB          | \\~367 MB | 462 ms          | 197 ms  |\n| large | \\~1.55 GB         | \\~1.2 GB | 1288 ms         | 600 ms  |\n| giant | \\~4.8 GB          | \\~4.4 GB | 4384 ms         | 1969 ms |\n\n#### With Register Tokens\n\n| Model | Max Mem (PyTorch) | Max Mem  | Speed (PyTorch) | Speed   |\n| ----- | ----------------- | -------- | --------------- | ------- |\n| small | \\~455 MB          | \\~110 MB | 297 ms          | 64 ms   |\n| base  | \\~720 MB          | \\~366 MB | 436 ms          | 200 ms  |\n| large | \\~1.55 GB         | \\~1.2 GB | 1331 ms         | 597 ms  |\n| giant | \\~4.8 GB          | \\~4.4 GB | 4472 ms         | 1995 ms |\n\n## Quantization Benchmark Results\n\n### Without Register Tokens\n\n| Model | Quant | Speed (ms) | Mem (MB) |\n| ----- | ----- | ---------- | -------- |\n| small | q4\\_0 | 46         | 49       |\n| small | q4\\_1 | 48         | 51       |\n| small | q5\\_0 | 63         | 54       |\n| small | q5\\_1 | 58         | 57       |\n| small | q8\\_0 | 50         | 70       |\n| base  | q4\\_0 | 141        | 129      |\n| base  | q4\\_1 | 135        | 140      |\n| base  | q5\\_0 | 162        | 150      |\n| base  | q5\\_1 | 161        | 160      |\n| base  | q8\\_0 | 125        | 212      |\n| large | q4\\_0 | 389        | 371      |\n| large | q4\\_1 | 382        | 407      |\n| large | q5\\_0 | 497        | 444      |\n| large | q5\\_1 | 478        | 480      |\n| large | q8\\_0 | 348        | 661      |\n| giant | q4\\_0 | 1268       | 1281     |\n| giant | q4\\_1 | 1248       | 1417     |\n| giant | q5\\_0 | 1625       | 1553     |\n| giant | q5\\_1 | 1576       | 1688     |\n| giant | q8\\_0 | 1059       | 2364     |\n\n### With Register Tokens\n\n| Model | Quant | Speed (ms) | Mem (MB) |\n| ----- | ----- | ---------- | -------- |\n| small | q4\\_0 | 52         | 49       |\n| small | q4\\_1 | 50         | 52       |\n| small | q5\\_0 | 59         | 54       |\n| small | q5\\_1 | 57         | 57       |\n| small | q8\\_0 | 51         | 70       |\n| base  | q4\\_0 | 136        | 129      |\n| base  | q4\\_1 | 133        | 139      |\n| base  | q5\\_0 | 164        | 150      |\n| base  | q5\\_1 | 158        | 160      |\n| base  | q8\\_0 | 124        | 211      |\n| large | q4\\_0 | 395        | 371      |\n| large | q4\\_1 | 395        | 407      |\n| large | q5\\_0 | 493        | 443      |\n| large | q5\\_1 | 490        | 480      |\n| large | q8\\_0 | 353        | 661      |\n| giant | q4\\_0 | 1275       | 1281     |\n| giant | q4\\_1 | 1261       | 1417     |\n| giant | q5\\_0 | 1615       | 1552     |\n| giant | q5\\_1 | 1583       | 1687     |\n| giant | q8\\_0 | 1065       | 2364     |\n\n## Conclusion\n\nFor our final project we developed dinov2.cpp, a lightweight C++ inference engine for the DINOv2 family of models. Our implementation enables efficient inference on edge devices, using the GGML tensor library with minimal dependencies.\n\nOur benchmarks demonstrate significant reductions in memory usage and inference time across all model sizes when compared to the original PyTorch implementation. Quantization improves these results further, allowing for efficient model loading and inference. We provide support for classification and real-time feature extraction, giving insight into how the model understands visual information. We implemented flash attention, and positional embedding interpolation features, giving users the ability to choose between classification accuracy and inference speed.\n\nFuture work may include adding support for different visual analysis tasks, such as depth estimation and semantic segmentation.\n\n## References\n\n1. Taghadouini, S., Gerganov, G., & Elion, M. (2024, April 11). *staghado/vit.cpp*. GitHub. [https://github.com/staghado/vit.cpp](https://github.com/staghado/vit.cpp)\n2. Gerganov, G., Devesa, D., GÃ¤ÃŸler, J., Bolz, J., & et al. (2025, May 2). *ggml-org/ggml*. GitHub. [https://github.com/ggml-org/ggml](https://github.com/ggml-org/ggml)\n3. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., & et al. (2024). *DINOv2: Learning Robust Visual Features without Supervision*. arXiv. [https://arxiv.org/abs/2304.07193](https://arxiv.org/abs/2304.07193)\n4. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., & et al. (2021). *An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale*. arXiv. [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)\n\n## ðŸ‘¥ Collaborators\n\n* Michael Krah (Boston University)\n* Zach Gentile (Boston University)","src/content/projects/dinov2cpp.mdx",[33],"../../assets/images/projects/dinov2cpp/thumbnail.jpg","c8f49a2f917b084d","dinov2cpp.mdx","ev-range-pred",{"id":36,"data":38,"body":42,"filePath":43,"assetImports":44,"digest":46,"legacyId":47,"deferredRender":23},{"title":39,"cover":40,"githubUrl":41},"Electric Vehicle Range Prediction","__ASTRO_IMAGE_../../assets/images/projects/ev-range-pred/thumbnail.gif","https://github.com/lavaman131/EV_range_prediction/tree/main","import { Image } from \"astro:assets\";\nimport outliers from \"../../assets/images/projects/ev-range-pred/report/outliers.png\";\nimport outliersResidual from \"../../assets/images/projects/ev-range-pred/report/outliers_residual.png\";\nimport quantile from \"../../assets/images/projects/ev-range-pred/report/quantile.png\";\nimport transforms from \"../../assets/images/projects/ev-range-pred/report/transforms.png\";\nimport corr from \"../../assets/images/projects/ev-range-pred/report/corr.png\";\n\n## Introduction\n\nIn recent years, more and more people are buying Electrical Vehicles\n(EVs) for environmental, aesthetic, and financial reasons. The number of\ncar companies inventing EVs for their brand are increasing. Companies\nsuch as Tesla, Ford, and Rivian are taking advantage of this move toward\nEVs. The goal of our project is to examine just how much the range of\nEVs changes due to other factors such as battery pack. In this paper we\nanalyze the relation between the range and other variables such as\nacceleration, top speed, battery pack, efficiency, fast charge and price\nin order to give us a deeper understanding of what element influences\nthe range of EVs the most. Additionally, we can also see what car\nmanufacturers should do to improve their EVs, and provide useful data\nfor car companies.\n\n\n## Background\n\n### Dataset\n\nDivyanshu Gupta, Kaggle (2021), Cars Dataset with Battery Capacity Data File. Retrieved from [Kaggle](https://www.kaggle.com/datasets/divyanshugupta95/cars-dataset-with-battery-pack-capacity).\n\n### Data Collection Method\n\nThe data was collected from different companies such as Tesla, Porsche,\nBMW. The data set also gives us the specific make and model of the cars.\nThe dataset contains 14 explanatory variables with 1 response variable\nand a total of 102 data points.\n\n\n## Preliminary Analysis\n\n### Hypothesized Variables That Impact Electric Vehicle Range\n\nThe main covariate that we believe will have the largest impact on the\nrange of an electric vehicle is battery packs in kilowatts per hour. The\nother variables that we are looking at would be acceleration, top speed,\nefficiency, how fast the car charges, and price. Qualitative variables\nwould be the plug style, number of seats, power train (all wheel drive\nvs. four wheel drive), and type of car. This mix of qualitative and\nquantitative variables will allow us to give a clearer understanding of\nhow different factors affect the range of EVs.\n\n\n\u003Cdiv className=\"table-auto\" style={{ overflowX: 'auto' }}>\n  \u003Ctable style={{ minWidth: '100%', fontSize: '0.9em' }}>\n    \u003Cthead>\n      \u003Ctr>\n        \u003Cth>\u003C/th>\n        \u003Cth>Brand\u003C/th>\n        \u003Cth>Model\u003C/th>\n        \u003Cth>AccelSec\u003C/th>\n        \u003Cth>TopSpeed_KmH\u003C/th>\n        \u003Cth>Range_Km\u003C/th>\n        \u003Cth>Battery_Pack Kwh\u003C/th>\n        \u003Cth>Efficiency_WhKm\u003C/th>\n        \u003Cth>FastCharge_KmH\u003C/th>\n        \u003Cth>RapidCharge\u003C/th>\n        \u003Cth>PowerTrain\u003C/th>\n        \u003Cth>PlugType\u003C/th>\n        \u003Cth>BodyStyle\u003C/th>\n        \u003Cth>Segment\u003C/th>\n        \u003Cth>Seats\u003C/th>\n        \u003Cth>PriceEuro\u003C/th>\n      \u003C/tr>\n    \u003C/thead>\n    \u003Ctbody>\n      \u003Ctr>\n        \u003Cth>0\u003C/th>\n        \u003Ctd>Tesla\u003C/td>\n        \u003Ctd>Model 3 Long Range Dual Motor\u003C/td>\n        \u003Ctd>4.6\u003C/td>\n        \u003Ctd>233\u003C/td>\n        \u003Ctd>460\u003C/td>\n        \u003Ctd>70.0\u003C/td>\n        \u003Ctd>161\u003C/td>\n        \u003Ctd>940\u003C/td>\n        \u003Ctd>Yes\u003C/td>\n        \u003Ctd>AWD\u003C/td>\n        \u003Ctd>Type 2 CCS\u003C/td>\n        \u003Ctd>Sedan\u003C/td>\n        \u003Ctd>D\u003C/td>\n        \u003Ctd>5\u003C/td>\n        \u003Ctd>55480\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Cth>1\u003C/th>\n        \u003Ctd>Volkswagen\u003C/td>\n        \u003Ctd>ID.3 Pure\u003C/td>\n        \u003Ctd>10.0\u003C/td>\n        \u003Ctd>160\u003C/td>\n        \u003Ctd>270\u003C/td>\n        \u003Ctd>45.0\u003C/td>\n        \u003Ctd>167\u003C/td>\n        \u003Ctd>250\u003C/td>\n        \u003Ctd>Yes\u003C/td>\n        \u003Ctd>RWD\u003C/td>\n        \u003Ctd>Type 2 CCS\u003C/td>\n        \u003Ctd>Hatchback\u003C/td>\n        \u003Ctd>C\u003C/td>\n        \u003Ctd>5\u003C/td>\n        \u003Ctd>30000\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Cth>2\u003C/th>\n        \u003Ctd>Polestar\u003C/td>\n        \u003Ctd>2\u003C/td>\n        \u003Ctd>4.7\u003C/td>\n        \u003Ctd>210\u003C/td>\n        \u003Ctd>400\u003C/td>\n        \u003Ctd>75.0\u003C/td>\n        \u003Ctd>181\u003C/td>\n        \u003Ctd>620\u003C/td>\n        \u003Ctd>Yes\u003C/td>\n        \u003Ctd>AWD\u003C/td>\n        \u003Ctd>Type 2 CCS\u003C/td>\n        \u003Ctd>Liftback\u003C/td>\n        \u003Ctd>D\u003C/td>\n        \u003Ctd>5\u003C/td>\n        \u003Ctd>56440\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Cth>3\u003C/th>\n        \u003Ctd>BMW\u003C/td>\n        \u003Ctd>iX3\u003C/td>\n        \u003Ctd>6.8\u003C/td>\n        \u003Ctd>180\u003C/td>\n        \u003Ctd>360\u003C/td>\n        \u003Ctd>74.0\u003C/td>\n        \u003Ctd>206\u003C/td>\n        \u003Ctd>560\u003C/td>\n        \u003Ctd>Yes\u003C/td>\n        \u003Ctd>RWD\u003C/td>\n        \u003Ctd>Type 2 CCS\u003C/td>\n        \u003Ctd>SUV\u003C/td>\n        \u003Ctd>D\u003C/td>\n        \u003Ctd>5\u003C/td>\n        \u003Ctd>68040\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Cth>4\u003C/th>\n        \u003Ctd>Honda\u003C/td>\n        \u003Ctd>e\u003C/td>\n        \u003Ctd>9.5\u003C/td>\n        \u003Ctd>145\u003C/td>\n        \u003Ctd>170\u003C/td>\n        \u003Ctd>28.5\u003C/td>\n        \u003Ctd>168\u003C/td>\n        \u003Ctd>190\u003C/td>\n        \u003Ctd>Yes\u003C/td>\n        \u003Ctd>RWD\u003C/td>\n        \u003Ctd>Type 2 CCS\u003C/td>\n        \u003Ctd>Hatchback\u003C/td>\n        \u003Ctd>B\u003C/td>\n        \u003Ctd>4\u003C/td>\n        \u003Ctd>32997\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Cth>...\u003C/th>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n        \u003Ctd>...\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Cth>97\u003C/th>\n        \u003Ctd>Nissan\u003C/td>\n        \u003Ctd>Ariya 63kWh\u003C/td>\n        \u003Ctd>7.5\u003C/td>\n        \u003Ctd>160\u003C/td>\n        \u003Ctd>330\u003C/td>\n        \u003Ctd>63.0\u003C/td>\n        \u003Ctd>191\u003C/td>\n        \u003Ctd>440\u003C/td>\n        \u003Ctd>Yes\u003C/td>\n        \u003Ctd>FWD\u003C/td>\n        \u003Ctd>Type 2 CCS\u003C/td>\n        \u003Ctd>Hatchback\u003C/td>\n        \u003Ctd>C\u003C/td>\n        \u003Ctd>5\u003C/td>\n        \u003Ctd>45000\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Cth>98\u003C/th>\n        \u003Ctd>Audi\u003C/td>\n        \u003Ctd>e-tron S Sportback 55 quattro\u003C/td>\n        \u003Ctd>4.5\u003C/td>\n        \u003Ctd>210\u003C/td>\n        \u003Ctd>335\u003C/td>\n        \u003Ctd>86.5\u003C/td>\n        \u003Ctd>258\u003C/td>\n        \u003Ctd>540\u003C/td>\n        \u003Ctd>Yes\u003C/td>\n        \u003Ctd>AWD\u003C/td>\n        \u003Ctd>Type 2 CCS\u003C/td>\n        \u003Ctd>SUV\u003C/td>\n        \u003Ctd>E\u003C/td>\n        \u003Ctd>5\u003C/td>\n        \u003Ctd>96050\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Cth>99\u003C/th>\n        \u003Ctd>Nissan\u003C/td>\n        \u003Ctd>Ariya e-4ORCE 63kWh\u003C/td>\n        \u003Ctd>5.9\u003C/td>\n        \u003Ctd>200\u003C/td>\n        \u003Ctd>325\u003C/td>\n        \u003Ctd>63.0\u003C/td>\n        \u003Ctd>194\u003C/td>\n        \u003Ctd>440\u003C/td>\n        \u003Ctd>Yes\u003C/td>\n        \u003Ctd>AWD\u003C/td>\n        \u003Ctd>Type 2 CCS\u003C/td>\n        \u003Ctd>Hatchback\u003C/td>\n        \u003Ctd>C\u003C/td>\n        \u003Ctd>5\u003C/td>\n        \u003Ctd>50000\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Cth>100\u003C/th>\n        \u003Ctd>Nissan\u003C/td>\n        \u003Ctd>Ariya e-4ORCE 87kWh Performance\u003C/td>\n        \u003Ctd>5.1\u003C/td>\n        \u003Ctd>200\u003C/td>\n        \u003Ctd>375\u003C/td>\n        \u003Ctd>87.0\u003C/td>\n        \u003Ctd>232\u003C/td>\n        \u003Ctd>450\u003C/td>\n        \u003Ctd>Yes\u003C/td>\n        \u003Ctd>AWD\u003C/td>\n        \u003Ctd>Type 2 CCS\u003C/td>\n        \u003Ctd>Hatchback\u003C/td>\n        \u003Ctd>C\u003C/td>\n        \u003Ctd>5\u003C/td>\n        \u003Ctd>65000\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Cth>101\u003C/th>\n        \u003Ctd>Byton\u003C/td>\n        \u003Ctd>M-Byte 95 kWh 2WD\u003C/td>\n        \u003Ctd>7.5\u003C/td>\n        \u003Ctd>190\u003C/td>\n        \u003Ctd>400\u003C/td>\n        \u003Ctd>95.0\u003C/td>\n        \u003Ctd>238\u003C/td>\n        \u003Ctd>480\u003C/td>\n        \u003Ctd>Yes\u003C/td>\n        \u003Ctd>AWD\u003C/td>\n        \u003Ctd>Type 2 CCS\u003C/td>\n        \u003Ctd>SUV\u003C/td>\n        \u003Ctd>E\u003C/td>\n        \u003Ctd>5\u003C/td>\n        \u003Ctd>62000\u003C/td>\n      \u003C/tr>\n    \u003C/tbody>\n  \u003C/table>\n\u003C/div>\n\n\n### Exploring Influence of Outliers\n\nWe tested different outlier removal methods such as taking away data\nthat was two and three standard deviations away from mean. Figure 1\ncompares the linear relationship of covariates against Range_Km with and\nwithout outliers respectively:\n\n\u003CImage src={outliers} alt=\"outliers\" />\n\nAfter analyzing the best-fit lines with and without outliers we went on\nto analyze the corresponding residual plots. Figure 2 compares the\nresidual plots of covariates against Range_Km with and without outliers\nrespectively:\n\n\u003CImage src={outliersResidual} alt=\"outliersResidual\" />\n\nWe found a minimal difference in the strength of the linear relationship\n(measured with $R^2$) between the covariates and response variable when\nfiltering outliers. Additionally, our dataset is relatively small with\n102 samples. For these reasons, we decided not to remove outliers for\nfurther stages of our analysis.\n\n\u003Cfigure>\n  \u003Ctable className=\"table-auto\">\n    \u003Cthead>\n      \u003Ctr>\n        \u003Cth>Covariate\u003C/th>\n        \u003Cth>RÂ² Before\u003C/th>\n        \u003Cth>RÂ² After\u003C/th>\n      \u003C/tr>\n    \u003C/thead>\n    \u003Ctbody>\n      \u003Ctr>\n        \u003Ctd>AccelSec\u003C/td>\n        \u003Ctd>0.46\u003C/td>\n        \u003Ctd>0.522\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>TopSpeed_KmH\u003C/td>\n        \u003Ctd>0.56\u003C/td>\n        \u003Ctd>0.463\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>Battery_Pack Kwh\u003C/td>\n        \u003Ctd>0.829\u003C/td>\n        \u003Ctd>0.753\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>Efficiency_WhKm\u003C/td>\n        \u003Ctd>0.098\u003C/td>\n        \u003Ctd>0.068\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>FastCharge_KmH\u003C/td>\n        \u003Ctd>0.569\u003C/td>\n        \u003Ctd>0.6\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>PriceEuro\u003C/td>\n        \u003Ctd>0.458\u003C/td>\n        \u003Ctd>0.411\u003C/td>\n      \u003C/tr>\n    \u003C/tbody>\n  \u003C/table>\n  \u003Cfigcaption className=\"text-center mt-2 text-sm italic\">\n    Table 1\n  \u003C/figcaption>\n  \u003C/figure>\n\n### Normal Quantile Plot\n\nIn order to verify one of the central assumptions of linear regression,\nnormally distributed residuals, we created a Q-Q plot in Figure 3 for\nall of our covariates to observe that this assumption holds true for\nmost of our covariates.\n\n\u003CImage src={quantile} alt=\"normalQuantile\" />\n\n## Transformations\n\nAnalyzing most of the residuals we can see that most of them are random,\nnormally distributed, independent, and show homoscedasticity. However,\nthese conditions are not met for the covariates Battery pack and Price\nin Euro. For this reason, we decided that it is important to transform\nBattery pack and Price in Euro.\n\nWe tested two conventional transformations. First, we applied square\nroot transformations on Battery pack and Price in Euro. Next, we applied\nlog transformations to the two covariates. We determined that the log\ntransformations led to a stronger linear relationship ($R^2$) so our\nfinal transformation was a log transform on Battery Pack and Price.\nFigure 4 shows the improved residual plots after the log transformation:\n\n\u003CImage src={transforms} alt=\"transforms\" />\n\n\u003Cfigure>\n  \u003Ctable className=\"table-auto\">\n    \u003Cthead>\n      \u003Ctr>\n        \u003Cth>Covariate\u003C/th>\n        \u003Cth>RÂ² Before\u003C/th>\n        \u003Cth>RÂ² After\u003C/th>\n      \u003C/tr>\n    \u003C/thead>\n    \u003Ctbody>\n      \u003Ctr>\n        \u003Ctd>AccelSec\u003C/td>\n        \u003Ctd>0.46\u003C/td>\n        \u003Ctd>0.539\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>TopSpeed_KmH\u003C/td>\n        \u003Ctd>0.56\u003C/td>\n        \u003Ctd>0.552\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>Battery_Pack Kwh\u003C/td>\n        \u003Ctd>0.829\u003C/td>\n        \u003Ctd>0.768\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>Efficiency_WhKm\u003C/td>\n        \u003Ctd>0.098\u003C/td>\n        \u003Ctd>0.087\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>FastCharge_KmH\u003C/td>\n        \u003Ctd>0.569\u003C/td>\n        \u003Ctd>0.187\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>PriceEuro\u003C/td>\n        \u003Ctd>0.458\u003C/td>\n        \u003Ctd>0.529\u003C/td>\n      \u003C/tr>\n    \u003C/tbody>\n  \u003C/table>\n  \u003Cfigcaption className=\"text-center mt-2 text-sm italic\">\n    Table 2\n  \u003C/figcaption>\n\u003C/figure>\n\n## Main Results\n\n### Multicollinearity\n\nWe created a correlation coefficient matrix (Figure 5) in order to test\nfor multicollinearity within our covariates. Range was highly correlated\nwith all of our covariates which is a good sign that our model is\npredictive of range. We noticed that acceleration has a negative\ncorrelation with most of the other variables including range. The\nefficiency coefficient had the lowest correlation coefficient with range\nat 0.48. There is high correlation between price and our other\ncovariates, and we believe there is evidence of multicollinearity\nbecause of the high correlation coefficient. Removing price may reduce\nthe multicollinearity issues we encounter. It may be best to use a\nstepwise forward regression, to allow the model to remove covariates\nthat are causing multicollinearity.\n\n\u003CImage src={corr} alt=\"correlation\" />\n\nWith the covariates that remained after feature selection, we measured multicollinearity using the variance inflation factor (VIF) given by:\n\n$$\nVIF_i = \\frac{1}{1 - {R_i}^2}\n$$\n\nThe result (Table 3) shows that all of the selected covariates are below 5, which means that they will not have multicollinearity. The variable TopSpeed is less significant than the other covariates, but because it is significant at the 5% level and has a VIF level below 10, we believe it should still be included for analysis.\n\n\u003Cfigure>\n  \u003Ctable className=\"table-auto\">\n    \u003Cthead>\n      \u003Ctr>\n      \u003Cth>Term\u003C/th>\n      \u003Cth>Estimate\u003C/th>\n      \u003Cth>Std Error\u003C/th>\n      \u003Cth>t Ratio\u003C/th>\n      \u003Cth>Prob > |t|\u003C/th>\n      \u003Cth>VIF\u003C/th>\n    \u003C/tr>\n  \u003C/thead>\n  \u003Ctbody>\n    \u003Ctr>\n      \u003Ctd>Intercept\u003C/td>\n      \u003Ctd>-511.9363\u003C/td>\n      \u003Ctd>34.95776\u003C/td>\n      \u003Ctd>-14.64\u003C/td>\n      \u003Ctd>&lt; .0001*\u003C/td>\n      \u003Ctd>.\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>TopSpeed_KmH\u003C/td>\n      \u003Ctd>0.4494041\u003C/td>\n      \u003Ctd>0.175015\u003C/td>\n      \u003Ctd>2.57\u003C/td>\n      \u003Ctd>0.0120*\u003C/td>\n      \u003Ctd>1.9033032\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>Log(Battery_Pack KWh)\u003C/td>\n      \u003Ctd>235.09752\u003C/td>\n      \u003Ctd>14.19828\u003C/td>\n      \u003Ctd>16.56\u003C/td>\n      \u003Ctd>&lt; .0001*\u003C/td>\n      \u003Ctd>2.7759986\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>Efficiency_WhKm\u003C/td>\n      \u003Ctd>-1.013424\u003C/td>\n      \u003Ctd>0.184575\u003C/td>\n      \u003Ctd>-5.49\u003C/td>\n      \u003Ctd>&lt; .0001*\u003C/td>\n      \u003Ctd>1.8698509\u003C/td>\n    \u003C/tr>\n    \u003C/tbody>\n  \u003C/table>\n  \u003Cfigcaption className=\"text-center mt-2 text-sm italic\">\n    Table 3\n  \u003C/figcaption>\n\u003C/figure>\n\n### Variable Selection\n\n\n\u003Cfigure>\n  \u003Ctable className=\"table-auto\">\n    \u003Cthead>\n      \u003Ctr>\n        \u003Cth>Method\u003C/th>\n        \u003Cth>Prob to enter/leave\u003C/th>\n        \u003Cth>Equation\u003C/th>\n        \u003Cth>Adjusted RÂ²\u003C/th>\n      \u003C/tr>\n    \u003C/thead>\n    \u003Ctbody>\n      \u003Ctr>\n        \u003Ctd>Forward Selection\u003C/td>\n        \u003Ctd>PE: 0.25\u003Cbr/>PL: 0.1\u003C/td>\n        \u003Ctd>$\\hat{y} = -909.39 + 0.29x_1 + 39.04x_2 - 2.08x_3 + 288.90x_4 - 28.80x_5 + 8.22x_6 - 45.13x_7 - 52.62x_8 + 70.71x_9 + 43.78x_{10}$\u003C/td>\n        \u003Ctd>0.941\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>Backward Elimination\u003C/td>\n        \u003Ctd>PE: 0.25\u003Cbr/>PL: 0.1\u003C/td>\n        \u003Ctd>$\\hat{y} = -909.39 + 0.29x_1 + 39.04x_2 - 2.08x_3 + 288.90x_4 - 28.80x_5 + 8.22x_6 - 45.13x_7 - 52.62x_8 + 70.71x_9 + 43.78x_{10}$\u003C/td>\n        \u003Ctd>0.941\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>Mixed\u003C/td>\n        \u003Ctd>PE: 0.25\u003Cbr/>PL:0.25\u003C/td>\n        \u003Ctd>$\\hat{y} = -909.39 + 0.29x_1 + 39.04x_2 - 2.08x_3 + 288.90x_4 - 28.80x_5 + 8.22x_6 - 45.13x_7 - 52.62x_8 + 70.71x_9 + 43.78x_{10}$\u003C/td>\n        \u003Ctd>0.941\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>AICC\u003C/td>\n        \u003Ctd>N/A\u003C/td>\n        \u003Ctd>$\\hat{y} = -909.39 + 0.29x_1 + 39.04x_2 - 2.08x_3 + 288.90x_4 - 28.80x_5 + 8.22x_6 - 45.13x_7 - 52.62x_8 + 70.71x_9 + 43.78x_{10}$\u003C/td>\n        \u003Ctd>0.941\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>BIC\u003C/td>\n        \u003Ctd>N/A\u003C/td>\n        \u003Ctd>$\\hat{y} = -909.39 + 0.29x_1 + 39.04x_2 - 2.08x_3 + 288.90x_4 - 28.80x_5 + 8.22x_6 - 45.13x_7 - 52.62x_8 + 70.71x_9 + 43.78x_{10}$\u003C/td>\n        \u003Ctd>0.941\u003C/td>\n      \u003C/tr>\n    \u003C/tbody>\n  \u003C/table>\n  \u003Cfigcaption className=\"text-center mt-2 text-sm italic\">\n    Table 4\n  \u003C/figcaption>\n\u003C/figure>\n\n\n### Model Fit\n\nThe transformations that we included in our model included the log of\nprice and log of battery pack. We achieved a perfect ${R_a}^2$ value and\nrealized that the car Model and brand covariate had a high cardinality\n(factor levels were equal to the number of data points in this case),\nhence, causing overfitting (over-generalization) of the fitted model.\nFor this reason, we removed the a Model and brand covariate and were\nleft with a much more applicable adjusted ${R_a}^2$ and low RMSE that\npredicts range.\n\n\u003Cfigure>\n  \u003Ctable className=\"table-auto\">\n    \u003Cthead>\n      \u003Ctr>\n        \u003Cth colSpan=\"2\">Summary of Fit\u003C/th>\n      \u003C/tr>\n    \u003C/thead>\n    \u003Ctbody>\n      \u003Ctr>\n        \u003Ctd>RSquare\u003C/td>\n        \u003Ctd>0.954078\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>RSquare Adj\u003C/td>\n        \u003Ctd>0.949031\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>Root Mean Square Error\u003C/td>\n        \u003Ctd>28.60422\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>Mean of Response\u003C/td>\n        \u003Ctd>338.6275\u003C/td>\n      \u003C/tr>\n      \u003Ctr>\n        \u003Ctd>Observations (or Sum Wgts)\u003C/td>\n        \u003Ctd>102\u003C/td>\n      \u003C/tr>\n    \u003C/tbody>\n  \u003C/table>\n  \u003Cfigcaption className=\"text-center mt-2 text-sm italic\">\n    Table 5\n  \u003C/figcaption>\n\u003C/figure>\n\n\u003Cfigure>\n  \u003Ctable className=\"table-auto\">\n  \u003Cthead>\n    \u003Ctr>\n      \u003Cth>Source\u003C/th>\n      \u003Cth>DF\u003C/th>\n      \u003Cth>Sum of Squares\u003C/th>\n      \u003Cth>Mean Square\u003C/th>\n      \u003Cth>F Ratio\u003C/th>\n    \u003C/tr>\n  \u003C/thead>\n  \u003Ctbody>\n    \u003Ctr>\n      \u003Ctd>Model\u003C/td>\n      \u003Ctd>10\u003C/td>\n      \u003Ctd>1546901.5\u003C/td>\n      \u003Ctd>154690\u003C/td>\n      \u003Ctd>189.0612\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>Error\u003C/td>\n      \u003Ctd>91\u003C/td>\n      \u003Ctd>74456.3\u003C/td>\n      \u003Ctd>818\u003C/td>\n      \u003Ctd>Prob > F\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>C. Total\u003C/td>\n      \u003Ctd>101\u003C/td>\n      \u003Ctd>1621357.8\u003C/td>\n      \u003Ctd>\u003C/td>\n      \u003Ctd>&lt;.0001*\u003C/td>\n    \u003C/tr>\n    \u003C/tbody>\n  \u003C/table>\n  \u003Cfigcaption className=\"text-center mt-2 text-sm italic\">\n    Table 6\n  \u003C/figcaption>\n\u003C/figure>\n\n\u003Cfigure>\n  \u003Ctable className=\"table-auto\">\n    \u003Ctr>\n      \u003Cth>Term\u003C/th>\n      \u003Cth>Estimate\u003C/th>\n      \u003Cth>Std Error\u003C/th>\n      \u003Cth>t Ratio\u003C/th>\n      \u003Cth>Prob &gt; |t|\u003C/th>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Cth>Intercept\u003C/th>\n      \u003Ctd>-909.3919\u003C/td>\n      \u003Ctd>117.3835\u003C/td>\n      \u003Ctd>-7.75\u003C/td>\n      \u003Ctd>{'\u003C'} .0001\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Cth>TopSpeed_KmH\u003C/th>\n      \u003Ctd>0.2942874\u003C/td>\n      \u003Ctd>0.140142\u003C/td>\n      \u003Ctd>2.10\u003C/td>\n      \u003Ctd>0.0385\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Cth>RapidCharge[No]\u003C/th>\n      \u003Ctd>39.042448\u003C/td>\n      \u003Ctd>8.552891\u003C/td>\n      \u003Ctd>4.56\u003C/td>\n      \u003Ctd>{'\u003C'} .0001\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Cth>Efficiency_WhKm\u003C/th>\n      \u003Ctd>-2.078849\u003C/td>\n      \u003Ctd>0.161668\u003C/td>\n      \u003Ctd>-12.86\u003C/td>\n      \u003Ctd>{'\u003C'} .0001\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Cth>Log(Battery_Pack Kwh)\u003C/th>\n      \u003Ctd>288.89749\u003C/td>\n      \u003Ctd>14.30833\u003C/td>\n      \u003Ctd>20.19\u003C/td>\n      \u003Ctd>{'\u003C'} .0001*\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Cth>BodyStyle{'{'}SPV&Hatchback&MPV&SUV-Station&Sedan&Cabrio&Liftback&Pickup{'}'}\u003C/th>\n      \u003Ctd>-28.80209\u003C/td>\n      \u003Ctd>5.374477\u003C/td>\n      \u003Ctd>-5.36\u003C/td>\n      \u003Ctd>{'\u003C'} .0001*\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Cth>BodyStyle{'{'}SPV&Hatchback-MPV&SUV{'}'}\u003C/th>\n      \u003Ctd>8.2153396\u003C/td>\n      \u003Ctd>3.623798\u003C/td>\n      \u003Ctd>2.27\u003C/td>\n      \u003Ctd>0.0258\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Cth>BodyStyle{'{'}Station&Sedan&Cabrio&Liftback-Pickup{'}'}\u003C/th>\n      \u003Ctd>-45.12672\u003C/td>\n      \u003Ctd>12.9488\u003C/td>\n      \u003Ctd>-3.49\u003C/td>\n      \u003Ctd>0.0008*\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Cth>BodyStyle{'{'}Station&Sedan-Cabrio&Liftback{'}'}\u003C/th>\n      \u003Ctd>-52.61664\u003C/td>\n      \u003Ctd>6.97113\u003C/td>\n      \u003Ctd>-7.55\u003C/td>\n      \u003Ctd>{'\u003C'} .0001*\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Cth>BodyStyle{'{'}Cabriolet-Liftback{'}'}\u003C/th>\n      \u003Ctd>70.712853\u003C/td>\n      \u003Ctd>11.46589\u003C/td>\n      \u003Ctd>6.17\u003C/td>\n      \u003Ctd>{'\u003C'} .0001*\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Cth>Log(PriceEuro)\u003C/th>\n      \u003Ctd>43.782414\u003C/td>\n      \u003Ctd>14.52427\u003C/td>\n      \u003Ctd>3.01\u003C/td>\n      \u003Ctd>0.0033*\u003C/td>\n    \u003C/tr>\n  \u003C/table>\n  \u003Cfigcaption className=\"text-center mt-2 text-sm italic\">\n    Table 7\n  \u003C/figcaption>\n\u003C/figure>\n\n\n\nAfter we used forward selection as our variable selection method and set\np-value enter is 0.25 and p-value leave as 0.1, we determined our final\nmodel to be as follows:\n\n$$\n\\begin{aligned}\n\\hat{y} = & -909.39 + 0.29x_1 + 39.04x_2 - 2.08x_3 + 288.90x_4 \\\\\n& - 28.80x_5 + 8.22x_6 - 45.13x_7 - 52.62x_8 \\\\\n& + 70.71x_9 + 43.78x_{10}\n\\end{aligned}\n$$\n\nwhere $x_i$ are listed in the table below:\n\n\u003Cfigure>\n  \u003Ctable className=\"table-auto\">\n    \u003Ctbody>\n    \u003Ctr>\n      \u003Ctd>$x_1$\u003C/td>\n      \u003Ctd>Topspeed_KmH\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>$x_2$\u003C/td>\n      \u003Ctd>Rapid Charge\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>$x_3$\u003C/td>\n      \u003Ctd>Efficiency_WhKm\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>$x_4$\u003C/td>\n      \u003Ctd>log(Battery_Pack Kwh)\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>$x_5$\u003C/td>\n      \u003Ctd>BodyStyle{'{'}SPV&Hatchback&MPV&SUVâ€“\u003Cbr/>Station&Sedan&Cabriolet&Lift back&Pickup{'}'}\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>$x_6$\u003C/td>\n      \u003Ctd>BodyStyle{'{'}SPV&Hatchbackâ€“MPV&SUV{'}'}\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>$x_7$\u003C/td>\n      \u003Ctd>BodyStyle{'{'}Station&Sedan&Cabriolet&Lift\u003Cbr/>backâ€“Pickup{'}'}\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>$x_8$\u003C/td>\n      \u003Ctd>BodyStyle{'{'}Station&Sedanâ€“Cabriolet&Lift\u003Cbr/>back{'}'}\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>$x_9$\u003C/td>\n      \u003Ctd>BodyStyle{'{'}Cabrioletâ€“Lift back{'}'}\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>$x_{10}$\u003C/td>\n      \u003Ctd>log(PriceEuro)\u003C/td>\n      \u003C/tr>\n    \u003C/tbody>\n  \u003C/table>\n  \u003Cfigcaption className=\"text-center mt-2 text-sm italic\">\n    Table 8\n  \u003C/figcaption>\n\u003C/figure>\n\nWe found our final model to be adequate as it had a high ${R_a}^2$ and\nall of the covariates we found were significant at the $\\alpha = 0.05$\nlevel. The reason we used forward selection is to remove the issue of\nmulticollinearity and it was the simplest and easiest model. Figure 9\nshows the performance of our model. Some points to highlight are that\nour model's ${R_a}^2$ is 0.949, has a statistically significant $F$\nstatistic of 189.061 in the ANOVA test.\n\n### Model Interpretation\n\nThere is a natural interpretation of the covariates in that each one\nshows us how much they affect the range of EVs. The data we got shows\nthat the battery pack does affect the range the most. Other than the\npowertrain covariate, all of the data was significant at the 5% level of\nsignificance. We found how the different body styles of cars can affect\nthe range as well.\n\n\n## Conclusion\n\nBased on the results of our model we can see that there is a strong\npositive correlation between battery pack and range. This can help\nconsumers and car manufacturers when designing new EVs to create larger\nbattery packs and understand that with the combination of Bodystyle we\ncan see which are the best cars for range. For example, Cabrio or\nconvertibles and hatchbacks have the greatest impact on range with their\ncoefficient being 42. SUVs and pickup trucks for example have a negative\neffect on range.\n\nAnother interesting covariate we found was how rapid charge effects\nrange, with it being the third largest $\\beta$ that we found. This makes\nsense as charging cars will lead to more range for the car, it could\nmake the car more efficient. The price was also another factor that\nimpacts range, one that makes sense as price increases, the technology\nand range of the EV increases as well. Our model allows car\nmanufacturers, consumers, and investors to understand the factors of EVs\nthat most affect range allowing for better improvement of EVs for the\nfuture, improving the environment and lowering the market for gas\npowered vehicles.\n\n## ðŸ‘¥ Collaborators\n\n* Yuhang Du (Boston University)\n* Matthew George (Boston University)\n* Xiangru He (Boston University)\n* Yujie Yang (Boston University)","src/content/projects/ev-range-pred.mdx",[45],"../../assets/images/projects/ev-range-pred/thumbnail.gif","6d47af0de8ee5381","ev-range-pred.mdx","graphic-memoir-analysis",{"id":48,"data":50,"body":54,"filePath":55,"assetImports":56,"digest":58,"legacyId":59,"deferredRender":23},{"title":51,"cover":52,"githubUrl":53},"LLM-based Literature Analysis + Generation","__ASTRO_IMAGE_../../assets/images/projects/graphic-memoir-analysis/thumbnail.gif","https://github.com/lavaman131/graphic_memoir_analysis","import { Image } from \"astro:assets\";\nimport panel_detection from \"../../assets/images/projects/graphic-memoir-analysis/report/panel_detection.png\";\nimport avg_panels from \"../../assets/images/projects/graphic-memoir-analysis/report/avg_panels.png\";\nimport avg_words from \"../../assets/images/projects/graphic-memoir-analysis/report/avg_words.png\";\nimport word_cloud from \"../../assets/images/projects/graphic-memoir-analysis/report/word_cloud.png\";\nimport ratio from \"../../assets/images/projects/graphic-memoir-analysis/report/ratio.png\";\nimport sentiment from \"../../assets/images/projects/graphic-memoir-analysis/report/sentiment.png\";\nimport GPT2 from \"../../assets/images/projects/graphic-memoir-analysis/report/GPT2.gif\";\n\n## Introduction\n\nAccording to Anantrasirichai and Bull's \"Artificial Intelligence in the Creative Industries: a Review,\" artificial intelligence is described as an algorithmic process which allows \"a computer system to develop and emulate human-like behavior and hence make decisions similar to (or in some cases, better than) humans\" (591). A large branch of AI is machine learning (ML) which \"employs computational methods to 'learn' information directly from large amounts of example data without relying on a predetermined equation or model\" (591). Among the most recent developments in artificial intelligence is generative AI which utilizes algorithms to very accurately (measured as the algorithm's ability to effectively predict the next word/token given a previous sequence of words/tokens) create or generate text and image content. Artificial intelligence (AI) and statistical analysis is an exciting and rapidly evolving field, but its impact on literature has yet to be fully explored. In the first part of my research I will attempt to create a statistical tool to analyze graphic memoirs with Marjane Satrapi's graphic memoir, *The Complete Persepolis*, as a case study by closely examining the text and image content (features) of her memoir utilizing machine learning techniques. I will then follow my analysis with an experiment using generative AI to recreate text in the style of Satrapi in order to explore the capabilities of AI. While there are previous papers describing the influence of Satrapi's work as a medium to promote political and social activism against Iran's strict Islamic regime (Stromberg 91-119), none follow a statistical and machine learning approach to automatically identify unique features in Satrapi's memoir for analysis and directly assess the effects of generative AI for this matter. My work also provides a baseline as well as an open-source tool for statistical methods in comparing different graphic memoirs.\n\n## Methods\n\nThere are numerous features of a graphic memoir that allow it to convey different messages. For instance, the average number of words per page which can be extracted using optical character recognition algorithms presented in Vetulani, Zygmunt, et al. can reveal much about an author's styleâ€”short versus long sentences can have different effects on readers. Additionally, the number of panels in a page allows certain authors to underscore a particular scene in their memoir. Using object recognition or shape detection algorithms, it may be possible to count the number of panels per page in a given graphic memoir. Also, there are currently sentiment analysis algorithms in place that allow computers to recognize the connotation of certain words and phrases with the capabilities of large language models (LLMs) which are algorithms that have the ability to effectively interpret and generate natural language. Looking at the ratio of positive, negative, and neutral sentiment words may reveal important information in that regard. Another important metric would be to consider the average ratio of words to images in each page of the memoir. Additionally, whether a memoir is illustrated in black and white may reveal much about the work versus colored content. The author's background including race, age, nationality, gender, and any other personal information all could be indirectly related to the aforementioned statistics. Through statistical analysis of these features I can gain a greater context of the features that allow Satrapi's work to be such an effective medium for political and social protest as well as create a new form of computational literary analysis that can be embedded with traditional analysis of text.\n\n## Findings\n\n### Object Detection for Counting Panels\n\nUsing traditional object detection algorithms and appropriate shape thresholding it is possible to get an accurate count of the average number of panels per page in a given chapter of Satrapi's work. Through observing the number of panels per page, the reader can understand and compare the presence or lack-thereof of images in different memoirs. In the case of Satrapi's memoir, the strong presence of images aligns well with a study by psychologists Piotr Winkielman and Yekaterina Gogolushko on the \"Influence of Suboptimally and Optimally Presented Affective Pictures and Words on Consumption-Related Behavior\" which reveals that a single image elicits a stronger emotional response than a single word. In this particular sample page below we can observe the results of the object detection algorithm. In this specific case, the algorithm detects 6 panels denoted by the green outlines in the right-most image:\n\n\u003CImage src={panel_detection} class=\"w-full sm:w-2/3\" alt=\"Panel Detection\" />\n\n### Chapter vs Average Number of Panels Per Page\n\nFurthermore, digging deeper into the distribution of chapters versus the average number of panels per page one can see that Satrapi typically does not use more than seven panels on average per page. With this information, one can use the average number of panels per page to compare different memoirs.\n\n\u003CImage src={avg_panels} class=\"w-full sm:w-2/3\" alt=\"Average Panels\" />\n\n### Chapter vs Average Number of Words Per Page\n\nMoving on to the average number of words per page, due to the nature of *The Complete Persepolis* being a graphic memoir, it is much less dense in terms of average word (at most 200 words) count per page compared to novels which typically average to much more. The statistic of average number of words per page can be used to contrast the writing styles of different authors in graphic memoirs and weigh the advantages and disadvantages of denser texts. A highly debated topic in literature is finding this optimal word-count balance. By repeating this analysis for multiple memoirs it may be possible to find a general trend in word-count.\n\n\u003CImage src={avg_words} class=\"w-full sm:w-2/3\" alt=\"Average Words\" />\n\n### Words Frequencies\n\nThe frequency of certain words in a graphic memoir can also reveal more about the theme of a particular memoir. In the case of Satrapi's *The Complete Persepolis*, below is a visualization (word cloud) of the most common words in the entire memoir along with an accompanying table:\n\n\u003CImage src={word_cloud} alt=\"Word Cloud\" />\n\nAs seen in the table above, among the most common words is \"know\". The word \"know\" may appear frequently because it shows Satrapi's continual quest for knowledge of the world around her through her childhood in Iran, teenage years in Austria, and early adulthood in Iran. Arguably, this statistic reveals Satrapi's focus on knowledge acquired through interpersonal relationships with family, friends, and acquaintances. These relationships lead Satrapi to develop a knack for social and political activism in order to inform individuals with stereotypical and disillusioned perspectives of her Iranian nationality. This raises the ultimate question if the knowledge we gain from other people is as reliable as the research we conduct. The broader implications of Satrapi's concept of social disillusionment through her encounters with the global community while in school at Austria can be related to the effects of social media today and false narratives surrounding demographics of people.\n\n### Chapter vs Word to Image Ratio\n\nA highly discussed topic introduced by Scott McCloud in the chapter \"Show and Tell\" of his work, *Understanding Comics: The Invisible Art*, is finding the \"perfect\" balance between words and images. According to McCloud the norm in literature is that great works of art and literature are only possible when the words and images are kept separate. In Scott's perspective, people are often taught with picture books as children because they are \"easier\" and less nuanced than the more \"real\" novels read as adults. However, the art of comics is finding the right balance of words and images. Specifically, Satrapi's memoir utilizes a word and picture combination known as interdependent which involves words and pictures going hand in hand to convey an idea that neither could convey alone. The graph below shows the word to image ratios in every chapter of Marjane Satrapi's graphic memoir, *The Complete Persepolis*. For reference, the higher the word to image ratio, the greater the number of words relative to images there are. By repeating this analysis, it is possible to compare the interaction between words and images to study the ratio of each and their relative effectiveness in graphic memoirs.\n\n\u003CImage src={ratio} class=\"w-full sm:w-2/3\" alt=\"Average Ratio\" />\n\n### Colors Present in Memoir\n\nAnother undervalued property of graphic memoirs is their color palette. Scott McCloud describes the effect of color in the chapter \"A Word About Color\" of his work, *Understanding Comics: The Invisible Art*. According to McCloud, while color gives the author more ability to explore with the expression of their images and \"objectify their subjects\" (189), \"unfortunately, color is still an expensive option and has historically been in the hands of larger, more conservative publishers\" (191). This leads to most comic artists to be forced to experiment with shades of black and white. As explained by McCloud, \"the differences between black-and-white and color comics are vast and profound affecting every level of the reading experience.\" (192) McCloud states that in \"black and white, the ideas behind the art are communicated more directly [where] meaning transcends form [and] art approaches language.\" (192) Furthermore, McCloud states that flat colors \"take on more significance...and through more expressive colors comics can become an intoxicating environment of sensations that only color can give.\" (192) Thus, color plays an important role in the meaning of graphic memoirs, however, cost can be prohibitive. This could be a potential reason Satrapi uses a black and white color palette as well as to communicate a more direct meaning of her work:\n\n\n## Discussion\n\nOne interesting statistic to explore is the sentiment of a graphic memoir. Using state of the art language models it is possible to identify the connotation and tone of pages in a graphic memoir. As a good starting point, we can place the concept of sentiment into three broad categories: negative, neutral, and positive. Linguistically speaking, it can be possible to create datasets that roughly map common English words and their meaning to these categories. In the case of Marjane's work, it is not surprising to see that the majority of pages are detected as negative because the graphic memoir is centered around the difficulty of her life with the Iranian Revolution of 1979, Iran-Iraq War, separation from her family, loss of loved ones, and other trauma. However, this method is still lacking in understanding the nuance of combinations of multiple sentiments being expressed in a page. While it is possible to take the ratio of positive, neutral, and negative to aggregate a statistic of each, it leads to difficulty in effectively communicating my analysis. Hence, sentiment as a comparison metric of different memoirs has the potential to be further explored in future works. A graph of the highest sentiment score for each page of the memoir can be visualized in the bar graph below:\n\n\u003CImage src={sentiment} class=\"w-full sm:w-2/3\" alt=\"Sentiment\" />\n\n### AI Generated Text Using GPT2\n\nAs a final experiment, the capabilities of generative AI are explored through a transfer learning application on a LLM architecture known as GPT2 (Generative Pre-Trained Transformer). The model was fine-tuned (trained) on Satrapi's memoir and then tasked with sentence-completion of similar style text to her memoir. An example of the results are shown below. In this example, it can be seen that there are generally inconsistencies in the AI-generated text such as repetitive statements and logical fallacies. Through this initial case study, generative AI is still a long way from effectively and convincingly generating text. In fact, the simplest case of the algorithm involves probabilistically generating text based on the context of previous words (tokens) in the text. In most cases, similar types of algorithms are found to regurgitate information that they are trained on rather than generating unique information.\n\n\u003CImage src={GPT2} class=\"w-full sm:w-2/3\" alt=\"GPT2\" />\n\n## Conclusion\n\nThe space of AI and literature is an exciting and upcoming field. It is possible to explore human creativity and the effectiveness of literature by utilizing statistics and machine learning. My work sets a framework for future statistical tools that can be used to compare and contrast pieces of literature, especially graphic memoirs.\n\n\n## References\n\n* Anantrasirichai, Nantheera, and David Bull. \"Artificial Intelligence in the Creative Industries: a Review.\" *The Artificial Intelligence Review*, vol. 55, no. 1, 2022, pp. 589-656. \u003Ca href=\"https://doi.org/10.1007/s10462-021-10039-7\">https://doi.org/10.1007/s10462-021-10039-7\u003C/a>\n\n* McCloud, Scott. *Understanding Comics*. William Morrow Paperbacks, 1994.\n\n* Satrapi, Marjane. *The Complete Persepolis*. Pantheon Books, 2007.\n\n* Stromberg, Fredrik. \"Schemata in the Graphic Novel Persepolis.\" *European Comic Art*, vol. 13, no. 2, 2020, pp. 91-119. \u003Ca href=\"https://doi.org/10.3167/eca.2020.130205\">https://doi.org/10.3167/eca.2020.130205\u003C/a>\n\n\n* Vetulani, Zygmunt, et al. \"How to Improve Optical Character Recognition of Historical Finnish Newspapers Using Open Source Tesseract OCR Engine - Final Notes on Development and Evaluation.\" *Human Language Technology. Challenges for Computer Science and Linguistics*, vol. 12598, Springer International Publishing AG, Switzerland, 2020, pp. 17-30. Lecture Notes in Computer Science. \u003Ca href=\"https://doi.org/10.1007/978-3-030-66527-2_2\">https://doi.org/10.1007/978-3-030-66527-2_2\u003C/a>\n\n* Winkielman, Piotr, and Yekaterina Gogolushko. \"Influence of Suboptimally and Optimally Presented Affective Pictures and Words on Consumption-Related Behavior.\" *Frontiers in Psychology*, vol. 8, 2018, p. 2261. \u003Ca href=\"https://doi.org/10.3389/fpsyg.2017.02261\">https://doi.org/10.3389/fpsyg.2017.02261\u003C/a>\n\n## ðŸ‘¥ Collaborators\n\n* None","src/content/projects/graphic-memoir-analysis.mdx",[57],"../../assets/images/projects/graphic-memoir-analysis/thumbnail.gif","be215026983e8cce","graphic-memoir-analysis.mdx","grammar-ninja",{"id":60,"data":62,"body":66,"filePath":67,"assetImports":68,"digest":70,"legacyId":71,"deferredRender":23},{"title":63,"cover":64,"githubUrl":65},"Grammar Ninja: LLM-based Grammar Correction","__ASTRO_IMAGE_../../assets/images/projects/grammar-ninja/thumbnail.png","https://github.com/rvineet02/grammar_ninja","## Abstract\n\nImproving oneâ€™s English language writing is a significant challenge without access to proficient teachers that can provide valuable feedback. Given the recent rapid acceleration in generative models ability to understand language, we aim to develop a model/fine-tune a model to provide an interface that will generate feedback given text as an input. Our goal is provide quantitative benchmarks for language proficiency in six different areas: cohesion, syntax, vocabulary, phraseology, grammar, and conventions in a provided writing. Additionally, we also generate feedback at the inference layers to provide concrete feedback as to how the input text can be improved. To conclude, our project aims to apply the concepts learned in class to a real-world challenge, by providing a interface to acquire feedback on English writing. By focusing on key areas of language skills and providing model generated actionable feedback, we hope to contribute a somewhat practical tool. \n\n## Introduction\n\nThis project aims to develop a Natural Language Processing (NLP) based system that can automatically evaluate and provide feedback on student argumentative essays. The system will focus on several key aspects of writing, including the effectiveness of arguments, grammar, use of evidence, syntax, and tone. The feedback can be either quantitative, in the form of scores in various categories, or qualitative, as generated English feedback that offers specific guidance and suggestions for improvement.\n\nTo this end, we have decided to split our problem into three separate parts - predicting feedback scores based on the writing, identifying the different parts of argumentative writing (think Parts-Of-Speech tagging but for sentences as opposed to words), and finally constructing a generative model to produce the corrected version of a given input sentence.\n\nWe also provide instructions to setup the project [here](#Setup).\n\n## Methodology\n\nIn this section, we will discuss the various approaches we have taken to fine-tune the three models. \n\n### Feedback Scores (Bert-cased)\n\nWe used the English feedback dataset [here](https://www.kaggle.com/competitions/feedback-prize-english-language-learning). This was dataset we chose to use to identify scoring the dataset on the following features: cohesion, syntax, vocabulary, phraseology, grammar, conventions.\n\n\u003Ch4> Data Exploration\u003C/h4>\n\nGiven some input data `X`, we want to predict writing feedback scores `y` . The writing feedback scores range from 1 to 5 in 0.5 increments.\n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled.png)\n\nWe noticed a few things, most importantly that the target classes are imbalanced:\n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%201.png)\n\nFor example, `cohesion`:\n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%202.png)\n\n### Imbalanced Data - Applying Target Transformations\n\nSo, we now had to make a decision regarding how to re-balance the dataset, so the model can learn equally well on less represented data as well. Here, we considered some strategies:\n\n- Log Transformation: Useful for right-skewed distributions. However, it can't be applied to zero or negative values directly.\n- Square Root Transformation: Less aggressive than log transformation and can be used for right-skewed distributions.\n- Box-Cox Transformation: A more general form of transformation that includes log and square root as special cases. It can only be applied to positive data.\n- Yeo-Johnson Transformation: Similar to Box-Cox but can be applied to data with both positive and negative values.\n\nAlternate Strategies:\n\n- Under-sampling; Delete some data from rows of data from the majority classes. In this case, delete 2 rows resulting in label B and 4 rows resulting in label C.\n    - Limitation: This is hard to use when you donâ€™t have a substantial (and relatively equal) amount of data from each target class.\n- Copy rows of data resulting minority labels. In this case, copy 4 rows with label A and 2 rows with label B to add a total of 6 new rows to the data set.\n    - Limitation: copying current data and you donâ€™t really present anything new. You will get better models, though.\n- SMOTE - Synthetic Minority Oversampling Technique\n    - Synthetically generate new data based on implications of old data. Basically, instead of deleting or copying data, you use the current inputs to generate new input rows that are unique but will have a label based on what the original data implies. In the case above, one simple way to think of this idea would be to add 4 rows with label A to the data where the inputs represent total or partial similarities in values to current input features. Repeat this process for 2 rows of label B as well.\n    - Limitation: If two different class labels have common neighboring examples, it may be hard to generate accurate data representing what each unique label may look like from the input side and therefore SMOTE struggles with higher dimensionality data.\n\nAfter some experimentation on small sample sizes with SMOTE, Box-Cox Transformation and Log Transformation we came to the conclusion that target rebalancing with logs would be the most appropriate to reconstruct an appropriate distribution. \n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%203.png)\n\nWe have transformed the each target with a `log1` transformation, with `cohesion` as an example below:\n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%204.png)\n\nWe then compute the Shapiro-Wilk Test, Skewness, Kurtosis, giving rise to the following analysis: \n\nOur  transformation has correctly changed the distributions' characteristics, often reducing right skewness but introducing left skewness and altering the tailedness (kurtosis). After these changes, the data now more similarly  conforms to a normal distribution according to the Shapiro-Wilk test.  \n\n\u003Ch4> Model Architecture\u003C/h4>\n\nAfter this, we decided to choose a pre-trained generative model that understands semantics of language, and then further fine-tuning with our dataset. And additionally from above, we have, the following constraint - we want a model that is robust to non-normality. \n\nAfter some research and considering a few generative models GPT-3.5/4, Llama, we decided to use BERT instead. This is because BERT has both a cased and uncased version. So, we can experiment in more ways with tokenization. \n\nThe following is the model architecture:\n\n- We are using the BERT model. We then added a `Dropout` and `Linear Layer` as well. We add these layers to ensure the model is able to regularize and classify the data better.\n- In the forward loop, there are 2 outputs from the `BERT` model layer.\n- The output of this, `pooled_output` is passed through the `Dropout` layer and then the `Linear` layer.\n- We set the number of dimensions in the `Linear` layer to be equal to the number of classes we have in the dataset.\n\nThis gives is the following output:\n\n```\nBERT_Classifier(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (drop): Dropout(p=0.0, inplace=False)\n  (out): Linear(in_features=768, out_features=6, bias=True)\n)\n```\n\n\u003Ch4> Training Details\u003C/h4>\n\nAfter reading latest consensus, experiments on Huggingface and local experimentation, we came to the conclusion that the following hyper-parameters are best for this model and data:\n\n```python\nMAX_LEN = 200\nTRAIN_BATCH_SIZE = 4\nVALID_BATCH_SIZE = 4\nEPOCHS = 45\nLEARNING_RATE = 2e-05\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n```\n\n### **Essay Dissection (Longformer)**\n\nThis model automatically segments texts and classifies a range of argumentative and rhetorical elements. The primary elements identified by this model include:\n\n- Position: Identifying the main stance or thesis of the essay.\n- Lead: Recognizing introductory statements or premises.\n- Rebuttal: Detecting responses to counterarguments.\n- Claim: Identifying specific assertions or points made in support of the position.\n- Evidence: Recognizing data, quotations, or other factual information supporting claims.\n- Counterclaim: Detecting opposing viewpoints or arguments.\n- Concluding Statement: Identifying concluding remarks or summaries of the argument.\n\nThe training data used is taken from the writing feedback dataset [here](https://www.kaggle.com/competitions/feedback-prize-2021/data?select=train.csv).\n\n\u003Ch4> Data Exploration\u003C/h4>\n\nThe first step was to explore the initial two datasets. We found the smaller dataset mostly had duplicates of the first one, so we continued with just the larger dataset.\n\n![Screenshot 2023-12-21 at 2.18.54â€¯PM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-21_at_2.18.54_PM.png)\n\nThe dataset was structured as shown above. There were overall about 15k documents/texts in the training dataset (which we also had access to). The split into the respective rhetorical parts is shown in the visual:\n\n![Screenshot 2023-12-21 at 10.42.09â€¯PM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-21_at_10.42.09_PM.png)\n\nThis problem could have been treated as sequence classification problem or a token classification problem. However, we decided to structure it as a token classification similar to POS tagging.\n\n- Using the id, discourse type and prediction string columns we concatenated the texts on their id, each word was tagged with a discourse type.\n    \n    ![Screenshot 2023-12-21 at 4.57.24â€¯PM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-21_at_4.57.24_PM.png)\n    \n- Now this text had to be converted to tokens suitable for our model. Since the first layer in our model was going to be a base pre-trained Longformer transformer, we went with the corresponding tokenizer.\n    \n    After converting the text to tokens, we needed to carefully align the tags with the tokenized text since some words would be split into multiple tokens.\n    \n    ![Screenshot 2023-12-21 at 5.02.41â€¯PM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-21_at_5.02.41_PM.png)\n    \n    These tokens would eventually be encoded by the tokenizer along with the label before being passed into the model.\n    \n\u003Ch4> Model Architecture\u003C/h4>\n\nThe model relied on the pre-trained Longformer transformer, with a context-window of 4096 tokens. The Longformer is an advanced version of the Transformer model, optimized for processing long documents. It overcomes the limitations of traditional Transformer models like BERT or GPT, which struggle with lengthy texts due to their quadratic self-attention mechanism. The Longformer uses a \"sliding window\" attention mechanism, which reduces computational complexity from quadratic to linear by focusing each token's attention on a nearby window of tokens. This design allows the Longformer to efficiently handle texts much longer than standard models, making it ideal for tasks involving large documents, such as legal analysis, long-form summarization, and detailed document review.\n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%205.png)\n\n    The image above shows the difference in memory usage between full self-attention models and Longformer models. Notice the exponential increase in the memory usage of the full-self attention as the sequence length increases. Comparatively, Longformer performs much better. \n    \n    Our model used to classify long text documents such as essays consists of:\n    \n    - A **pre-trained Longformer** layer that can process text sequences of up to 4096 tokens, making it well-suited for lengthy essays.\n    - A **linear layer** (**`dense1`**) that reduces the dimensionality from the Longformer's output size to 256.\n    - A **ReLU activation** to introduce non-linearity, allowing the model to capture complex patterns.\n    - A **dropout layer** to mitigate overfitting by randomly zeroing some fraction of the output units during training.\n    - A final **linear layer** (**`dense2`**) that maps the reduced representation to the number of target labels (**`num_labels`**).\n    \nThe model outputs logits for each label, which can be converted into probabilities for essay classification. The **`forward`** method defines the data flow from input to output, utilizing **`input_ids`** and **`attention_mask`** to handle and process the input text.\n\n\u003Ch4> Training Details\u003C/h4>\n\nAfter extensive testing on smaller subsets of the data, we decided on the following hyper-parameters:\n\n```python\nnum_epochs = 10\nmax_seq_len = 800\nbatch_size = 16\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nloss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n\n# Scheduler to step learning rate\nscheduler = StepLR(optimizer, step_size=2, gamma=0.75)\n\n# Early stopping parameters\nbest_val_loss = float('inf')\nepochs_no_improve = 0\npatience = 2\n```\n    \nWe also employed mixed precision training and data parallelism to decrease training time. You can find the training loss and validation loss over epochs below.\n    \n\n### Grammar Correction (Mistral 7B)\n\n\u003Ch4> Data Exploration\u003C/h4>\n\nWe used the CoEdIT dataset from grammarly for our grammar correction and writing improvement which can be found [here](https://huggingface.co/datasets/grammarly/coedit).\n\nHere is how the dataset looks like:\n\n![Screenshot 2023-12-20 at 6.57.04â€¯AM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-20_at_6.57.04_AM.png)\n\nFor this model, we have the following tasks: grammatical error correction, neutralize, simplification, paraphrase, coherence, and clarity improvement.\n\nBased on availability of the particular task and relevance for grammatical and writing improvement we chose to only train on neutralize, coherence, and grammatical error correction tasks. The distribution of grammar scenes is as follows:\n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%206.png)\n\nHere is an example of existing prompts in our dataset for different writing improvement tasks:\n\n![Screenshot 2023-12-20 at 7.16.41â€¯AM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-20_at_7.16.41_AM.png)\n\nWe apply pre-processing to separate the dataset into instruction, input, and response format to prepare it for instruction fine-tuning. After preprocessing the dataset looks like this:\n\n![Screenshot 2023-12-20 at 7.19.44â€¯AM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-20_at_7.19.44_AM.png)\n\nNext, to format the text for a generation pipeline we draw inspiration from the Alpaca Dataset formatting:\n\n```diff\nBelow is an instruction that describes a task, paired with an input \nthat provides further context. Write a response that appropriately \nthe request.\n\n### Instruction:\n{instruction}\n\n### Input:\n{sentence}\n\n### Response: \n{corrected_sentence}\n```\n\nBelow is an example of the prompt applied to an example sentence.\n\n```\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately the request.\n\n### Instruction:\nCorrect this sentence\n\n### Input:\nHellow there!\n\n### Response:\nHello there!\n```\n\nWe observed a few things in the data that allowed us to make a few decisions about the max input sequence to the model. Here, the figure displays the input token length distribution with the prompt applied to text in the dataset. The mean is depicted as a vertical red line at around 101 tokens:\n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%207.png)\n\nGiven the distribution above, we decided to standardize `token_size` of 200 based on distribution of input token lengths for sentences (model still takes up to `token_size` 4096 assuming enough GPU VRAM).\n\nHowever, during during inference, a `token_size` greater than 200 may not perform as well so a sliding window of 200 across each sentence may be necessary rather than predicting on the max token size of the particular LLM we are using.\n\n\u003Ch4> Model Architecture\u003C/h4>\n\nWe fine-tune the following base model -  **[Mistral-7B-v0.1](https://arxiv.org/abs/2310.06825)** (base model). For some context, the Mistral-7B-v0.1 Large Language Model (LLM) is a pretrained generative text model with 7 billion parameters. Mistral-7B-v0.1 outperforms Llama 2 13B on all benchmarks we tested.\n\nBelow, we describe some architectural details: It uses a similar architecture to Llama, but with improvements including: sliding window attention, rolling buffer cache, and pre-fill and chunking.\n\n![Screenshot 2023-12-20 at 9.28.10â€¯AM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-20_at_9.28.10_AM.png)\n\n![Screenshot 2023-12-20 at 9.28.47â€¯AM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-20_at_9.28.47_AM.png)\n\n![Screenshot 2023-12-20 at 9.30.31â€¯AM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-20_at_9.30.31_AM.png)\n\nWe used the base model rather than the instruction-tuned model for chat since it makes more sense for our use-case to instruction-tune our model from the base model backbone (out-of-the-box only generates text, i.e., it does not follow instructions well).\n\n\u003Ch4> Training Details\u003C/h4>\n\nGiven the model is fairly large (though it is not on the small end for LLM), extensive hyperparameter tuning not possible due to compute restrictions (7B parameter model)\n\nWe reviewed accepted literature and guides online to make use of more common settings for hyperparameters:\n\n```\nper_device_train_batch_size=2  # Batch size,\ngradient_accumulation_steps=1  # No gradient accumulation steps,\nnum_train_epochs=1  # Fine-tune on entire dataset\nlearning_rate=2.5e-5  # Want a small lr for finetuning\nbf16=True  # Use mixed precision training with bfloat16,\noptim=\"paged_adamw_8bit\"  # Use 8-bit AdamW\nlogging_steps=25  # When to start reporting loss\nlogging_dir=LOGGING_DIR  # Directory for storing logs\nsave_strategy=\"steps\"  # Save the model checkpoint every logging step\nsave_steps=25  # Save checkpoints every 25 steps\nevaluation_strategy=\"steps\"  # Evaluate the model every logging step\neval_steps=25  # Evaluate and save checkpoints every 25 steps\n```\n\nWe fine-tune on a subset of the [CoEdIT](https://huggingface.co/datasets/grammarly/coedit) dataset (around 40000 samples). The backbone of the model was not changed during fine-tuning since the architecture is already very robust for generation tasks.\n\nWe used VM instance with 8 vCPUs, 40GB RAM, 24GB VRAM (NVIDIA RTX A5000) to fine-tune the model. We use techniques to significantly improve the efficiency of training with little to no performance sacrifice. The basic premise of the fine-tuning was the following - we utilize a parameter-efficient fine-tuning method (PEFT) called Quantized Low-Rank Adaptation (QLoRA). \n\nIn the past, fine-tuning large LLMs was a resource-intensive task, requiring substantial computational power and high-end GPU resources, thus limiting accessibility. Traditional methods involved fine-tuning the model on extensive datasets, followed by a step called 4-bit quantization for the model to function on consumer-grade GPUs after fine-tuning. This approach reduced resource usage but at the cost of the model's full capabilities, leading to compromised results.\n\nQLoRA emerges as a significant advancement within the realm of parameter-efficient fine-tuning (PEFT). PEFT aims to modify only a small fraction of a model's parameters, making fine-tuning more efficient and less resource-intensive. QLoRA aligns with this goal by allowing for the efficient fine-tuning of large language models using a single GPU. It maintains the high-level performance of a full 16-bit model even when reduced to 4-bit quantization. For perspective, while traditional methods required something like 780 gigabytes of VRAM to fine-tune a 65 billion parameter model, QLoRA achieves this with just a single 48 gigabyte VRAM GPU. This approach not only fits within the PEFT paradigm but also makes such advanced modeling far more attainable and practical.\n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%208.png)\n\n## Evaluation\n\n### **Feedback Scores (Bert-cased)**\n\n\u003Ch4> Train and Validation Performance\u003C/h4>\n\nThe train and validation loss of the model can be seen below: \n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%209.png)\n\nFrom this we can see that the model is performing quite well on the out-of-sample data, in-fact, it is performing better on the samples than on the training data itself. \n\nAdditionally, we tested two different `bert` models - `cased` and `uncased`. Testing on both models, it was clear that the `cased` model was better:\n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%2010.png)\n\nFrom the above, we concluded that the casing of a word, and by extension, how casing is applied within the context of a sentence alters its correctness on various metrics, and by using a uncased model and grouping these words together we were losing this signal. \n\nHence, we finally decided to stick with the `cased` model with the hyper-parameters discussed above.\n\n### **Essay Dissection (Longformer)**\n\n\u003Ch4> Train and Validation Performance\u003C/h4>\n\nFor this model, we compute the cross entropy loss. We compute the validation with the following formula: `number of correct predictions / total number of tokens`. Our model had an **accuracy of about 70%** on the testing data. Some of the performance curves can be seen below:\n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%2011.png)\n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%2012.png)\n\n### Grammar Correction (Mistral 7B)\n\n\u003Ch4> Train and Validation Performance\u003C/h4>\n\nThe train and validation loss of the model can be seen below: \n\n![train_loss (1).svg](../../assets/images/projects/grammar-ninja/report/train_loss_(1).svg)\n\n![val_loss (1).svg](../../assets/images/projects/grammar-ninja/report/val_loss_(1).svg)\n\nNote for the following before-after tests the final output is post-processed by cutting off the next sequence of conversations (prevent model from continuing indefinitely or until token limit)\n\n\u003Ch4> Benchmark Performance vs Fine-tune Performance\u003C/h4>\n\nWe compare the performance of the model before fine-tuning on the dataset (benchmark) and the fine-tuned model. In the figures below are the content after the `### Response` tag are generated:\n\n\u003Ch4> Before Fine-tuning\u003C/h4>\n\n```\n### Instruction:\nRemove grammar mistakes\n\n### Input:\nNLP, it stand for Natural Language Processing, is a field in computer science, where focus on how computers can understanding and interact with human language.\n\n### Response:\nNLP, it stands for Natural Language Processing, is a field in computer science, where focus on how computers can understand and interact with human language.\n```\n\n\u003Ch4> After Fine-tuning\u003C/h4>\n\n```\n### Instruction:\nRemove grammar mistakes\n\n### Input:\nNLP, it stand for Natural Language Processing, is a field in computer science, where focus on how computers can understanding and interact with human language.\n\n### Response:\nNLP, it stands for Natural Language Processing, is a field in computer science where the focus is on how computers can understand and interact with human language.\n```\n\nAs can be seen, fine-tuning improves grammar correction of LLM compared to out-of-the-box.\n\n## Presentation of Results\n\n### Feedback Scores (Bert-cased)\n\n![Untitled](../../assets/images/projects/grammar-ninja/report/Untitled%2013.png)\n\nFor the feedback score model, we have the above frontend to display the results as percentages. We planned to combine all three models into this UI, but due to time constraints were not able to complete this. \n\n### Inference Scripts\n\nWe created inference scripts that can be used to test our trained models. They can be accessed on our repo by going to the tools folder and visiting one of the three models. More details are provided on the README of our GitHub repo for setup.\n\nHere is an example text that we use the test our trained models:\n\n```bash\ncat ../../examples/nlp.txt\n```\n\n```\nNLP, it stand for Natural Language Processing, is a field in computer science, where focus on how computers can understanding and interact with human language. It's goal is to make computers can understand and respond to text or voice data. But, it's hard because languages is very complex and have many rules that often not follow logic.\n\nIn field of NLP, machine learn algorithms is used for make computers can process and analyze large amounts of natural language data. The problems is that, even with advanced algorithms, computers often don't understand the nuances, like sarcasm or idioms, in human languages. So, many times, they makes errors when they tries to interpret what a human is saying or writing.\n```\n\nBelow our the outputs of our model for the example sentence.\n\n### Feedback Scores (Bert-cased)\n\n```bash\ncd feedback_scores\npython inference.py ../../examples/nlp.txt\n```\n\n![Screenshot 2023-12-22 at 5.28.39â€¯PM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-22_at_5.28.39_PM.png)\n\n### Essay Dissection (Longformer)\n\n```bash\ncd essay_dissection\npython inference.py ../../examples/nlp.txt\n```\n\n![Screenshot 2023-12-22 at 5.25.07â€¯PM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-22_at_5.25.07_PM.png)\n\n### Grammar Correction (Mistral 7B)\n\n- Note that this model takes a while to run if you are not using GPU\n    - since it involves multiple sliding windows if text is too long\n\n```bash\ncd grammar\npython inference.py ../../examples/nlp.txt\n```\n\n![Screenshot 2023-12-22 at 5.39.22â€¯PM.png](../../assets/images/projects/grammar-ninja/report/Screenshot_2023-12-22_at_5.39.22_PM.png)\n\n## Future Work\n\nFor the BERT model specifically, we had some thoughts on how improve the model selection and the training process. Ideally, we would like to work with larger datasets and perform Hill Climbing because it can take lots of models and pick the best small subset of models. Additionally, we could add more specificity to the loss function - custom different loss rate per target; 2 stage pooling. First pool either words, sentences, or paragraphs and then also using different max_lens for training and inference. Also, it seems like there could be merits to doing some form of ensemble models though architecting this might be different with pre-trained large language models. \n\nIn the essay dissection model, we discussed utilizing a dual-Longformer setup that could potentially enhance our model's accuracy as seen [here](https://www.kaggle.com/code/abhishek/two-longformers-are-better-than-1). To counteract the skewed learning from imbalanced datasets, we also discussed implementing data rebalancing strategies such as SMOTE, which generates synthetic samples for minority classes, thus enhancing the training process. \n\nIn our grammar improvement LLM, we currently combine a set of prompts (clarity, coherence, grammatical error correction, and neutralize the sentence)\n\nWe decided to not fully utilize the entire CoEdIT dataset, since we thought that it wouldnâ€™t directly relate to improving a sentence. These additional capabilities can be trained by including them in future fine-tuning runs and manifested in a separate UI that allows a user to paraphrase and/or simplify their writing.\n\nWe also used a barebones CLI inference script. In the future, given more time, we would package the three inference scripts as a [Gradio](https://github.com/gradio-app/gradio/) space to make our demo more interactive.\n\n## Setup\n\n### Environment Variables\n\n```bash\nexport HF_HUB_ENABLE_HF_TRANSFER=1\nexport HF_HOME=$HOME/.cache/huggingface/transformers\n```\n\n### Install Environment\n\n```bash\n# install dependencies\nconda env create --file config/${CONDA_ENV_FILE}\n# install grammar-ninja package\npip install -e .\n# download spacy model\npython -m spacy download en_core_web_md\n```\n\n## Demo\n\n```bash\ncd tools\n```\n\n### Essay Dissection (Longformer)\n\n```bash\ncd essay_dissection\npython inference.py ../../examples/nlp.txt\n```\n\n### Feedback Scores (Bert-cased)\n\n```bash\ncd feedback_scores\npython inference.py ../../examples/nlp.txt\n```\n\n### Grammar Correction (Mistral 7B)\n\n```bash\ncd grammar\npython inference.py ../../examples/nlp.txt\n```\n\n## ðŸ‘¥ Collaborators\n\n* Vineet Raju (Boston University)\n* Dhruv Chandwani (Boston University)","src/content/projects/grammar-ninja.mdx",[69],"../../assets/images/projects/grammar-ninja/thumbnail.png","d085b12fa5b39163","grammar-ninja.mdx","hand-gesture-recognition",{"id":72,"data":74,"body":78,"filePath":79,"assetImports":80,"digest":82,"legacyId":83,"deferredRender":23},{"title":75,"cover":76,"githubUrl":77},"ðŸ–– Hand Gesture Recognition","__ASTRO_IMAGE_../../assets/images/projects/hand-gesture-recognition/thumbnail.gif","https://github.com/lavaman131/hand_gesture_recognition","import { Image } from \"astro:assets\";\nimport confusionMatrix from \"../../assets/images/projects/hand-gesture-recognition/reports/confusion_matrix.png\";\nimport gui from \"../../assets/images/projects/hand-gesture-recognition/reports/gui.png\";\nimport template1 from \"../../assets/images/projects/hand-gesture-recognition/binary_hands/1.png\";\nimport template2 from \"../../assets/images/projects/hand-gesture-recognition/binary_hands/2.png\";\nimport template3 from \"../../assets/images/projects/hand-gesture-recognition/binary_hands/3.png\";\nimport template4 from \"../../assets/images/projects/hand-gesture-recognition/binary_hands/4.png\";\nimport template5 from \"../../assets/images/projects/hand-gesture-recognition/binary_hands/5.png\";\nimport hand1 from \"../../assets/images/projects/hand-gesture-recognition/hands/1.png\";\nimport hand2 from \"../../assets/images/projects/hand-gesture-recognition/hands/2.png\";\nimport hand3 from \"../../assets/images/projects/hand-gesture-recognition/hands/3.png\";\nimport hand4 from \"../../assets/images/projects/hand-gesture-recognition/hands/4.png\";\nimport hand5 from \"../../assets/images/projects/hand-gesture-recognition/hands/5.png\";\n\n## ðŸ“š Problem Definition\n\nThe problem is to recognize sign-language hand gestures from a video stream. This is useful because it can be used to create human computer interfaces that are more accessible to people with hearing disabilities. My analysis assumes that the background is relatively static and that the hand is the only moving object in the video stream.\n\nSome difficulties that I anticipate are:\n\n- The hand can be in different orientations and positions in the video stream.\n- The hand can be in different lighting conditions.\n- The hand can be occluded by other objects in the video stream.\n- The hand can be in motion.\n\nThe gestures are defined as follows:\n\n- **One**: The thumb is extended and the other fingers are closed.\n- **Two**: The thumb and the index finger are extended and the other fingers are closed.\n- **Three**: The thumb, index finger, and middle finger are extended and the other fingers are closed.\n- **Four**: The thumb, index finger, middle finger, and ring finger are extended and the little finger is closed.\n- **Five**: All fingers are extended.\n\n\u003Cdiv style=\"display: flex; justify-content: space-between; gap: 10px; margin: 20px 0;\">\n    \u003CImage src={hand1} alt=\"Hand 1\" style=\"flex: 1; max-width: 19%; height: auto;\" />\n    \u003CImage src={hand2} alt=\"Hand 2\" style=\"flex: 1; max-width: 19%; height: auto;\" />\n    \u003CImage src={hand3} alt=\"Hand 3\" style=\"flex: 1; max-width: 19%; height: auto;\" />\n    \u003CImage src={hand4} alt=\"Hand 4\" style=\"flex: 1; max-width: 19%; height: auto;\" />\n    \u003CImage src={hand5} alt=\"Hand 5\" style=\"flex: 1; max-width: 19%; height: auto;\" />\n\u003C/div>\n\n## ðŸ› ï¸ Method and Implementation\n\n### Classical Computer Vision Algorithms\n\nI use binary image analysis followed by max contour detection for the segmentation of the hand. I also use template matching (with templates augmented via different scales and rotations to capture possible scales and orientations of the hand) with the maximum normalized correlation coefficient for classifying the hand movement as the digit 1, 2, 3, 4, or 5. More detailed analysis of the algorithms used is provided below.\n\n#### Binary Image Conversion Using Skin Color\n\nBy utilizing thresholding of RGB, HSV, and YCRB color profiles utilizing a combination of bitwise AND along with bitwise OR operators (which can be seen in the `color_model_binary_image_conversion` function I created [here](./a2/data/preprocessing.py) in my code) of an image as described in [this paper](https://arxiv.org/pdf/1708.02694.pdf), I was able to segment skin in an image with high accuracy. Using this mask, I was able to convert the original image to a binary image where the skin is black and the background is white. This binary image is then used for contour detection and template matching in the following steps.\n\n#### Contour Detection for Hand Segmentation\n\nAfter converting the image to a binary format, the next step is to segment the hand from the background. This is achieved through contour detection, a process used to find the outlines of objects within binary images. The algorithm I employ for this purpose is based on the principle of finding continuous curves that delineate object boundaries in an image. The specific steps are as follows:\n\n1. **Find Contours**: Utilize OpenCV's `findContours` function, which implements the Suzuki85 algorithm, to detect the contours in the binary image. This function returns a list of contours found, with each contour represented as a vector of points.\n\n2. **Select Max Contour**: Among all detected contours, the one with the maximum area is considered to represent the hand. This is based on the assumption that the hand is the largest skin-colored object in the image. The area of a contour is calculated using OpenCV's `contourArea` function.\n\n3. **Draw/Use Contour**: The maximal contour is then used either to create a mask for the hand or to extract the hand's outline for further processing. This step is crucial for isolating the hand from the rest of the image, ensuring that subsequent steps operate solely on the hand region.\n\nMathematically, contour detection can be seen as the process of identifying the boundaries $C$ of a region $R$ in the binary image where $C = \\partial R$. The Suzuki85 algorithm efficiently traces these boundaries by examining the connectivity of edge pixels in the image.\n\n#### Template Matching for Sign Language Digit Classification\n\nTemplate matching is a technique in computer vision used for finding areas of an image that match a template image. In the context of classifying hand movements as digits 1 through 5, I leverage template matching to compare the segmented hand region against a set of pre-defined templates corresponding to each digit. The process involves:\n\n1. **Template Preparation**: Generate a set of template images for each digit (1-5), capturing various scales and orientations to account for different hand sizes and positions. This augmentation ensures a comprehensive set of references for matching.\n\n\u003Cdiv style=\"display: flex; justify-content: space-between; gap: 10px; margin: 20px 0;\">\n    \u003CImage src={template1} alt=\"Template 1\" style=\"flex: 1; max-width: 19%; height: auto;\" />\n    \u003CImage src={template2} alt=\"Template 2\" style=\"flex: 1; max-width: 19%; height: auto;\" />\n    \u003CImage src={template3} alt=\"Template 3\" style=\"flex: 1; max-width: 19%; height: auto;\" />\n    \u003CImage src={template4} alt=\"Template 4\" style=\"flex: 1; max-width: 19%; height: auto;\" />\n    \u003CImage src={template5} alt=\"Template 5\" style=\"flex: 1; max-width: 19%; height: auto;\" />\n\u003C/div>\n\n2. **Normalized Correlation Coefficient**: For each template, calculate the match with the segmented hand using the normalized correlation coefficient. This metric measures the similarity between the template and portions of the target image, with a range from -1 (no match) to 1 (perfect match). Mathematically, it is defined as:\n\n$$\n\\begin{align*}\n&\\text{NCC}(T, I) = \\frac{\\sum_{x,y} [T(x,y) - \\bar{T}][I(x,y) - \\bar{I}]}{\\sqrt{\\sum_{x,y} [T(x,y) - \\bar{T}]^2 \\sum_{x,y} [I(x,y) - \\bar{I}]^2}} \\\\\n&\\text{where } T \\text{ is the template, }\\\\\n&I \\text{ is the image under examination, and }\\\\\n&\\bar{T}, \\bar{I} \\text{ are the mean values of } T \\text{ and } I \\text{ respectively.}\n\\end{align*}\n$$\n\n3. **Matching and Classification**: For each digit's set of templates, compute the NCC across the hand region. The digit whose template yields the highest NCC is determined to be the hand's sign. This approach effectively classifies the hand gesture by finding the most similar template in terms of shape and orientation.\n\nBy employing these algorithms, the system can segment the hand from the background and classify its gesture into one of the five digits, using the principles of binary image analysis, contour detection, and template matching.\n\n## ðŸ”¬ Experiments\n\nI conducted the following experiments to evaluate the performance of the hand gesture recognition system and logged the results in the `./experiments` directory with [hydra](https://hydra.cc/docs/intro/) configuration files. See more details about this in the [usage section](#-usage) of the README.\n\n### Different Region of Interest (ROI) Sizes\n\nI experimented with different regions of interest (ROI) to evaluate the performance of the hand gesture recognition system. The ROI is defined as the location in each frame where the processing happens (visually depicted by a green rectangle in my GUI). Intuitively, I saw the best performance when the ROI was centered around the hand. I decided to use an ROI width and height of 640 pixels and 790 pixels respectively. This is because using the full frame as the ROI would skew the results of the segmentation of the hand as the background would be included in the processing. Another practical implication is that the user can move their face outside of the ROI and not have it interfere with the processing of the hand gesture recognition system. Finally, the implementation of an ROI allows for a more intuitive and user-friendly experience for the user as they can see where the processing is happening.\n\n### Different Gamma Levels for Source Image\n\nI experimented with different gamma levels for the source image to evaluate the performance of the hand gesture recognition system. The gamma level is a parameter that controls the brightness of the source image. After experimenting with gamma levels from 0.1 to 1.2, I found that the best performance was achieved when the gamma level was set to 0.375. This is because it allowed there to be a greater contrast between the hand and the background. The gamma level can be changed based on the lighting conditions of the environment.\n\n### Template Matching\n\n#### Different Scales and Rotations\n\nTraditional template matching is very error prone because it relies on the exact match of the template with the image in terms of size and orientation. To overcome this, I experimented with different scales and rotations of the template images to evaluate the performance of the hand gesture recognition system. I found that the best performance was achieved when the template images were augmented with different scales and rotations. This is because it allowed the system to recognize the hand gesture even if the hand was in different orientations and positions in the video stream. The system was able to recognize the hand gesture even if the hand was occluded by other objects in the video stream.\n\nI ended up using the following scales (scale factor) and rotations (in degrees) for the template images:\n\n- **Scales**: [0.7, 0.8, 0.9, 1.0]\n- **Rotations**: [-20.0, -10.0, 0.0, 10.0, 20.0]\n\n#### Front and Back Camera Template Images\n\nAnother important experiment I conducted was to evaluate the performance of the system with template images taken with a front and back camera. I found that the system was able to perform better when the template image were taken with the same type of camera as the intended use. This is because the two different cameras lead to different orientations and shapes of the hand. I have included the template images taken with the front and back camera in the `./templates` directory. The template image can be specified by the user depending on whether they are using the front or back camera in the GUI (see the [advanced usage section](#advanced-usage) of the README for more details about this).\n\n#### Hand Template Image vs Finger Template Image\n\nI experimented with using a hand template image and a finger template image to evaluate the performance of the hand gesture recognition system. I found that the system was able to perform  much better with a hand template image. This is because the hand template image was able to capture the shape and orientation of the hand better than the finger template image.\n\n## ðŸ“ˆ Results\n\nI ran 20 trials with each finger to evaluate the performance of the hand gesture recognition system. For instance, I used the following command for the sign language digit 5 with 20 frames and a 5-second delay to start the camera and allow the user to prepare their hand for the gesture recognition system to capture the frames and process them for classification:\n\n```bash\npython main.py \\\nhydra.job.name=class_5 \\\nprocessing.ground_truth_label=5 \\\ncamera.num_frames=20 \\\ncamera.start_delay_seconds=5\n```\n\nThe following are the results of the experiments which I logged in the `./experiments` directory. Then I used the `./tools/evaluate.py` script to evaluate the performance of the hand gesture recognition system and logged the results in the `./reports` directory.\n\n### Confusion Matrix\n\n\u003CImage src={confusionMatrix} alt=\"Confusion Matrix\" />\n\n## ðŸ—£ï¸ Discussion\n\nThe performance of the system is impressive given the simplicity of the algorithms used. In the experiments I ran, the system was able to classify the sign-language digit 5 with 100% accuracy. However, the environment I ran my experiments in was relatively controlled with a static background and good lighting conditions. Using binary image analysis, max contour detection for the segmentation of the hand, and template matching proved to be a basic system to recognize sign-language hand gestures from a video stream.\n\n## ðŸ† Conclusions\n\nWhile it is possible to recognize sign-language hand gestures from a video stream using classical computer vision algorithms, the performance of the hand gesture recognition system is far from perfect. Additionally, the amount of processing that occurs per frame makes the algorithm very slow (re-implementing things in C++ could help ðŸ˜Š). The system works best when the user tries to shape their hand to mimic the template images for the sign-language digit (1-5) that they are trying to automatically classify. The hand should be still and the background should be relatively static with not too much overexposure or underexposure in the camera. The system will not work well if the hand is in motion or if the background is not relatively static. Future work could involve using machine learning or deep learning algorithms to improve the performance of the hand gesture recognition system.\n\n## ðŸŽ¬ Credits and Bibliography\n\n[Gamma Correction](https://docs.opencv.org/3.4/d3/dc1/tutorial_basic_linear_transform.html)\n\n[Gamma Correction 2](https://pyimagesearch.com/2015/10/05/opencv-gamma-correction/)\n\n[Skin Detection](https://arxiv.org/pdf/1708.02694.pdf)\n\n[Contour Approximation](https://pyimagesearch.com/2021/10/06/opencv-contour-approximation/)\n\n[Template Matching](https://pyimagesearch.com/2021/10/11/opencv-template-matching/)\n\n## ðŸ‘¥ Collaborators\n\n* None","src/content/projects/hand-gesture-recognition.mdx",[81],"../../assets/images/projects/hand-gesture-recognition/thumbnail.gif","143dbfc5bf64d156","hand-gesture-recognition.mdx","human-connectome",{"id":84,"data":86,"body":90,"filePath":91,"assetImports":92,"digest":94,"legacyId":95,"deferredRender":23},{"title":87,"cover":88,"githubUrl":89},"Detecting Brain Regions Associated with Autism Development","__ASTRO_IMAGE_../../assets/images/projects/human-connectome/thumbnail.gif","https://github.com/lavaman131/human_connectome","import { Image } from \"astro:assets\";\nimport connectome_diagram from \"../../assets/images/projects/human-connectome/report/connectome_diagram.svg\";\n\n\n## Inspiration\nBrain connectomics or brain network research has rapidly expanded using functional MRI (fMRI) and diffusion-weighted MRI (dwMRI). A common product of these varied analyses is a connectivity matrix (CM). A CM stores the connection strength between any two regions (\"nodes\") in a brain network. This format is useful for several reasons: \n1. it is highly distilled, with minimal data size and complexity\n2. graph theory can be applied to characterize the network's topology\n3. it retains sufficient information to capture individual differences such as age, gender, intelligence quotient (IQ), or disease state\n\nIn this project, the connectivity matrices of patients diagnosed with autism spectrum disorder (ASD) and typically developing (TD) autism are compared using graph statistical algorithms to determine regions of the brain associated with development of autism. \n\n## Statistical Analysis\nThis analysis relies on a simplified 2-complex based order d Formanâ€“Ricci curvature [[1]](#1) of an edge $e = \\{u, v\\} \\in E$ is given by:\n\n$$\\begin{aligned} \\mathfrak {C}_G^{\\,2,d}(e) {\\mathop {=}\\limits ^{\\mathrm {def}}}\\mathfrak {C}_G^{\\,2,d}(u,v) = \\omega (e) \\left[ \\left( \\sum _{e \\sim f_d^2} \\frac{\\omega (e)}{\\omega (f_d^2)} \\,{+}\\, \\sum _{v \\sim e} \\frac{\\omega (v)}{\\omega (e)} \\right) \\ \\text {---}\\ \\sum _{e'||e,\\, e',e \\sim f_d^2} \\frac{ \\sqrt{\\omega (e) \\omega (e')} }{ \\omega (f_d^2)} \\right] \\end{aligned}$$\n\nThe difference of the curvatures is found between a pair of autism spectrum disorder (ASD) and typically developing (TD) autism connectivity matrices each constructed by taking the aggregated mean of all such similar samples corresponding to each diagnosis.\n\n## Results\n\n### Top 10 Statistically Significant Curvature Differences\nIt is important to note that some connections may occur in the same region of the brain, hence it may seem like a vertex is connected to itself which is not the case.\n\n#### 5 Most Negative Curvatures \n\n| Connected Region                                 | Curvature Difference |\n| ------------------------------------------------ | -------------------- |\n| Right Putamen, Right Insular Cortex              | -2331330.433152141   |\n| Right Insular Cortex, Right Insular Cortex       | -1994182.8538039196  |\n| Right Frontal Pole, Right Frontal Orbital Cortex | -1845748.8731592936  |\n| Right Frontal Pole, Right Frontal Pole           | -1716334.5267126393  |\n| Right Insular Cortex, Right Insular Cortex       | -1614932.5721171317  |\n\n#### 5 Most Positive Curvatures \n| Connected Region                                                                             | Curvature Difference |\n| -------------------------------------------------------------------------------------------- | -------------------- |\n| Left Central Opercular Cortex, Left Central Opercular Cortex                                 | 1934234.5367499162   |\n| Right Insular Cortex, Right Putamen                                                          | 1971224.916725241    |\n| Left Thalamus, Brain-Stem                                                                    | 2294358.294582237    |\n| Right Frontal Pole, Right Frontal Orbital Cortex                                             | 2565673.39796964     |\n| Left Middle Temporal Gyrus posterior division, Left Middle Temporal Gyrus posterior division | 3425965.9113210947   |\n\n## Discussion\nThe ten connections between the brain yielding statistically significant results (from most significant to least significant) towards the development of autism are:\n\n1. Left Middle Temporal Gyrus posterior division, Left Middle Temporal Gyrus posterior division \n2. Right Frontal Pole, Right Frontal Orbital Cortex\n3. Right Putamen, Right Insular Cortex   \n4. Left Thalamus, Brain-Stem     \n5. Right Insular Cortex, Right Insular Cortex   \n5. Right Insular Cortex, Right Putamen   \n6. Left Central Opercular Cortex, Left Central Opercular Cortex \n8. Right Frontal Pole, Right Frontal Orbital Cortex\n7. Right Frontal Pole, Right Frontal Pole\n9. Right Insular Cortex, Right Insular Cortex   \n\n### Connections Visualized\n\u003CImage src={connectome_diagram} alt=\"connectome_diagram\" />\n\n## Replicate Results\n\nVisit the [repo](https://github.com/lavaman131/human_connectome) for more information.\n\nAfter running the analysis, the output will be the top 5 most positive and top 5 most negative edges based on curvature difference:\n\n\u003Ctable className=\"table-auto\">\n  \u003Cthead>\n    \u003Ctr>\n      \u003Cth>Connected Regions\u003C/th>\n      \u003Cth>Curvature Difference\u003C/th>\n      \u003Cth>Type\u003C/th>\n    \u003C/tr>\n  \u003C/thead>\n  \u003Ctbody>\n    \u003Ctr>\n      \u003Ctd>Right Putamen â†” Right Insular Cortex\u003C/td>\n      \u003Ctd>-2,331,330.43\u003C/td>\n      \u003Ctd>Negative\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>Right Insular Cortex â†” Right Insular Cortex\u003C/td>\n      \u003Ctd>-1,994,182.85\u003C/td>\n      \u003Ctd>Negative\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>Right Frontal Pole â†” Right Frontal Orbital Cortex\u003C/td>\n      \u003Ctd>-1,845,748.87\u003C/td>\n      \u003Ctd>Negative\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>Right Frontal Pole â†” Right Frontal Pole\u003C/td>\n      \u003Ctd>-1,716,334.53\u003C/td>\n      \u003Ctd>Negative\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>Right Insular Cortex â†” Right Insular Cortex\u003C/td>\n      \u003Ctd>-1,614,932.57\u003C/td>\n      \u003Ctd>Negative\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>Left Central Opercular Cortex â†” Left Central Opercular Cortex\u003C/td>\n      \u003Ctd>1,934,234.54\u003C/td>\n      \u003Ctd>Positive\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>Right Insular Cortex â†” Right Putamen\u003C/td>\n      \u003Ctd>1,971,224.92\u003C/td>\n      \u003Ctd>Positive\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>Left Thalamus â†” Brain-Stem\u003C/td>\n      \u003Ctd>2,294,358.29\u003C/td>\n      \u003Ctd>Positive\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>Right Frontal Pole â†” Right Frontal Orbital Cortex\u003C/td>\n      \u003Ctd>2,565,673.40\u003C/td>\n      \u003Ctd>Positive\u003C/td>\n    \u003C/tr>\n    \u003Ctr>\n      \u003Ctd>Left Middle Temporal Gyrus posterior division â†” Left Middle Temporal Gyrus posterior division\u003C/td>\n      \u003Ctd>3,425,965.91\u003C/td>\n      \u003Ctd>Positive\u003C/td>\n    \u003C/tr>\n  \u003C/tbody>\n\u003C/table>\n\n## References\n\u003Ca id=\"1\">[1]\u003C/a> Chatterjee, T., Albert, R., Thapliyal, S. *et al.* Detecting network anomalies using Formanâ€“Ricci curvature and a case study for human brain networks. *Sci Rep* **11**, 8121 (2021). https://doi.org/10.1038/s41598-021-87587-z\n\n## ðŸ‘¥ Collaborators\n\n* Dr. Tanima Chatterjee (Boston University)","src/content/projects/human-connectome.mdx",[93],"../../assets/images/projects/human-connectome/thumbnail.gif","ae6a4fb23111403f","human-connectome.mdx","object-tracking",{"id":96,"data":98,"body":102,"filePath":103,"assetImports":104,"digest":106,"legacyId":107,"deferredRender":23},{"title":99,"cover":100,"githubUrl":101},"ðŸ‘£ Single Object Tracking, Multi-Object Tracking, and Data Association","__ASTRO_IMAGE_../../assets/images/projects/object-tracking/thumbnail.gif","https://github.com/lavaman131/object_tracking","import { Image } from \"astro:assets\";\nimport mot from \"../../assets/images/projects/object-tracking/report/mot.gif\";\nimport sot from \"../../assets/images/projects/object-tracking/report/sot.gif\";\n\n## ðŸ“š Problem Definition\n\n### Single Object Tracking\n\nIn the realm of computer vision and video processing, tracking the movement of objects across a sequence of frames is a fundamental challenge. One specific aspect of this challenge is Single Object Tracking (SOT), where the goal is to follow the trajectory of a single entity, such as a vehicle, across multiple frames.\n\nThe core task is to process detected 2D locations (x, y coordinates) of a vehicle as it moves across various frames of a video. These detections are inherently noisy due to various factors like camera movement, changes in lighting, and occlusions. The objective is to implement a filtering technique that can generate a smooth and continuous 2D track of the vehicle, mitigating the noise and inaccuracies in the observed locations. The choice of an alpha-beta filter or a Kalman filter allows for a balance between simplicity and effectiveness, leveraging predictions and measurements to estimate the vehicle's current and future states with a higher degree of accuracy.\n\n### Multi-Object Tracking and Data Association\n\nExpanding the challenge to Multi-Object Tracking (MOT), this project addresses the complexity of tracking multiple objects simultaneously. In real-world scenarios, a video may contain several objects of interest, each requiring tracking and identification over time. The task involves processing bounding boxes for multiple detected objects across video frames and efficiently tracking them by assigning a unique ID to each object. This ID must persist as long as the object is present and detected in the video.\n\nThe critical challenge in MOT is the Data Association problem, which involves correctly associating the detected bounding boxes with the corresponding tracks (or IDs) over time, especially in scenarios where objects may intersect, occlude each other, or leave and re-enter the frame. This requires sophisticated algorithms that can manage uncertainties and ambiguities in object detection and identification, ensuring accurate and consistent tracking of multiple objects throughout the video sequence.\n\nThis project aims to develop and implement solutions for both SOT and MOT, leveraging filtering techniques and data association strategies to achieve robust and reliable object tracking in video data.\n\n## ðŸ›  Methods\n\n### Alpha-Beta Filtering in 1D and 2D\n\nTo smooth out the noisy detections and predict future positions of the tracked objects, we implemented the alpha-beta filter, a simplified form of the Kalman filter. The alpha-beta filter is well-suited for real-time applications due to its computational efficiency and ease of implementation.\n\n#### 1D Alpha-Beta Filter\n\nIn the 1D case, the alpha-beta filter estimates the position and velocity along a single axis. The filter operates in two main steps: prediction and update.\n\n**Prediction Step:**\n\n- **Position Prediction:** $\\hat{x}_{k|k-1} = \\hat{x}_{k-1|k-1} + \\hat{v}_{k-1|k-1} \\Delta t$\n- **Velocity Prediction:** $\\hat{v}_{k|k-1} = \\hat{v}_{k-1|k-1}$\n\n**Update Step:**\n\n- **Measurement Residual:** $r_k = z_k - \\hat{x}_{k|k-1}$\n- **Position Update:** $\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + \\alpha r_k$\n- **Velocity Update:** $\\hat{v}_{k|k} = \\hat{v}_{k|k-1} + \\left( \\frac{\\beta}{\\Delta t} \\right) r_k$\n\nWhere:\n- $\\hat{x}_{k|k}$ and $\\hat{v}_{k|k}$ are the updated estimates of position and velocity at time $k$.\n- $z_k$ is the measured position at time $k$.\n- $\\Delta t$ is the time interval between measurements.\n- $\\alpha$ and $\\beta$ are the filter gains controlling the responsiveness to new measurements.\n\n#### 2D Extension\n\nFor 2D tracking, the alpha-beta filter is applied separately to the x and y coordinates. This assumes that the motion in each dimension is independent, which is acceptable in many tracking scenarios.\n\n**Prediction Step:**\n\n- **Position Prediction:** $\\hat{\\mathbf{x}}_{k|k-1} = \\hat{\\mathbf{x}}_{k-1|k-1} + \\hat{\\mathbf{v}}_{k-1|k-1} \\Delta t$\n- **Velocity Prediction:** $\\hat{\\mathbf{v}}_{k|k-1} = \\hat{\\mathbf{v}}_{k-1|k-1}$\n\n**Update Step:**\n\n- **Measurement Residual:** $\\mathbf{r}_k = \\mathbf{z}_k - \\hat{\\mathbf{x}}_{k|k-1}$\n- **Position Update:** $\\hat{\\mathbf{x}}_{k|k} = \\hat{\\mathbf{x}}_{k|k-1} + \\alpha \\mathbf{r}_k$\n- **Velocity Update:** $\\hat{\\mathbf{v}}_{k|k} = \\hat{\\mathbf{v}}_{k|k-1} + \\left( \\frac{\\beta}{\\Delta t} \\right) \\mathbf{r}_k$\n\nWhere:\n- $\\hat{\\mathbf{x}}_{k|k}$ and $\\hat{\\mathbf{v}}_{k|k}$ are the updated estimates of position and velocity vectors.\n- $\\mathbf{z}_k$ is the measured position vector at time $k$.\n\n#### Selection of Filter Gains\n\nThe values of $\\alpha$ and $\\beta$ determine the filter's performance:\n\n- **High $\\alpha$ and $\\beta$:** More responsive to new measurements but less smooth trajectories.\n- **Low $\\alpha$ and $\\beta$:** Smoother trajectories but slower to react to changes.\n\nThese gains are typically tuned empirically based on the characteristics of the motion and the measurement noise.\n\n### Data Association Using the Hungarian Algorithm\n\nIn multi-object tracking, correctly associating detections to existing tracks is crucial. We implemented the Hungarian matching algorithm as our data association approach due to its ability to find the optimal assignment between detections and tracks efficiently.\n\n#### Cost Matrix Construction\n\nWe construct a cost matrix $C$ where each element $c_{ij}$ represents the cost of assigning detection $i$ to track $j$. The cost is computed based on a distance metric, such as the Euclidean distance between the predicted position of the track and the detected position:\n\n$$c_{ij} = \\| \\hat{\\mathbf{x}}_{j|k-1} - \\mathbf{z}_i \\| $$\n\n#### Hungarian Algorithm Steps\n\n1. **Initialization:** Start with the cost matrix $C$.\n2. **Row Reduction:** Subtract the minimum value in each row from all the elements of that row.\n3. **Column Reduction:** Subtract the minimum value in each column from all the elements of that column.\n4. **Assignment:** Cover all zeros in the matrix using the minimum number of horizontal and vertical lines. If the number of lines equals the number of tracks, an optimal assignment is found.\n5. **Adjustment:** If not all zeros are covered, find the smallest uncovered value, subtract it from all uncovered elements, and add it to elements at the intersections of the covering lines. Repeat the assignment step.\n\n#### Updating Tracks\n\nAfter obtaining the optimal assignment:\n\n- **Matched Pairs:** Update the state estimates of the tracks using the associated detections.\n- **Unmatched Detections:** Initialize new tracks for these detections.\n- **Unmatched Tracks:** Increase the missed detection count; if it exceeds a threshold, the track is terminated.\n\n#### Advantages\n\n- **Optimal Matching:** Ensures the minimum total cost, reducing the likelihood of incorrect associations.\n- **Scalability:** Efficient for a reasonable number of objects, making it suitable for real-time applications.\n- **Flexibility:** Can incorporate additional factors like appearance similarity by adjusting the cost function.\n\n## ðŸ“ˆ Results\n\n### Single Object Tracking\n\n\u003CImage src={sot} alt=\"Single Object Tracking\" />\n\nThe alpha-beta filter effectively smoothed out the noise in the detected positions, resulting in a more accurate and stable trajectory of the vehicle. By adjusting the filter gains, we achieved a balance between responsiveness and smoothness, accommodating both steady motion and sudden maneuvers.\n\n### Multi-Object Tracking\n\n\u003CImage src={mot} alt=\"Multi-Object Tracking\" />\n\nThe implementation of the Hungarian algorithm for data association enabled reliable tracking of multiple vehicles simultaneously. The algorithm efficiently matched detections to existing tracks, even in challenging scenarios with occlusions and crossing paths. The combination of alpha-beta filtering for state estimation and optimal assignment through the Hungarian algorithm resulted in robust multi-object tracking performance.\n\n## ðŸ‘¥ Collaborators\n\n* None","src/content/projects/object-tracking.mdx",[105],"../../assets/images/projects/object-tracking/thumbnail.gif","f920658c3d6b92f6","object-tracking.mdx","plantify-dr",{"id":108,"data":110,"body":114,"filePath":115,"assetImports":116,"digest":118,"legacyId":119,"deferredRender":23},{"title":111,"cover":112,"githubUrl":113},"Plant Disease Detection App","__ASTRO_IMAGE_../../assets/images/projects/plantify-dr/thumbnail.png","https://github.com/lavaman131/PlantifyDr","import { Image } from \"astro:assets\";\n\nimport Resources from \"../../assets/images/projects/plantify-dr/report/Resources.png\";\nimport Flowchart from \"../../assets/images/projects/plantify-dr/report/Flowchart.jpg\";\nimport ResNet50 from \"../../assets/images/projects/plantify-dr/report/ResNet50.png\";\nimport SkipConnection from \"../../assets/images/projects/plantify-dr/report/SkipConnection.png\";\nimport CosineAnnealing from \"../../assets/images/projects/plantify-dr/report/CosineAnnealing.png\";\nimport TomatoTable from \"../../assets/images/projects/plantify-dr/report/Tomato_Table.png\";\nimport TomatoGraph from \"../../assets/images/projects/plantify-dr/report/Tomato_Graph.png\";\nimport TomatoConfusionMatrix from \"../../assets/images/projects/plantify-dr/report/Tomato_Confusion_Matrix.png\";\nimport TomatoResults from \"../../assets/images/projects/plantify-dr/report/Tomato_Results.png\";\n\n## Video Explanation & Demo\n\n\u003Ciframe \n  width=\"750\" \n  height=\"422\"\n  src=\"https://www.youtube.com/embed/uRbDXtMIHRk\" \n  title=\"PlantifyDr Demo\"\n  frameborder=\"0\" \n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" \n  allowfullscreen\n/>\n\n## Built With\n\n- [fastai](https://docs.fast.ai/) - A deep learning library written in Python based off of PyTorch\n- [Flask](https://flask.palletsprojects.com/en/1.1.x/) - Used as web framework for deployment\n- [Docker Compose](https://docs.docker.com/compose/) - Used to run application\n- [WSGI](https://wsgi.readthedocs.io/en/latest/what.html) - Web Server Gateway Interface to communicate with web application\n- [AWS](https://aws.amazon.com/) - For hosting ML models\n- [Swift](https://developer.apple.com/swift/) - For creating iOS app\n- [Postman](https://www.postman.com/) - For testing API\n\n\u003CImage src={Resources} alt=\"Resources\" />\n\n\n\n## Project Plan\n\n\u003CImage src={Flowchart} alt=\"Flowchart\" />\n\n## Model architecture\n\n- The power of convolutional neural networks (CNNs), specifically, `ResNet-50` allows computer to detect patterns in the pixels of images that humans cannot see.\n- As opposed to other ResNet variants, ResNet-50 provides compromise of additional model layers (50) while keeping model storage relatively low (~100mb).\n\n  \u003CImage src={ResNet50} alt=\"ResNet50\" />\n\n- Finally, skip connections helps mitigate vanishing gradient and model performance will perform at least as good as previous layer.\n\n  \u003CImage src={SkipConnection} alt=\"SkipConnection\" />\n\n## Deep Learning techniques used\n\n1. **Model training** with `Learning rate (LR) scheduler` using `cosine annealing` as opposed to more traditional LR scheduling from `lr_max/div`Â toÂ `lr_max` where `div` is a number (100000.0 in my case & default for fastai library) for optimal learning rate for better training results\n\n2. **Fine tuning model** withÂ `freeze`Â forÂ `freeze_epochs` (transfer learning)Â then withÂ `unfreeze`Â fromÂ epochsÂ using `discriminative LR` (lower LR for earlier layers and greater LR for later layers)\n\n\u003CImage src={CosineAnnealing} alt=\"CosineAnnealing\" />\n\n## Results\n\nMy final models each achieved a validation `accuracy of >= 99.2%`.\n\n#### Here are my results for tomatos:\n\n\u003CImage src={TomatoTable} alt=\"TomatoTable\" />\n\u003CImage src={TomatoGraph} alt=\"TomatoGraph\" />\n\u003CImage src={TomatoConfusionMatrix} alt=\"TomatoConfusionMatrix\" />\n\u003CImage src={TomatoResults} alt=\"TomatoResults\" />\n\n#### Statistical Analysis:\n\n- The `Matthews correlation coefficient (MCC)` is in essence a correlation coefficient value between -1 and +1 commonly used in machine learning that considers class imbalances. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. It takes into consideration true and false positives and negatives. My model achieved a `MCC of 0.991`.\n- The `F1 score` can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. This is also helpful in considering class imbalances. My model achieved an `F1 score of 0.992`.\n- As shown in the train loss and valid loss columns and the Epochs vs Training and Validation graph my model has a `good fit` after `12 epochs` of training\n  - A `good fit` is identified by a training and validation loss that decreases to a point of stability with a minimal gap between the two final loss values.\n- Therefore, based on the previous statistics it can be inferred that the `validation accuracy` from my results is reliable to diagnose and treat plant diseases.\n\n## Experiment for yourself:\n\nIn your terminal run:\n\n```bash\npip install -r requirements.txt\n```\n\nGet the data from:\nhttps://www.kaggle.com/lavaman151/plantifydr-dataset \n\n## Points of Improvement:\n\n- In the future, I would like to add more plant types and diseases to my dataset.\n- Additionally, I want to add more features like plant nutritional deficiency recognition which can help provide insight into pesticide free and biological treatment of plants.\n- Something else I might have done differently was build my app in React Native or Flutter as it would have allowed me to bundle my iOS and Android app together which would be easier to do as the only developer.\n- Finally, I would like to implement and experiment with more state-of-the art models including XResNet and its variants for transfer learning on different datasets.\n\n## Final Words\n\nI hope my experiences building this app can shed some light into someone who is looking to get started on a similar ML project. This project was my first experience with deep learning and sparked my passion for computer vision. I loved every bit of the four months I spent on this project.\n\n## Get my app\n\nMy app was available previously on the App Store and Google Play Store, however, it is no longer available since I wasn't able to afford to renew my subscription and the cost of maintaining the services for the app was too high at the time.\n\n## ðŸ‘¥ Collaborators\n\n* None","src/content/projects/plantify-dr.mdx",[117],"../../assets/images/projects/plantify-dr/thumbnail.png","9957ddd46584ebe7","plantify-dr.mdx","motion-gpt15",{"id":120,"data":122,"body":126,"filePath":127,"assetImports":128,"digest":130,"legacyId":131,"deferredRender":23},{"title":123,"cover":124,"githubUrl":125},"MotionGPT 1.5","__ASTRO_IMAGE_../../assets/images/projects/motion-gpt1.5/thumbnail.gif","https://github.com/lavaman131/motionGPTv1.5","import { Image } from \"astro:assets\";\nimport flowmdm_failure from \"../../assets/videos/projects/motion-gpt1.5/report/flowmdm_fail.mp4\";\nimport motiongpt_failure from \"../../assets/videos/projects/motion-gpt1.5/report/motiongpt_fail.mp4\";\nimport motiongptv15_pipeline from \"../../assets/images/projects/motion-gpt1.5/report/motiongptv1.5_pipeline.png\";\nimport motiongptv15_success from \"../../assets/videos/projects/motion-gpt1.5/report/motiongptv15_success.mp4\";\n\n\n## Introduction\n\nThe field of computer vision has witnessed significant advancements in the development of photorealistic avatars for various applications, including virtual reality, gaming, robotics, and medicine. However, generating realistic and seamless human motion compositions remains a challenging task due to the complex nature of human movement and the lack of datasets providing long motion sequences with diverse textual annotations.\n\nRecent works have explored different approaches to address this problem. FlowMDM, a pioneering diffusion-based model, introduced Blended Positional Encodings (BPE) and Pose-Centric Cross-Attention (PCCAT) to generate smooth and realistic motion compositions without the need for post-processing or additional denoising steps. The model achieves state-of-the-art results on the HumanML3D and Babel datasets. Additionally, FlowMDM introduces two new metrics, Peak Jerk (PJ) and Area Under the Jerk (AUJ), to better assess the smoothness and realism of generated motion compositions.\n\nAnother notable work is MotionGPT, a unified motion-language model that handles multiple motion-relevant tasks by treating human motion as a specific language. MotionGPT employs discrete vector quantization to transfer 3D motion into motion tokens, enabling language modeling on both motion and text in a unified manner. The model achieves state-of-the-art performances on tasks such as text-driven motion generation, motion captioning, motion prediction, and motion in-between.\n\nHowever, FlowMDM struggles with very complex descriptions and does not generalize well to out of distribution motions. This makes it hard to adapt to new motion generation applications. Additionally, FlowMDM is not powered by natural language descriptions, making it difficult to integrate in multimodal language models. Additionally, MotionGPT struggles to generate effective 3D motions via qualitative analysis, perhaps overfitting to the training dataset. By using a hybrid approach of combining aspects of FlowMDM and MotionGPT, we are able to improve generalizability of human composition models while providing greater accessibility to these systems via natural language-powered descriptions.\n\nBuilding upon the groundbreaking work of FlowMDM and MotionGPT, we introduce MotionGPTv1.5, a language-enhanced human motion composition model that pushes the boundaries of seamless and realistic motion generation. Our approach makes the first steps to leverage the motion generation pipeline of these models and incorporates natural language text prompts to improve performance on out-of-distribution data.\n\nBy utilizing a language model to effectively prompt the motion generation backbone, MotionGPTv1.5 can generate more diverse and coherent motion compositions, even when faced with novel or unseen scenarios. The integration of natural language prompts allows for a more intuitive and user-friendly interface, enabling users to guide the motion generation process with simple textual descriptions as well as better generalizability to extrapolated motions.\n\n## Method\n\nOur method works like the following (as summarized in the figure below).\n\n1. A user inputs a natural language prompt to interact with an off-the-shelf LLM (Llama 3 in our case). The userâ€™s query is converted to an embedding and compares the similarity of the word embedding to relevant motions that FlowMDM was trained on which are stored in a vector database. The matching word embeddings in the vector database are then injected into the following system prompt for the LLM with a `{motion_dictionary}` placeholder key that allows the model to contextualize the actions that the model, FlowMDM, was conditioned on. The assumption here is that FlowMDM has been sufficiently trained on a reasonable set of â€œbase actionsâ€ that can be combined to produce a more complex action.\n2. The LLM will generate an output a JSON object depending on the userâ€™s request like the following (note that `\u003Cmotion>` and `\u003C/motion>` special tokens surround the JSON text to appropriately parse the following). In this stage, since the model has no prior knowledge of the required durations of each action, we omit the generation of action durations. This can optionally be included with a reasonable prompt as well, but we choose to fine-tune a custom motion duration prediction model with DistilBERT, yielding reasonable results. We have found that the model does not struggle at all to produce the expected format.\n\n```json\n{\n  \"text\": [\n    \"stand\",\n    \"lift right leg\",\n    \"move right leg away from left leg\",\n    \"lower right leg\"\n  ]\n}\n```\n\n3. As explained previously, LLMs provide arbitrary information when asked to guess the duration of each motion of an exercise, so we fine-tune DistilBERT with a lightweight LSTM output layer proceeding the final hidden layer of BERT. We then proceed with a token regression task to predict the duration for each motion in seconds. For more information please refer to the supplementary code and the [file](https://github.com/lavaman131/motionGPTv1.5/blob/main/src/mgpt/nlp/utils/duration_extraction.py) in the repo.\n4. Finally, the output of DistilBERT is post-processed back to the word resolution by averaging the durations of the same word id for a given token because of how tokenization is handled by BERT. The duration is then scaled by the frames per second of our video (in this case 30 fps) yielding a final output that looks like this:\n\n```json\n{\n  \"text\": [\n    \"stand\",\n    \"lift right leg\",\n    \"move right leg away from left leg\",\n    \"lower right leg\"\n  ],\n  \"lengths\": [67, 94, 176, 91]\n}\n```\n\n\u003CImage src={motiongptv15_pipeline} alt=\"MotionGPTv1.5 Pipeline\" />\n\n## Results\n\n### MotionGPTv1.5 Generated Motion\n\nHere is the same example for hip abduction on MotionGPTv1.5 using the user prompt: Can you generate the hip abduction motion?\n\n\u003Cdiv className=\"flex justify-center my-8\">\n  \u003Cvideo controls width=\"480\" height=\"240\">\n    \u003Csource src={motiongptv15_success} type=\"video/mp4\" />\n    Your browser does not support the video tag.\n  \u003C/video>\n\u003C/div>\n\n### Failure Cases of FlowMDM and MotionGPT\n\nFlowMDM produces a similar failure case as well for the same hip abduction exercise:\n\n```json\n{\n  \"text\": [\"hip abduction\"],\n  \"lengths\": [300] # number of frames to generate at 30fps\n}\n```\n\n\u003Cdiv className=\"flex justify-center my-8\">\n  \u003Cvideo controls width=\"480\" height=\"240\">\n    \u003Csource src={flowmdm_failure} type=\"video/mp4\" />\n    Your browser does not support the video tag.\n  \u003C/video>\n\u003C/div>\n\nFor instance hip abduction, a common physical therapy exercise is not well supported by MotionGPT as seen below:\n\n```markdown\n# Exercise: Hip Abduction\n\n# Prompt:\nStand straight, hold support. Lift one leg sideways, keep straight. Return leg. Alternate sides. Repeat.\n```\n\n\u003Cdiv className=\"flex justify-center my-8\">\n  \u003Cvideo controls width=\"480\" height=\"240\">\n    \u003Csource src={motiongpt_failure} type=\"video/mp4\" />\n    Your browser does not support the video tag.\n  \u003C/video>\n\u003C/div>\n\nThe results are obviously not ideal, but MotionGPTv1.5 is better than out-of-the-box for these models.\n\n## Applications\n\n### Assistive Physical Therapy Chatbot\n\nOne neat application is the ability of MotionGPTv1.5 to aid in the rehabilitation process of patients. Although very preliminary (not perfect), by improving motion generation and language models generalizability to new motions, we can naturally adapt them to represent unseen and more complex motions.\n\n## Conclusions\n\nIn conclusion, MotionGPTv1.5 represents a first step forward in the field of language-enhanced human motion composition. By building upon the existing frameworks of FlowMDM and MotionGPT, it offers improved performance on out-of-distribution data and more intuitive user interaction. While there remain challenges to overcome, particularly in the generation of complex and unseen motions, the progress made by MotionGPTv1.5 is promising. The potential applications, such as in physical therapy, highlight the value of further research and development in this area. We look forward to the continued evolution and refinement of this model.\n\n## ðŸ‘¥ Collaborators\n\n* None","src/content/projects/motion-gpt1.5.mdx",[129],"../../assets/images/projects/motion-gpt1.5/thumbnail.gif","47dfb0ddac24f333","motion-gpt1.5.mdx","reasoning-slms",{"id":132,"data":134,"body":138,"filePath":139,"assetImports":140,"digest":142,"legacyId":143,"deferredRender":23},{"title":135,"cover":136,"githubUrl":137},"DAPO: A Case Study with Small Language Models","__ASTRO_IMAGE_../../assets/images/projects/reasoning-slms/thumbnail.png","https://github.com/lavaman131/reasoning-slms","import { Image } from \"astro:assets\";\nimport all_ablations from \"../../assets/images/projects/reasoning-slms/report/all_ablations.png\";\nimport check_numbers from \"../../assets/images/projects/reasoning-slms/report/check_numbers.png\";\nimport check_answer from \"../../assets/images/projects/reasoning-slms/report/check_answer.png\";\n\n\n## Introduction\n\nOver the past year, we've seen reinforcement-learning-based fine-tuning methods supercharge large language models (LLMs), giving them the ability to â€œthink throughâ€ problems step by step. Proximal Policy Optimization (PPO) got us started, then DeepSeek's Group Relative Policy Optimization (GRPO) showed how you can skip a heavyweight critic network by comparing groups of sampled outputs. Building on that, **Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO)** took things further with:\n\n* **Clip-Higher Strategy**: separate clipping thresholds for â€œtoo smallâ€ and â€œtoo largeâ€ policy updates\n* **Dynamic Sampling**: drop prompts that give no learning signal (all-correct or all-wrong)\n* **Token-Level Loss**: compute gradients per token to avoid bias against long, detailed answers\n* **Overlong Filtering & Reward Shaping**: handle truncated or overly verbose responses more gracefully\n\nThose tricks unlocked better reasoning benchmarks on huge 30-50B-parameter modelsâ€”so we asked ourselves: **can a 0.5B-parameter model learn the same tricks?** In this experiment, we applied DAPO to the lightweight [Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct) and ran ablations to see which pieces really matter when using a smaller language model. For the purposes of this exploration we considered small language models to be less than one billion parameters.\n\n---\n\n## Background\n\n### Proximal Policy Optimization (PPO)\n\nPPO is a workhorse policy-gradient algorithm that strikes a sweet spot between stability and simplicity. At each step, it:\n\n1. Samples a batch of trajectories\n2. Optimizes a *clipped* surrogate objective to keep policy changes small\n3. Uses only first-order gradientsâ€”no expensive second-order trust region calculations\n\nThis gives you reliable, DRL-style training without wrestling with hard constraints. PPOâ€™s proved itself on robotics and Atari; weâ€™re now using it to steer LLMs toward better answers.\n\n\u003Cdetails>\n\u003Csummary>ðŸ¤“ Math highlight (clipped objective)\u003C/summary>\n\n$$\n\\mathcal{L}_{\\mathrm{PPO}}(\\theta) = \\mathbb{E}_t\\bigl[ \\min\\bigl(r_t(\\theta)\\hat A_t,\\;\\text{clip}(r_t(\\theta),1-\\epsilon,1+\\epsilon)\\hat A_t \\bigr)\\bigr],\n$$\n\nwhere $r_t(\\theta)=\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\rm old}}(a_t|s_t)}$.\n\n\u003C/details>\n\n---\n\n### Group Relative Policy Optimization (GRPO)\n\nThe DeepSeek paper introduces an innovative approach for enhancing reasoning capabilities in large language models using reinforcement learning, specifically focusing on Group Relative Policy Optimization (GRPO). Unlike conventional RL methods that rely on separate critic models for evaluating policy performance, GRPO reduces computational costs by estimating performance baselines directly from sampled group outputs.\n\nGRPO works by initially sampling multiple outputs from the existing policy model for each input query. The policy is then optimized by maximizing an objective function that compares the current policy outputs against these sampled outputs. This optimization process uses a clipped policy ratio to stabilize training, closely related to PPO, but notably avoids using an additional large-scale critic model. The optimization reward signals in GRPO are derived from both the accuracy and the correctness of the format, encouraging the model to provide correct answers and adhere to structured response formats.\n\nðŸ” GRPO loss (high-level)\n\n1. Generate $G$ answers $\\{o_i\\}$ for each query\n2. Compute a â€œpseudo-advantageâ€ $\\hat A_{i,t}$ by comparing each tokenâ€™s reward to the group\n3. Apply a clipped update just like PPO, but **no separate critic** required\n\n\u003Cdetails>\n\u003Csummary>ðŸ¤“ Math highlight GRPO Loss function\u003C/summary>\n\n$$\n\\begin{align*}\n\\mathcal{L}_{\\mathrm{GRPO}}(\\theta)\n&= -\\frac{1}{\\sum_{i=1}^G \\lvert o_i\\rvert}\n    \\sum_{i=1}^G \\sum_{t=1}^{\\lvert o_i\\rvert}\n    \\Biggl[\\,\n      \\min\\Bigl(\n        \\frac{\\pi_\\theta\\bigl(o_{i,t}\\mid q,\\,o_{i,\u003Ct}\\bigr)}\n             {\\pi_{\\theta_{\\mathrm{old}}}\\bigl(o_{i,t}\\mid q,\\,o_{i,\u003Ct}\\bigr)}\n        \\,\\hat{A}_{i,t}, \n        \\notag\\\\[-0.5ex]\n&\\quad\\quad\\quad\n        \\mathrm{clip}\\Bigl(\n          \\frac{\\pi_\\theta\\bigl(o_{i,t}\\mid q,\\,o_{i,\u003Ct}\\bigr)}\n               {\\pi_{\\theta_{\\mathrm{old}}}\\bigl(o_{i,t}\\mid q,\\,o_{i,\u003Ct}\\bigr)},\n          1-\\epsilon,\\;1+\\epsilon\n        \\Bigr)\\,\\hat{A}_{i,t}\n      \\Bigr)\n      \\;-\\;\\beta\\,D_{\\mathrm{KL}}\\!\\bigl[\\pi_\\theta\\;\\big\\|\\;\\pi_{\\mathrm{ref}}\\bigr]\n    \\Biggr]\n\\end{align*}\n$$\n\n\u003C/details>\n\nCompared to previous methods, GRPO demonstrates superior efficiency by substantially reducing the requirements and complexity of training resources. DeepSeekâ€™s experimental results show significant improvements in reasoning tasks, outperforming conventional supervised fine-tuning methods. Models fine-tuned with GRPO exhibit strong reasoning capabilities, achieving benchmark performance comparable to industry-leading models like OpenAI's o1 series. Thus, GRPO represents a highly effective and resource-efficient approach for training advanced reasoning capabilities into LLMs.\n\n---\n\n### Decoupled Clip & Dynamic Sampling Policy Optimization (DAPO)\n\nDAPO keeps the GRPO spirit but adds four critical tricks for **stability** and **diversity**:\n\n1. **Clip-Higher Strategy**\n\n   * Use two epsilons ($\\epsilon_{\\rm low},\\epsilon_{\\rm high}$) so you donâ€™t squash exploration too aggressively.\n\n2. **Dynamic Sampling**\n\n   * Skip prompts where all $G$ answers are identical (all right or all wrong). Pull fresh samples so gradients arenâ€™t wasted.\n\n3. **Token-Level Loss**\n\n   * Instead of averaging at the sequence level (which underweights long, detailed answers), sum up per-token contributions.\n\n4. **Overlong Filtering & Reward Shaping**\n\n   * Mask or softly penalize answers truncated by token limits, and apply a gentle length penalty to discourage rambling without crushing good content.\n\n\u003Cdetails>\n\u003Csummary>ðŸ¤“ Math highlight DAPO Loss function\u003C/summary>\n\n$$\n\\begin{aligned}\n\\mathcal{L}_{\\text{DAPO}}(\\theta) &= \\mathbb{E}_{(q,a) \\sim \\mathcal{D}, \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(\\cdot|q)} \\\\\n&\\quad \\left[ \\frac{1}{\\sum_{i=1}^G |o_i|} \\sum_{i=1}^G \\sum_{t=1}^{|o_i|} \\min \\left( r_{i,t}(\\theta) \\hat{A}_{i,t}, \\text{clip}(r_{i,t}(\\theta), 1 - \\epsilon_{\\text{low}}, 1 + \\epsilon_{\\text{high}}) \\hat{A}_{i,t} \\right) \\right] \\\\\n&\\text{s.t. } 0 \u003C |\\{o_i \\mid \\text{is\\_equivalent}(a, o_i)\\}| \u003C G,\n\\end{aligned}\n$$\n\n\u003C/details>\n\nTogether, these tweaks gave DAPO a clear win on the AIME 2024 benchmark, beating GRPO on a 32B-parameter model in half the steps. We wanted to see if those benefits survive down-sizing.\n\n---\n\n### Dr. GRPO â€“ A â€œDone Rightâ€ Revision\n\nDr. GRPO refines GRPO by removing two subtle biases:\n\n* **Length bias**: penalizing longer correct answers\n* **Difficulty bias**: skew from question hardness\n\nIt replaces group-normalized rewards with a simple \"subtract the group mean\" advantage:\n\n$$\n\\tilde A_{i,t} = R(q,o_i) \\;-\\; \\frac{1}{G}\\sum_{j=1}^G R(q,o_j).\n$$\n\nThis brings GRPO closer to vanilla PPO principlesâ€”but still without a standalone critic.\n\n---\n\n## Our Setup\n\n### Integrating DAPO into TRL\n\nWe used the [TRL library](https://github.com/lvwerra/trl), the de-facto RL-for-LLM toolkit, and added DAPOâ€™s algorithms as new methods. Since there was no proper implementation of DAPO on TRL, we implemented a basic version of DAPO in the library. This let us spin up ablations by toggling each feature in a few lines of code.\n\n### Dataset: GSM8K\n\nWe fine-tuned on the [OpenAI Grade School Math 8K (GSM8K)](https://huggingface.co/datasets/openai/gsm8k) dataset. It is a dataset with 8.5K diverse word problems requiring multi-step reasoning. Below is an example of the structure of an instance in the dataset.\n\n```json\n{\n    \"question\": \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\",\n    \"answer\": \"Natalia sold 48/2 = $\u003C\u003C48/2=24>>24$ clips in May. Natalia sold 48+24 = $\u003C\u003C48+24=72>>72$ clips altogether in April and May. 72\"\n}\n```\n\n### Reward Functions\n\nWe combined:\n\n* **Exact-answer reward** (big bonus if final answer matches)\n* **Format reward** (smaller bonus for correct JSON/LaTeX-style structure)\n* **Partial credits** for close or partially correct formats\n* **Soft length penalties** if needed (overlong shaping)\n\n---\n\n## Ablation Studies\n\nWe ran 250-step experiments on Qwen2.5-0.5B. Starting from a **base GRPO** setup, we flipped one switch at a time. The experiments can be viewed [here](https://api.wandb.ai/links/artificial-intelligence-research/3nc1wr0u):\n\n| Method                      | Reward |\n| --------------------------- | :----: |\n| Base GRPO                   |  7.63  |\n| + Overlong Filtering        |  8.36  |\n| + Clip-Higher               |  7.19  |\n| + Soft Overlong Punishment  |  5.03  |\n| + DAPO Token-Level Loss     |  5.45  |\n| + Dr. GRPO Token-Level Loss |  8.00  |\n| + Disable Reward Scaling    |  8.13  |\n| + Remove KL Divergence      |  9.16  |\n| + Dynamic Sampling          |  4.58  |\n\n### DAPO (BNPO) Token-Level Loss\n\nAfter establishing a baseline, we ran ablation experiments one by one with each of our methods. The first method implemented was DAPO Token-Level loss. While GRPO's loss is sequence-level, this loss is token-level. Token-level policy loss calculates gradients per token across a batch, unlike sample-level loss which averages per response and can underweight tokens in longer sequences. This approach ensures fairer gradient contributions regardless of response length, improving training stability and providing healthier control over output length dynamics in long-CoT scenarios.\n\n\n###  Dr. GRPO Token-Level Loss\n\nIn this experiment we changed the loss function to the Dr. GRPO Loss function. This approach removes the length and biases inherent in the original GRPO objective, providing unbiased policy gradients and improving token efficiency by mitigating artificial response length increases.\n\n### Disable Reward Scaling\n\nIn this experiment we disabled the scaling of rewards by the reward standard deviation, as it has shown to introduce bias. This method was suggested in the Dr. GRPO implementation. The Dr. GRPO paper integrates this with their loss function but we are testing them separately.\n\n\n### Clip-Higher Strategy\n\nThe first method implemented was the Clip-Higher strategy. Inside the GRPO loss function, the clip function's epsilon value is decoupled into epsilon-low and epsilon-high. If epsilon-high is increased, then this increases the probability for low-probability token samples to be selected. This results in the increased diversity of samples.\n\n### Overlong Filtering\n\nFor this experiment we implement Overlong Filtering. The rewards for all truncated responses are masked. This prevents confusing the model by penalizing a sound reasoning process because the response is too long.\n\n### Soft Overlong Punishment\n\nFor this experiment we implement a Soft Overlong Punishment to the reward. Consider a punishment threshold $L_{cache}$. This threshold determines how close to the maximum completion length (number of tokens) a response can be before it is penalized with a soft punishment. Once the response length exceeds the maximum completion length, a hard reward penalty of -1 is applied.\n\n### Dynamic Sampling\n\nFor this experiment we implement Dynamic Sampling as it is defined in the DAPO paper. If all of a response's tokens are the same value, then they are filtered out and replaced with a new sample. This helps filter out responses where the model is overconfident on or completely lost on. These responses are replaced with responses where the model can learn more constructively.\n\n### Remove KL Divergence Penalty\n\nFor this experiment we removed the KL Divergence penalty term. During training of reasoning models, the model distribution may diverge significantly from the initial model, and thus the term may not be necessary.\n\n> **Key takeaways**:\n>\n> * **Dr. GRPO loss**, **Overlong Filtering**, **no KL penalty**, and **no reward scaling** all boosted rewards.\n> * DAPO's clip-higher, soft overlong punishment, token-level loss, and dynamic sampling actually hurt on this tiny model.\n\n\u003Cfigure>\n  \u003CImage src={all_ablations} alt=\"Reward curves for all ablations\" />\n  \u003Cfigcaption>\u003Csmall>Reward curves after 250 steps: baseline GRPO vs. each ablation.\u003C/small>\u003C/figcaption>\n\u003C/figure>\n\n\n---\n\n## Final Configuration vs. Baseline\n\nOur **final recipe** for Qwen2.5-0.5B:\n\n* **Loss:** Dr. GRPO token-level\n* **Techniques enabled:** Overlong Filtering\n* **Techniques disabled:** Reward Scaling, KL penalty, Clip-Higher, Dynamic Sampling, Soft Overlong Punishment\n\nCompared to vanilla GRPO:\n\n\u003CImage src={check_numbers} alt=\"Numerical correctness rewards\" />\n\u003CImage src={check_answer} alt=\"Format correctness rewards\" />\n\n* **Numerical accuracy** improved steadily.\n* **Format correctness** saw an even bigger liftâ€”a sign our tweaks help the model stay \"on-script\" with answer formatting.\n\n---\n\n## Results\n\nHere is a test question:\n\n```\nQuestion: John runs a telethon to raise money. For the first 12 hours,\nhe generates $5000 per hour. The remaining 14 hours,\nhe generates 20% more per hour. How much total money does he make?\n\nAnswer: 144000\n```\n\n\nHere is the full reasoning-style answer output by the model:\n\n```\nResponse:\n\u003Cstart_working_out>\n    John generates $5000 per hour for the first 12 hours,\n    so thatâ€™s $5000 Ã— 12 = $60000.\n    For the remaining 14 hours, 20% more per hour means\n    $5000 Ã— 20% = $1000 extra, so $5000 + $1000 = $6000 per hour.\n    Over 14 hours thatâ€™s $6000 Ã— 14 = $84000.\n    In total: $60000 + $84000 = $144000.\n\u003Cend_working_out>\n\u003CSOLUTION>144000\u003C/SOLUTION>\n```\n\n## Discussion\n\n1. **Dr. GRPO is surprisingly powerful** on small modelsâ€”removing that sequence-length bias really matters.\n2. **Overlong Filtering** prevents punishing good reasoning just because you hit a token cap.\n3. DAPOâ€™s fancy token-level loss and dynamic sampling might be overkill (or need retuning) when parameters are limited.\n4. We suspect that tiny models benefit more from bias-fixing and stability tweaks than from aggressive exploration strategies.\n\n### Instruction Tuning Is Crucial\n\nWe found that the quality of the base instruction-tuned model shapes everything downstream. For instance, when we tried Googleâ€™s Gemma 3 1B (with a lighter instruction-tuning regimen), fine-tuned outputs often derailed into gibberish by the end of the reasoning trace. In contrast, the Qwen familyâ€”already robustly instruction-tuned on reasoning datasetsâ€”delivered crisp, human-readable step-by-step answers right out of the gate. This suggests tiny-model DAPO is only as good as your starting point.\n\n### Precision & Attention Kernels\n\nWe also observed that mixed-precision (bfloat16 vs. fp16) and attention implementation (SDPA vs. FlashAttention2) can make or break training stabilityâ€”worth tuning alongside your RL tricks.\n\n---\n\n## Conclusion & Future Work\n\n* **Small-scale DAPO** isn't just a \"big-model only\" party. With the right subset of techniques (Dr. GRPO, overlong filtering, no KL/reg scaling), you can teach a 0.5B-parameter model to reason effectively.\n* **Next steps**: compare against [Group Policy Gradient (GPG)](https://arxiv.org/abs/2504.02546), a minimal RL objective for LLMs, and explore curriculum scheduling on prompt difficulty.\n\n---\n\nFeel free to clone, tinker, and let us know what worksâ€”or doesn'tâ€”at small scale!\n\n\n## ðŸ‘¥ Collaborators\n\n* Zach Gentile (Boston University)","src/content/projects/reasoning-slms.mdx",[141],"../../assets/images/projects/reasoning-slms/thumbnail.png","8cbd9462aeccabe1","reasoning-slms.mdx","responsible-ai-audit",{"id":144,"data":146,"body":150,"filePath":151,"assetImports":152,"digest":154,"legacyId":155,"deferredRender":23},{"title":147,"cover":148,"githubUrl":149},"Birdy: Responsible AI Audit of Twitter","__ASTRO_IMAGE_../../assets/images/projects/responsible-ai-audit/thumbnail.png","https://github.com/lavaman131/responsible-ai-audit","import { Image } from \"astro:assets\";\nimport fig1 from \"../../assets/images/projects/responsible-ai-audit/report/fig1.png\";\nimport fig2 from \"../../assets/images/projects/responsible-ai-audit/report/fig2.png\";\nimport fig3 from \"../../assets/images/projects/responsible-ai-audit/report/fig3.png\";\n\n## Fictitious Case Study\n\n### Background\n\nBirdy, a major social media platform, where users can post text up to a certain number of characters. It has also long faced criticism for its handling of hate speech. To address this, the platform launched CleanTalk, an AI-driven system designed to identify and remove hate speech content. Recognizing the importance of fairness and accuracy in content moderation, Birdy aimed to make CleanTalk a model of responsible AI deployment.\n\nCleanTalk was designed to analyze posts in real-time, flagging content deemed as hate speech for review or automatic removal. The system's effectiveness relied heavily on its training data - a vast collection of social media posts manually labeled by a diverse group of annotators from across the globe. Birdy believed that by leveraging a broad annotator base, CleanTalk could minimize bias and accurately reflect the varied understandings of what constitutes hate speech.\n\n### Deployment\n\nThe development of CleanTalk involved an extensive data collection and annotation phase. Birdy engaged with a wide range of annotators, including freelancers, NGO partners, and in-house teams, to label posts as hate speech or non-hate speech. The platform sought to ensure diversity among annotators in terms of demographic backgrounds, believing this would help capture a wide spectrum of perspectives on hate speech.\n\nUpon deployment, CleanTalk was hailed as a pioneering effort in AI moderation, capable of discerning complex patterns of hate speech while accommodating the nuances of different cultures and languages. The system was trained on millions of posts, with the underlying assumption that a diverse annotator pool would mitigate biases.\n\n### Failure\n\nDespite Birdy's efforts, CleanTalk soon encountered challenges. Users began to notice inconsistent moderation outcomes, where posts of a particular category were flagged and hate speech removed, while other offensive posts of a different category remained untouched. Some investigators began looking in the training data used by CleanTalk and found that it was disproportionately trained with white annotators and their ideas about hate speech. \n\n### Legal Action\n\nThe findings sparked a public outcry, leading to a high-profile class action lawsuit brought by plaintiffs (a group of users of Birdy) who felt they had been wrongly censored by Birdy. The plaintiffs alleged that Birdy, in using CleanTalkâ€™s biased system, were unduly abridging their right to free speech. As part of discovery, the court appointed a third-party neutral auditing agency to further investigate CleanTalk and the initial allegations by users. The results of this audit, though perhaps not enough to hold Birdy liable or not liable, could help push the result of the class action in a particular direction. \n\n### Conclusion\n\nThe Birdy case study underscores the nuanced challenge of training AI models for complex tasks like hate speech detection. It illustrates that diversity in data annotation, while crucial, is not a panacea for bias. Ensuring fairness and accuracy in AI systems requires ongoing evaluation, transparency in methodology, and a commitment to understanding and mitigating the potential biases introduced at every stage of the AI development lifecycle. This case study also highlights the careful balancing act that systems like CleanTalk must endeavor to navigate: how do we decide what is considered hate speech and how do we prioritize user safety while also ensuring free speech?\n\n## Audit Context\n\n### Objectives\n\nBirdy must ensure fairness and accuracy in its content moderation to ensure that all users feel safe using the platform. Given the case study where it was noted that offensive posts of one category were flagged when posts of another category were not flagged, it is critical that Birdy considers legal and technical recommendations to reduce bias in its system amongst different kinds of users. Birdy also needs to find a way to balance interests with its role as a social media platform where content should be moderated versus freedom of expression.\n\n### Prospective Auditors\n\nOur prospective auditor would be a court-appointed neutral auditor. This makes the most sense in a class action context given that this is a role already used in such cases.\n\n### What/When of Audit\n\nWe are auditing Birdyâ€™s content moderation model, which takes in a post and outputs whether or not it is offensive. We seek to understand how the demographic background of data annotators influences Birdyâ€™s hate speech detection model. We aim to identify any bias in the data and in model development, with certain choices that were made. \n\nThe audit is conducted during the discovery phase of the class action lawsuit.\n\n### Methods, Tools, Metrics, and Standards\n\n1. Legal\n\n    a. Debates about content moderation consistently center on the question of who gets to decide what warrants being taken down. There may be certain areas of general consensus (for example, CSAM), but it becomes much less clear in the realm of hate/offensive speech. \n\n    b. In order to measure fairness, weâ€™ll be specifically looking at the standard deviation of TPR based on the annotatorâ€™s demographics. A high standard deviation would therefore suggest that there is wide variety in the TPR between the different groups of annotators. This, compounded with the fact that 80% of the dataset is made up of white annotators, would give us reason to pause and consider the implications of enforcing a system that is so heavily skewed toward one perspective. \n\n2. Technical: Data collection\n\n    a. Social bias frames dataset has text paired with Y/N offensiveness annotations (no, neutral, yes) along with the demographic information about annotators originally collected from various online communities from a University of Washington study\n\n    b. Neutral labels were preserved to avoid flagging a non-offensive tweet â†’ however it leads the model to be more uncertain so we considered neutral as an incorrect classification\n\n        \u003Col type=\"i\">\n\n            \u003Cli>Neutral labels act as a way to â€œregularizeâ€ the models predictions, allowing us to control unfamiliar examples\u003C/li>\n\n            \u003Cli>Consequently neutral predictions can directly be analyzed by human annotators to revise the model performance\u003C/li>\n\n        \u003C/ol>\n\n3. Data preprocessing\n\n    a. Data entries with missing labels were dropped from the dataset\n\n    b. Train and validation sets provided were merged and then split into demographic subsets\n\n    c. Each demographic subset was then split into an 80-20 percent train-validation split\n\n    d. Test set was kept as is and treated as untouched holdout set for further performance analysis\n\n4. For each demographic subset:\n\n    a. Fine-tuned pretrained Distil-Bert model to predict if text is offensive for 450 training steps, measuring train and validation performance\n\n5. Measure discrepancy between each model in training, validation, and holdout set via metrics of FP, TP, FN, TN, precision, recall, ROC-curve analysis.\n\n## Case Study Analysis\n\n### Stakeholders\n\n* Social Media Companies: want to more consistently moderate their hate speech using this system, flaws in the system could backfire on the social media companies & create new problem\n* Individual Users: tension & differing interests between users who (a) posted harmful content and (b) were exposed to harmful content and feel like theyâ€™ve been harmed \n* Civil Rights/Advocacy Groups: Many groups, like the ACLU, have advocated for social media companies to implement more specific hate speech policies. However, theyâ€™ve also pointed out that social media companies tend to remove posts by certain demographics over others â€“ even if the content is the same.\n* State Legislators: Several states, such as Texas, have proposed legislation that prohibit social media companies from discriminating in their content moderation on the basis of viewpoint. They would likely prefer a more hands-off approach to content moderation where most things, unless truly dangerous or advocating for violence, are permitted. \n* EU: While this algorithm is based in the United States and will be subject to its laws, the EU might have an interest in encouraging Birdy to take more stringent measures to protect its online users. \n* Brands/Advertisers: Advertisers want to appeal to a broad audience and may not (for the sake of their brand) want to place ads on a platform they feel is hosting hate speech. One example of this is X. After Elon Musk endorsed an anti-semitic conspiracy theory on X, several major brands paused their activity on X.\n\n### Human Values\n\n* Freedom of Expression/Freedom of Speech\n\n    * In the United States (particularly when compared to other jurisdictions like the EU), we have a uniquely strong tradition of free speech. As seen in past court decisions (such as *Snyder v. Phelps*), we tend lean toward allowing speech even if it is blatantly offensive or harmful to certain groups. While the freedom of speech of users vis-a-vis the social media company isnâ€™t a legal standard since these companies are private entities, itâ€™s still a significant human value to consider. \n\n      * It is also incredibly difficult to create one general definition of hate speech. Each person will have their own unique interpretation of the term. Hence, freedom of expression also cautions us from over-moderating content and forcing one groupâ€™s idea of hate speech onto others. \n\n    * Content moderation decisions have also been interpreted as the social mediaâ€™s editorial discretion. One current example of this is a recent decision by the Eleventh Circuit Court of Appeals in *NetChoice, LLC, et al. v. Attorney General* that struck down portions of a Florida law requiring social media platforms to moderate content neutrally. Hence, while bias may be found via this audit, the question still remains unclear regarding what can be done from a legal standpoint. \n\n* Fairness \n\n    * Content moderation systems inevitably have to decide what their definition of hate speech is. We care about fairness in content moderation regarding (a) the way that the rules are consistently/inconsistently applied and (b) who gets to decide what is hate speech. This audit will focus primarily on (b).\n  \n* Transparency\n    * This human value takes a bit of a backseat in the context of this audit, but it is nonetheless still important. Birdyâ€™s users have a right to know how they are being moderated and Birdy has a responsibility to make this information accessible somewhere on its platform. This responsibility also extends to making users aware of CleanTalk and how CleanTalk works. While Birdy may have its rules of conduct explicitly written out, ensuring transparency also means explaining how CleanTalk enforces these rules and how it was trained to do so. This audit seeks to expose this and help make it known to the plaintiffs in the class action.\n  \n### Life Cycle of the System\n\nThe Machine Learning Life Cycle of CleanTalk involves the following procedures:\n\nThe system is deployed on Birdyâ€™s social media platform via an API developed by CleanTalk that parses the text of users tweets using a machine learning model and returns the sentiment of the text.\n\nThe system is both monitored and controlled by parties by Birdy and CleanTalk. For purposes of transparency, Birdy makes information about their use of CleanTalk known in their terms and conditions as well as in their rules and policies. A team at Birdy occasionally monitors and reviews the posts that are flagged as toxic. Birdy ultimately has the final say on the systemâ€™s actual application to its users. CleanTalk developed the algorithm through a process of data collection and annotation. CleanTalk will oversee the actual training (and subsequent retraining) of its system. It has broader control over what text will be flagged as toxic and the sensitivity of its algorithm. \n\nThe system will be periodically retrained every six months by CleanTalk offline to stay up to date with new terminology, trends, or slang. This retraining process involves the following like a typical ML DevOps pipeline:\n\nA preprocessing algorithm will be applied to tweets that happened during the new period on Birdyâ€™s platform. Additionally, to introduce diversity in data, a small portion of text will be scraped from openly accessible social media platforms. A sample of this processed data that is deemed to have a reasonable quality via a downstream algorithm will be assigned to annotators to identify the sentiment of the text.\n\nMultiple diverse annorators will label the same text for sentiment and this will be added to a retraining dataset. The diversity of both the annotators and the dataset is crucial to ensuring fairness. The existing model will be fine-tuned using state-of-the-art techniques in natural language understanding and standard evaluation metrics such as precision, recall, accuracy, etc. Part of the modelâ€™s training will involve supervised learning with the annotatorâ€™s data and ground truth label of the majority annotation.\n\nThe new ML modelâ€™s performance will be compared to the old model.\nAfter ensuring the new modelâ€™s performance is better than the old model according to previously stated evaluation metrics, the system will be accessible as an API endpoint to Birdyâ€™s engineering team.\n\n## Audit Question / Issue\n\n### Question\n\nIs Birdyâ€™s use of CleanTalk resulting in unfair and biased content moderation that unfairly abridged certain usersâ€™ freedom of expression?\n\n### Why This Matters\n\nThis question is important from a desire to protect the userâ€™s freedom of expression while still maintaining a fair and safe online environment. Any content moderation system will inevitably run into the issue of users feeling like they were unfairly censored; there will never be one single definition of what constitutes hate speech, nor what the necessary response to hate speech should be. Thus, while we shouldnâ€™t expect a perfect system, we should expect it to have the cultural nuance to balance the two aforementioned interests as best as possible. If we find wide discrepancies based on the annotatorâ€™s demographics, we have reason to be concerned.\n\n### Expected Findings\n\nFrom a preliminary assessment of the data used to train the system, we can already see a significant bias. 80% of the annotators are white. While this is probably reflective of other biases in the tech workforce rather than any intentional design choice, we expect that the audit will expose issues associated with this.  These discrepancies, notably discrepancies in the TPR rates between different groups of annotators, will point to an unfairness in the content moderation system that we should be concerned about.\n\n## Methods\n\n### Normative Question\n\nIs Birdyâ€™s use of CleanTalk resulting in unfair and biased content moderation that unduly abridges certain groupâ€™s freedom of expression? (Also: Who gets to define hate speech? Is a super robust/strict content moderation system desirable?)\n\n* We need to contextualize this audit in a value-based framework centered on free speech, user safety and content moderation.\n    * Content moderation decisions by social media companies are interpreted, to some degree, by courts as them exercising their editorial discretion under the First Amendment. So, even if the audit does find bias or issues with their data collection, itâ€™s not immediately obvious what can be done to rectify this or if anything should be done. (example: Florida law regulating content moderation on the basis of viewpoint was recently partly struck down by the Court of Appeals.)\n    * Bias is undesirable and we want a consistent content moderation system. However, we also have to balance this interest with the USâ€™s strong free speech tradition. Thus, while high TPRâ€™s demonstrate the accuracy of the system, users still may not agree that what is considered â€œtoxicâ€ is in fact toxic. While this issue may never be fully avoided (people will always disagree with what is considered toxic), at the very least the impact can be lessened by consistent TPR across different groups of annotators.\n\n### Technical Methods\n\n* Identify how model performance varies based on the annotator demographic that it is trained on â†’ does this vary among demographics?\n* False positive rate vs true positive rate (ROC curve) to visualize model performance and choose threshold for considering a model biased\n* Analyze false positives (model is too strict on certain posts) and false negatives (model is not strict enough in enforcing moderation) and if it varies among demographics on the same test set\n* Based on above analysis, determine threshold for classifying model as unfair\n\n## Findings & Conclusions\n\n### Model Bias and Dataset Composition Analysis\n\n\u003Ch4>Definitions\u003C/h4>\n\n\u003Ch5>Model Bias ($\\beta$)\u003C/h5>\n\nFor our evaluation, we define model bias ($\\beta$) as the standard deviation of the true positive rate (TPR) of the model concerning a certain demographic. For example, the model shown in Table 1 was trained on a random subset of data and validated against separate test sets, with a $\\beta = 0.12$. \n\n\u003Cfigure>\n\u003Cdiv className=\"table-auto\" style={{ overflowX: 'auto' }}>\n  \u003Ctable style={{ minWidth: '100%', fontSize: '0.9em' }}>\n  \u003Ctr>\n    \u003Cth>Label\u003C/th>\n    \u003Cth>white_male_conservative\u003C/th>\n    \u003Cth>white_male_liberal\u003C/th>\n    \u003Cth>white_female_liberal\u003C/th>\n    \u003Cth>black_female_moderate_liberal\u003C/th>\n    \u003Cth>white_female_conservative\u003C/th>\n    \u003Cth>all\u003C/th>\n  \u003C/tr>\n  \u003Ctr>\n    \u003Ctd>TPR\u003C/td>\n    \u003Ctd>0.896\u003C/td>\n    \u003Ctd>0.915\u003C/td>\n    \u003Ctd>0.816\u003C/td>\n    \u003Ctd>0.584\u003C/td>\n    \u003Ctd>0.661\u003C/td>\n    \u003Ctd>0.912\u003C/td>\n  \u003C/tr>\n  \u003Ctr>\n    \u003Ctd>FNR\u003C/td>\n    \u003Ctd>0.104\u003C/td>\n    \u003Ctd>0.085\u003C/td>\n    \u003Ctd>0.184\u003C/td>\n    \u003Ctd>0.416\u003C/td>\n    \u003Ctd>0.339\u003C/td>\n    \u003Ctd>0.087\u003C/td>\n  \u003C/tr>\n  \u003Ctr>\n    \u003Ctd>TNR\u003C/td>\n    \u003Ctd>0.491\u003C/td>\n    \u003Ctd>0.489\u003C/td>\n    \u003Ctd>0.617\u003C/td>\n    \u003Ctd>0.747\u003C/td>\n    \u003Ctd>0.587\u003C/td>\n    \u003Ctd>0.478\u003C/td>\n  \u003C/tr>\n  \u003Ctr>\n    \u003Ctd>FPR\u003C/td>\n    \u003Ctd>0.509\u003C/td>\n    \u003Ctd>0.511\u003C/td>\n    \u003Ctd>0.383\u003C/td>\n    \u003Ctd>0.253\u003C/td>\n    \u003Ctd>0.413\u003C/td>\n    \u003Ctd>0.522\u003C/td>\n  \u003C/tr>\n\u003C/table>\n\u003Cfigcaption className=\"text-center mt-2 text-sm italic\">\n    Table 1\n\u003C/figcaption>\n\u003C/div>\n\u003C/figure>\n\n\u003Ch5>Fairness Metrics\u003C/h5>\n\nFairness is a comparative metric of bias, where:\n- Models with lower $\\beta$ are considered *more fair*\n- Models with higher $\\beta$ are considered *less fair*\n\n**Note**: Models are not considered 'fair' in absolute terms, rather, they are comparatively 'more fair' or 'less fair' than other models.\n\n\u003Ch4> Dataset Composition ($C$) \u003C/h4>\nWe define composition ($C$) as the percent makeup of a dataset regarding a certain demographic. For our dataset, the racial composition is:\n\n$$\nC = \\begin{bmatrix}\n\\text{White}: 83\\\\\n\\text{Asian}: 6\\\\\n\\text{Black}: 4\\\\\n\\text{Hispanic}: 4\\\\\n\\text{Native American}: 0\\\\\n\\text{Other}: 0\n\\end{bmatrix}\n$$\n\nThe dataset bias ($B$) is calculated as the standard deviation of $C$, giving us $B = 28.12$.\n\n\u003Ch4>Training Data Analysis\u003C/h4>\n\nFor our white male conservative demographic:\n$$\nC_{\\text{white\\_male\\_conservative}} = \\begin{bmatrix}\n\\text{White}: 100\\\\\n\\text{Asian}: 0\\\\\n\\text{Black}: 0\\\\\n\\text{Hispanic}: 0\\\\\n\\text{Native American}: 0\\\\\n\\text{Other}: 0\n\\end{bmatrix}\n$$\n\nThis results in $B = 37.27$.\n\n\u003CImage \n  src={fig1} \n  alt=\"Model performance across demographics\" \n  class=\"mx-auto\"\n/>\n\n\u003Cdiv className=\"flex flex-row gap-8 justify-center\">\n  \u003CImage \n    src={fig2} \n    alt=\"Validation annotator race\"\n    width={350}\n    height={150}\n  />\n\n  \u003CImage \n    src={fig3} \n    alt=\"Validation annotator political standing\" \n    width={350}\n    height={150}\n  />\n\u003C/div>\n\n### Performance Evaluation\n\nThe model performance shows:\n\n$$\\beta_{\\text{white\\_male\\_liberal}} > \\beta_{\\text{white\\_female\\_liberal}} > \\beta_{\\text{black\\_female\\_moderate\\_liberal}}$$\n\n\u003Ch4>Key Findings\u003C/h4>\n\n1. **Model Performance**: Models trained on white male data demonstrate superior performance compared to those trained on white female or black female data.\n\n2. **Dataset Bias Impact**: The high $B$ value in our dataset explains the poor performance of models trained on minority racial groups, as the dataset is skewed towards white people.\n\n### Content Moderation Implications\n\n\u003Ch4>Observed Biases\u003C/h4>\n\n- White male annotators show higher TPR rates\n- 80% of annotator dataset comprised of white males\n- Significant TPR disparities between demographics\n\n\u003Ch4>Challenges\u003C/h4>\n\n1. One-size-fits-all moderation standards prove problematic\n2. Risk of:\n   - Inaccurate content flagging\n   - Potential freedom of speech impediments\n   - System inconsistency\n\n\u003Ch4>Impact on User Engagement\u003C/h4>\n\n- Both strict and lenient moderation can create chilling effects\n- Marginalized groups face disproportionate online hostility\n- Risk of reduced participation in digital discourse\n\n## Recommendations\n\n### Section 230 (Policy Recommendations)\n\nSection 230 of the Communications Decency Act of 1996 is a fundamental law that shaped the Internet. It is a legal provision that protects Internet service providers from being sued for content published by third parties on their sites. It states: \"No provider or user of an interactive computer service shall be considered the publisher or speaker of any information provided by another information content provider\". This provision was introduced to encourage online companies to moderate their content without fear of being held responsible for anything posted by users. Article 230 also allows social platforms to moderate their content by deleting messages that do not comply with the platform's ethical standards, provided they act in \"good faith\".\nClearly defining what constitutes 'good faith' in the context of Section 230 is crucial. The notion of good faith is often open to interpretation and can be exploited in different ways. For example, a company that deliberately moderates a population category may be considered discriminatory. In another case, if an individual A is moderated for similar content for which an individual B is not, if the case is isolated, this could be attributed to an error or lack of attention. We could therefore assume that the company has not acted in \"bad faith\". However, if suspicious moderation is observed within the same platform, and the latter invokes vague arguments such as a lack of technical, financial or human resources allocated to moderation services, it would be essential to establish a clear audit process to remove any ambiguity about the notion of good faith. This could include financial audits to verify the real existence of a lack of resources, and ethical or internal audits to assess whether moderation is truly intentional. Defining this term more precisely would help Birdy comprehend the legal requirements around moderating content on its platform. Having an explicit understanding of what qualifies as acting in 'good faith' would allow Birdy to reduce the risk of lawsuits by verifying that its moderation practices conform to legal criteria.\n\n### Exploring Further Cultural Nuance (Birdy-specific recommendation)\n\nThe conclusion of this audit still has several gaps. For instance, we focus specifically on English text and say nothing about other languages. Second, the demographics are limited to race, gender, and political orientation but there is nothing about nationality. However, defining objectionable content is highly nuanced and can vary significantly across countries/cultures, even among English-speaking individuals. What may be considered an inoffensive joke in South Africa could be viewed as harassment in the United States. It is therefore recommended that Birdy conduct a comprehensive audit to align and assess how their use of CleanTalk seeks to balance these varying cultural nuances. \nSpecifically, the audit should articulate granular guidelines for discerning when constructive criticism crosses into a personal attack, when humor becomes a form of harassment or hate speech, and when repeated offensive behavior constitutes a violation of community standards. These threshold definitions should draw from established legal precedents as well as consultations with subject matter experts - including legal scholars, psychologists, and human rights specialists - to ensure the guidelines are fair, contextually appropriate, and consistent with ethical principles for the online environment. With well-defined policies aligned through a rigorous auditing process, platforms like Birdy can more effectively safeguard their user communities while upholding open and respectful dialogue.\n\n### Human-in-the-Loop / Procedural Justice\n\nIn tandem with a more balanced and diversified content moderation process, Birdy should implement a stronger human-in-the-loop, beyond just having someone occasionally review the posts that CleanTalk flags as toxic. Along with providing users the reasoning behind their content removal/flagging and the policies that have been violated, users should have access to clear-cut tools to be able to appeal the decision should they disagree with the decision. Many social media platforms, such as Instagram and more recently Snapchat, have integrated this function as a way to correct any possible shortcomings of the content moderation system. Moreover, the appeals process upholds the fundamental principle of free speech by allowing individuals to challenge censorship and have their voices heard in the digital public sphere. Doing so might also mitigate recidivism as offering an appeals process ensures procedural justice. Furthermore, integrating a stronger human-in-the-loop aspect not only enhances transparency and accountability but also ensures a more nuanced understanding of context, cultural sensitivities, and intent behind user-generated content. By offering users the opportunity to appeal decisions, platforms foster trust and empower users to participate in refining community standards, ultimately leading to a more inclusive and balanced online environment.\n\n### Technical\n\n\u003Ch4>Model Monitoring\u003C/h4>\n\n* Continuously monitor performance of model in production\n  * Introduce logging for number of posts blocked for demographic groups\n  * Measure user satisfaction of system via surveys\n  * Measure number of likes and dislikes for system\n* Recommend using multi-modal LMs to filter out text, image, and audio\n* Diversify training data\n  * Original training data was heavily imbalanced, with 80% of the annotators being white\n  * Any imbalance in training data directly affects performance of model â†’ it is critical for Birdy to take proactive steps to ensure training data for content moderation is balanced among different annotator demographics\n  \n\u003Ch4>System Design\u003C/h4>\n\n1. When user clicks on a Birdy post, they can either label it as offensive or do nothing and Birdy can use this to further train the model â†’ reinforcement learning human feedback\n   a. Periodically, ask user if this filter was effective.\n2. Demographic-based filtering, enabled by user through profile page\n3. Simple keyword filtering (non-machine learning approach for more fine-grained control of filtering by users) where users can specify certain content they do not want to see\n\n## Summary\n\nThe audit of CleanTalk reveals biased training data leads to biased models and moderation outcomes, while also highlighting the concerns about content moderation and freedom of speech. Exacerbated by an unbalanced dataset of predominantly white annotators, models trained on biased datasets perform worse in fairness metrics. The audit revealed that models trained on white male data perform much better than those trained on white female or black female data, highlighting a detrimental instance of data imbalance. Imbalance present in the training data and testing data reveals that a one-size-fits-all approach to content moderation poses a serious risk of flawed and inconsistent results. In light of this audit, we encourage CleanTalk to further diversify its training to data and implement a human-in-the-loop for appeals. Policymakers may also consider Section 230 of the Communications Decency Act and what exactly â€œgood faithâ€ content moderation looks like.\n\n## ðŸ‘¥ Collaborators\n\n* Amudha Sairam (UC Berkeley)\n* Arthur Clenet (UC Berkeley)\n* Rena Lu (UC Berkeley)\n* Safa Basravi (UC Berkeley)\n* Sam Wu (Boston University)","src/content/projects/responsible-ai-audit.mdx",[153],"../../assets/images/projects/responsible-ai-audit/thumbnail.png","63aa40b2f64fa324","responsible-ai-audit.mdx","sketch-n-solve",{"id":156,"data":158,"body":163,"filePath":164,"assetImports":165,"digest":167,"legacyId":168,"deferredRender":23},{"title":159,"cover":160,"githubUrl":161,"paperUrl":162},"Sketch-n-Solve","__ASTRO_IMAGE_../../assets/images/projects/sketch-n-solve/thumbnail.png","https://github.com/lavaman131/sketch-n-solve","https://arxiv.org/abs/2409.14309","## Abstract\n\nWe present Sketch 'n Solve, an open-source Python package that implements efficient randomized numerical linear algebra (RandNLA) techniques for solving large-scale least squares problems. While sketch-and-solve algorithms have demonstrated theoretical promise, their practical adoption has been limited by the lack of robust, user-friendly implementations. Our package addresses this gap by providing an optimized implementation built on NumPy and SciPy, featuring both dense and sparse sketching operators with a clean API. Through extensive benchmarking, we demonstrate that our implementation achieves up to 50x speedup over traditional LSQR while maintaining high accuracy, even for ill-conditioned matrices. The package shows particular promise for applications in machine learning optimization, signal processing, and scientific computing.","src/content/projects/sketch-n-solve.mdx",[166],"../../assets/images/projects/sketch-n-solve/thumbnail.png","0cef516c27c33d97","sketch-n-solve.mdx","blog",["Map",171,172,182,183,193,194,203,204,213,214,223,224],"ai-coding-infrastructure",{"id":171,"data":173,"body":178,"filePath":179,"digest":180,"legacyId":181,"deferredRender":23},{"title":174,"date":175,"authors":176,"description":177},"How I Shipped 100k LOC in 2 Weeks with Coding Agents","2025-11-12","Alex Lavaee","Open sourcing my developer workflow with AI agentsâ€”skills, sub-agents, and autonomous execution. A 5-minute setup that provides the missing infrastructure layer for AI coding tools.","## The Problem\n\nOver the last two weeks, I shipped 100,000 lines of high-quality code using AI agents. But here's what I learned talking to engineers across companies: we're being asked to adopt AI coding tools (Cursor, Windsurf, GitHub Copilot, Claude Code) without instructions, support, or infrastructure on how to get ROI in production.\n\nWhen we onboard developers, we give them documentation, coding standards, proven workflows, and collaboration tools. When we \"deploy\" AI agents, we give them nothing. They start fresh every time. No project context, no memory of patterns, no proven workflows.\n\nSo I compiled **AI Coding Infrastructure**, the missing support layer that agents need. Five components:\n\n1. **Project Memory** (AGENTS.md): Your tech stack, patterns, conventions that agents read automatically before every response\n2. **Proven Workflows** (Skills): Battle-tested TDD, debugging, code review patterns agents MUST follow\n3. **Specialization** (Sub-Agents): 114+ domain experts working in parallel, not one generalist\n4. **Planning Systems** (ExecPlans): Self-contained living docs for complex features\n5. **Autonomous Execution** (Ralph): Continuous loops for overnight autonomous development\n\n**Get started**: [github.com/flora131/agent-instructions](https://github.com/flora131/agent-instructions)\n\n## How It Works\n\n### Project Memory\n\nA single markdown file (`AGENTS.md` or `CLAUDE.md`) in your project root contains your tech stack, architectural patterns, and coding conventions. Agents automatically read this before every response, transforming them from stateless tools into stateful team members who know your project.\n\n### Skills: Mandatory Workflow Discipline\n\nSkills are proven patterns from [Anthropic Skills](https://github.com/anthropics/skills) \u003Ca href=\"#ref-2\">[2]\u003C/a> and [Superpowers](https://github.com/obra/superpowers) \u003Ca href=\"#ref-3\">[3]\u003C/a>: spec-driven development (SDD), test-driven development (TDD), systematic debugging, code review, brainstorming, etc.\n\n**The difference**: Mandatory First Response Protocol. Before ANY response, agents MUST:\n\n1. List available skills\n2. Check if ANY skill matches\n3. If yes â†’ Read and follow it exactly\n\nAgents can't rationalize \"this is too simple for TDD\" or \"let me gather info first.\" If the skill applies, they MUST use it. This prevents the most common failure mode: shortcuts that seem reasonable but create bugs and technical debt.\n\n_Credit: [Superpowers](https://github.com/obra/superpowers) framework by Jesse Vincent (obra) \u003Ca href=\"#ref-3\">[3]\u003C/a>_\n\n### Custom Skills: Tailored to Your Workflow\n\nYou can also create custom skills for your unique workflows and patterns. One incredibly useful that I built is a meta-prompting skill called prompt-engineer that helps agents improve their own prompts based on project context and past failures. This was built using Anthropic's skill creation in [Anthropic Skills](https://github.com/anthropics/skills) \u003Ca href=\"#ref-2\">[2]\u003C/a>.\n\n_Credit: Built on Anthropic's Task tool \u003Ca href=\"#ref-2\">[2]\u003C/a>_\n\n### Sub-Agents: Parallel Specialization\n\n114+ specialized agents (Python Pro, React Specialist, Cloud Architect, Security Engineer, ML Engineer, etc.) orchestrated by Agent Organizer. Up to 50 agents work in parallel on independent tasks.\n\nExample: Building a notification system? Agent Organizer dispatches Backend Developer, WebSocket Engineer, Database Optimizer, and React Specialist all simultaneously.\n\n_Credit: Uses Awesome Claude Code Subagents \u003Ca href=\"#ref-1\">[1]\u003C/a>_\n\n### ExecPlans: Self-Contained Living Documents\n\nFor complex features, agents auto-generate ExecPlans in `specs/`, which are fully self-contained documents that anyone can implement from without external context. Updated as work progresses. Includes Purpose, Progress, Decision Log, Plan of Work, Validation.\n\nNothing gets lost across context switches or handoffs.\n\n### Ralph: Overnight Autonomous Development\n\nNamed after the [Ralph Wiggum meme](https://ghuntley.com/ralph/) \u003Ca href=\"#ref-4\">[4]\u003C/a>, Ralph enables agents to run in continuous loops with no manual intervention.\n\n**How it works**: Four files in `.ralph/` directory:\n- `prompt.md` â€” Your instructions (\"Port TypeScript to Python\")\n- `sync.sh` â€” Single iteration (reads prompt, runs Claude CLI, logs output)\n- `ralph.sh` â€” Continuous loop (runs sync, sleeps 10s, repeats)\n- `visualize.py` â€” Colored output showing progress\n\nThe agent works, commits changes, sleeps 10 seconds, continues until task completion. Manages its own context across iterations.\n\n**Real-world results**:\n- Ships 6+ repos overnight at YC hackathons\n- Builds programming languages autonomously\n- Migrates entire codebases between technologies\n\nBest on cloud VMs (AWS EC2, Google Compute, DigitalOcean) running a [tmux](https://github.com/tmux/tmux) session.\n\n_Credit: [Geoffrey Huntley](https://ghuntley.com/) \u003Ca href=\"#ref-4\">[4]\u003C/a>, high-level implementation from [repomirror](https://github.com/repomirrorhq/repomirror) \u003Ca href=\"#ref-6\">[6]\u003C/a>_\n\n## Real Results\n\nTwo weeks. 100,000 lines of production-quality code. I took the final 20% as the engineer, but the infrastructure got me to 80% much faster than before.\n\n**What worked**:\n- Sub-agents prevented bottlenecks through parallel execution\n- Mandatory TDD caught bugs in design phase\n- ExecPlans survived context switches\n- Ralph built core features overnight\n\n**Critical human touch**:\n- **Reviewing agent-written plans**: Caught architectural issues and edge cases before implementation\n- **Refining skills and sub-agents**: Customized agent behavior to match my workflow and this learning loop was essential\n- Final integration testing, business logic decisions, performance optimization\n\nThis transforms you from a passive user into a power user who configures how agents think and collaborate.\n\n## My AI-Augmented Development Workflow\n\nThe infrastructure above enables a research-driven development process that balances AI assistance with human oversight. Here's the workflow that produced 100k lines of quality code:\n\n```mermaid\n%%{init: {\n  'theme': 'default',\n  'flowchart': {\n    'htmlLabels': false,\n    'wrap': true\n  },\n  'themeVariables': {\n    'primaryColor': '#E8F4F8',\n    'primaryTextColor': '#1a1a1a',\n    'primaryBorderColor': '#4A90A4',\n    'secondaryColor': '#F0E8F4',\n    'secondaryTextColor': '#1a1a1a',\n    'secondaryBorderColor': '#8B7AA8',\n    'tertiaryColor': '#F4F0E8',\n    'tertiaryTextColor': '#1a1a1a',\n    'tertiaryBorderColor': '#A89B7A',\n    'noteBkgColor': '#E8F8E8',\n    'noteTextColor': '#1a1a1a',\n    'noteBorderColor': '#6FA86F',\n    'fontSize': '16px'\n  }\n}}%%\ngraph TD\n    A[Deep Research]:::primaryStyle --> B[\"Product Requirements Document (PRD)\"]:::tertiaryStyle\n    B --> C[Brainstorm with Coding Agent]:::secondaryStyle\n    C --> D[Technical Design/Spec]:::noteStyle\n    D --> E[Human Review & Feedback]:::primaryStyle\n    E --> F[Refined Technical Design/Spec]:::noteStyle\n    F --> G[Implementation Plan Doc/ExecPlan]:::tertiaryStyle\n    G --> H[Implementation with Sub-Agents]:::secondaryStyle\n    H --> I[Testing & Validation]:::primaryStyle\n\n    classDef primaryStyle fill:#E8F4F8,stroke:#4A90A4,stroke-width:3px,color:#1a1a1a\n    classDef secondaryStyle fill:#F0E8F4,stroke:#8B7AA8,stroke-width:3px,color:#1a1a1a\n    classDef tertiaryStyle fill:#F4F0E8,stroke:#A89B7A,stroke-width:3px,color:#1a1a1a\n    classDef noteStyle fill:#E8F8E8,stroke:#6FA86F,stroke-width:3px,color:#1a1a1a\n```\n\n### Research & Requirements\n\nStart with comprehensive research into the problem space: user needs, existing solutions, relevant technologies, and constraints. This research feeds into a **Product Requirements Document (PRD)** that articulates the *what* and *why*, including the problem statement, target users, success metrics, and business objectives without prescribing implementation details.\n\n### AI-Assisted Design\n\nThis is where AI coding infrastructure becomes critical. **Brainstorm with your coding agent** to explore technical possibilities. The agent leverages its knowledge of patterns and best practices to generate multiple approaches, identify challenges, and discuss trade-offs. This exploratory phase surfaces ideas that might not emerge from solo brainstorming.\n\nFormalize the output into a **Technical Design/Spec** (often auto-generated as an ExecPlan). This describes the *how*: architecture decisions, API designs, data models, technology stack, system components, and scalability/security considerations.\n\n### Human Validation Loop\n\n**Critical checkpoint**: Experienced engineers review the AI-assisted spec. This human oversight catches edge cases, validates assumptions, and ensures alignment with organizational standards. This acknowledges that AI assistance needs human verification. I spent significant time here catching architectural issues before implementation.\n\nIncorporate feedback into a **Refined Technical Design/Spec**. This might involve adjusting architecture, adding clarifications, or reconsidering technology choices. The refined spec represents the agreed-upon technical approach with human validation baked in.\n\n### Execution\n\nBreak the refined spec into an **Implementation Plan Doc** (ExecPlans in `specs/`). This includes task decomposition, effort estimates, dependency mapping, and milestone definitions.\n\nDuring **Implementation**, sub-agents work in parallel on independent tasks. Ralph handles overnight autonomous development for foundational features. Mandatory TDD skills catch bugs in the design phase.\n\n**Testing** validates against both PRD objectives and technical spec requirements: unit tests, integration tests, performance testing, and final QA.\n\n### Why This Works\n\n**AI-augmented but human-validated**: Balances the speed and breadth of AI with the judgment and experience of senior engineers. AI assists exploration and implementation while humans validate critical decisions.\n\n**Separation of concerns**: Clear distinction between product requirements (PRD), technical design (Spec), and execution planning (Plan Doc/ExecPlan). Each artifact serves its specific purpose.\n\n**Feedback integration**: Explicit human review loop after initial spec ensures first drafts benefit from iteration before implementation begins.\n\n**Research-driven**: Starting with deep research rather than jumping to requirements ensures decisions are grounded in solid understanding of the problem space.\n\nThis workflow is particularly effective for complex projects where upfront planning investment pays dividends, teams leveraging AI coding tools, and organizations wanting to maintain human control over critical technical decisions while benefiting from AI capabilities.\n\n## 5-Minute Setup\n\n1. Copy template files to your project (`AGENTS.md`/`CLAUDE.md`, `specs/`, optionally `.ralph/`)\n2. Open in your AI coding tool (Cursor, Windsurf, GitHub Copilot, Claude Code)\n3. Ask: **\"Set up agent instructions, skills, and sub-agent support for this project\"**\n\nAgent analyzes your codebase and populates AGENTS.md, then installs 100+ skills and 114+ sub-agents to your config directory.\n\n**You get**: Project memory, mandatory workflows, specialized agents, ExecPlan templates, Ralph setup.\n\n**Repository**: [github.com/flora131/agent-instructions](https://github.com/flora131/agent-instructions) \u003Ca href=\"#ref-5\">[5]\u003C/a>\n\n## Why Open Source This?\n\nDevelopers aren't seeing production ROI from AI coding tools. Without infrastructure and support, these tools aren't being maximized for their potential.\n\nThis infrastructure made the difference for me. From inconsistent results to 100,000 lines in two weeks. If it helps others build faster, it should be shared.\n\n**PRs welcome** \u003Ca href=\"#ref-5\">[5]\u003C/a>: Build skills for your workflow, create domain sub-agents, improve setup, find better ExecPlan patterns, extend Ralph.\n\nWe're at an inflection point. AI coding tools are deploying widely, but the infrastructure layer is missing. Let's build it together and make it easier and faster for developers to use.\n\n## Credits\n\nBuilt on excellent work by:\n- **Superpowers** \u003Ca href=\"#ref-3\">[3]\u003C/a>: Mandatory skill checking, TDD discipline, systematic debugging (Jesse Vincent)\n- **Anthropic Skills** \u003Ca href=\"#ref-2\">[2]\u003C/a>: Skills system and reusable patterns framework\n- **Ralph Method** \u003Ca href=\"#ref-4\">[4]\u003C/a>: Continuous agent loops for autonomous development (Geoffrey Huntley)\n- **Sub-Agent Architecture**: Anthropic's Task tool and orchestration patterns \u003Ca href=\"#ref-2\">[2]\u003C/a>\n\n## Additional Resources\n\n### Complete Ralph Setup Script\n\nThe sync script that powers autonomous execution:\n\n```bash\n#!/usr/bin/env bash\n\ncat .ralph/prompt.md | \\\n    claude -p --output-format=stream-json --verbose \\\n        --dangerously-skip-permissions --add-dir . | \\\n    tee -a .ralph/claude_output.jsonl | \\\n    uv run --no-project .ralph/visualize.py --debug\n```\n\nPlace this in `.ralph/sync.sh` and make it executable. The continuous loop (`.ralph/ralph.sh`) repeatedly calls this script with 10-second sleeps between iterations.\n\n---\n\n## Key Takeaways\n\n- **AI Coding Infrastructure** is the missing support layer for coding agents, providing project memory, proven workflows, specialization, planning systems, and autonomous execution\n- **Mandatory skill checking** prevents agents from rationalizing away best practices, making it structurally impossible to skip proven workflows like TDD\n- **114+ specialized sub-agents** enable parallel execution (up to 50 agents) with domain expertise instead of one generalist\n- **Ralph method** enables overnight autonomous development through continuous agent loops\n- **5-minute setup** via single prompt installs the complete infrastructure across any AI coding tool\n\n### References\n\n\u003Cdiv id=\"ref-1\">[1] Awesome Claude Code Subagents - https://github.com/VoltAgent/awesome-claude-code-subagents\u003C/div>\n\u003Cdiv id=\"ref-2\">[2] Anthropic Skills Framework - https://github.com/anthropics/skills\u003C/div>\n\u003Cdiv id=\"ref-3\">[3] Superpowers Framework by Jesse Vincent - https://github.com/obra/superpowers\u003C/div>\n\u003Cdiv id=\"ref-4\">[4] Ralph Wiggum Method by Geoffrey Huntley - https://ghuntley.com/ralph/\u003C/div>\n\u003Cdiv id=\"ref-5\">[5] Agent Instructions Repository - https://github.com/flora131/agent-instructions\u003C/div>\n\u003Cdiv id=\"ref-6\">[6] Repomirror - High-level implementation of Ralph method - https://github.com/repomirrorhq/repomirror\u003C/div>","src/content/blog/ai-coding-infrastructure.mdx","d28257553c4ba70a","ai-coding-infrastructure.mdx","context-engineering-ai-ides",{"id":182,"data":184,"body":189,"filePath":190,"digest":191,"legacyId":192,"deferredRender":23},{"title":185,"date":186,"authors":187,"description":188},"Building Products with Agentic-Powered IDEs","2025-07-23","Alex Lavaee, Norin Lavaee","How context engineering transforms AI-powered development tools from disappointing to transformative through smart prompting, MCP servers, and strategic tool integration.","AI-powered IDEs and terminal apps promised to revolutionize software development.\n\nIf you just \"plug and play,\" you're going to be disappointed. Context engineering makes the difference.\n\nContext engineering is the discipline of designing, constructing, and managing dynamic systems that provide language models (LMs) contextâ€”with the right information, tools, and instructions in the optimal format and at the right time, so the model can accurately and effectively accomplish its task.\n\nRecently, we built a multi-agentic system for a marketplace application using Cursor. AI-powered developer tools can be exceptionally powerful for speeding up development, but despite all the hype, they're far from \"set it and forget it.\"\n\nWe learned of the importance of context engineering or smart choices about tools, prompting, and frameworks.\n\n## Frontend Development with Figma Integration\n\nFor the frontend, we used TypeScript, Vite, and Tailwind CSS, with all our designs living in Figma. To move quickly, we experimented with Figma's First Draft AI tool with mixed results where detailed prompting could get you good enough outputs.\n\nThe real unlock came when we connected the Figma MCP server to Cursor, making sure to use Auto Layout for our designs in Figma. From there, getting the designs and components created for our FE was super fast (maybe not pixel-perfect but surprisingly close for a quick build). The setup for MCPs is super simple on Cursor (or other VSCode-based editors):\n\nUnder Cursor > Settings > Cursor Settings > MCP Tools you can add Figma to your mcp.json file and then switch to dev mode in Figma while keeping the app open.\n\n```json\n\"figma\": {\n     \"url\": \"http://127.0.0.1:3845/sse\"\n}\n```\n\u003Cbr>\u003C/br>\n\nFor iterating on frontend components and designs, we learned that the more prescriptive and detailed our prompts, the better the output. Describing styles, specifying \"vibe,\" calling out exact files, and feeding Cursor a handful of design screenshots from other apps allowed us to leverage its multimodal capabilities with the right amount of context, which was key.\n\nWe started treating Cursor almost like a design partner, feeding it rich, structured content to get closer to our vision faster.\n\n## Database Integration and Documentation Access\n\nNext, we connected the Context7 MCP for accessing the latest documentation for libraries, plus the Supabase MCP with read access to give Cursor a direct line to our database structure. Notably, we only provide read access to the MCP for additional security.\n\nContext7 MCP configuration:\n```json \n\"context7\": {\n     \"command\": \"npx\",\n     \"args\": [\"-y\", \"@upstash/context7-mcp\"]\n}\n```\n\u003Cbr>\u003C/br>\n\nSupabase MCP configuration:\n```json\n\"supabase\": {\n     \"command\": \"npx\",\n     \"args\": [\n       \"-y\",\n       \"@supabase/mcp-server-supabase@latest\",\n       \"--read-only\",\n       \"--project-ref=\"\n     ],\n     \"env\": {\n       \"SUPABASE_ACCESS_TOKEN\": \"\"\n     }\n}\n```\n\u003Cbr>\u003C/br>\n\nThis additional context helped us with debugging a row-level security (RLS) bug and gave Cursor a more actionable view of our entire stack. This was helpful for brainstorming or debugging with the chat in *Ask* mode.\n\n## Development-Specific Instructions\n\nAnother way that we were able to improve the return with Cursor was through development-specific instructions where we created a `.cursor/rules` file to outline information about our project, repos, and testing instructions. Since we were building different pieces and integrating as we went along, this was very helpful context when we needed to prompt Cursor to consistently format its responses with our development conventions and reference our tech stack. If we were working on this project longer term, we would also explore how to set up workflows to update the `.cursor/rules` file as our project changes and also link design docs, architecture diagrams, and technical specifications using Slack, Teams, and Google Drive MCPs.\n\n## The Main Takeaway\n\nContext is king and AI IDEs and terminal apps don't work out of the box. To get real productivity, you have to rethink your setup with MCP servers, widely-supported frameworks, and detailed, intentional prompts.\n\n## Current Challenges\n\nThere are still challenges thoughâ€¦\n\nFor true enterprise scale there's the bigger question: how do you pipe in codebase, developer actions, and real org context in a structured, privacy-safe way to continually self-improve these models and systems so the developer tools work better?\n\nGoogle's been vocal about their approach \u003Ca href=\"#ref-1\">[1]\u003C/a>, capturing and learning from engineer/AI interactions to improve performance. They plan on including these improvements in future versions of Gemini but aren't revealing their dataset. Still, personalization based on your team's code, processes, habits, and interactions is the real unlock (check out what Asimov is doing here \u003Ca href=\"#ref-2\">[2]\u003C/a>). Even with better prompting and MCP plugins, the scaling challenge isn't solved until AI can learn from your org's data at a deeper level. Despite significant improvements we still had to prompt the model at times that 'lives were at stake' and really only got 70-80% of the way there.\n\n## Future Opportunities\n\nWhere do future opportunities still lie?\n\nOne gap we discovered was the lack of a seamless and collaborative experience with these tools. We found Cursor genuinely helpful as a sounding board for brainstorming and debugging, often surfacing useful hypotheses or edge cases we hadn't considered. However, these IDEs haven't really nailed this \"pair programming with AI\" workflow yet. There's huge potential here and Cline is looking at this \u003Ca href=\"#ref-3\">[3]\u003C/a> but there's still significant room for a true world class developer experience here.\n\n## Security Considerations\n\nFinally, a critical note is security. Agents paired with MCP servers have known vulnerabilities (including the Supabase MCP that does have clear risks and steps to help mitigate them \u003Ca href=\"#ref-4\">[4]\u003C/a>), so it's essential to run proper vulnerability scanning and threat assessments and use best practices when using MCP servers and connecting data sources to any LLMs. One such safety measure is to use containerized agentic development tools like Dagger's Container Use (e.g. Docker container for agentic workflows). Another is logging MCP calls by models or possibly using guard models for agent YOLO mode (where the agent auto-confirms MCP calls). The last thing you want is a tool meant to boost productivity accidentally exposing sensitive data (e.g. API keys). This is an area that demands more investment and attention as these tools go mainstream.\n\n## Looking Ahead\n\nWe're also excited to continue and explore pipelines in the wave of agentic coding CLIs like Claude Code, Gemini CLI, and Codex which provide an alternative perspective where there is still not a clear system design for agentic coding yet. The same tricks we shared above apply to these systems, but with even more flexibility for parallel agents. We see a new form of \"AI bash scripting\" emerging where traditional shell scripts can be used to spawn a swarm of agents to achieve complex software engineering tasks. Stay tuned for more thoughts.\n\nWhat's the biggest context challenge you've hit with AI coding tools? Did we miss anything? Feel free to reach out and we'd be happy to make further revisions to this post.\n\n## References\n\n\u003Ca id=\"ref-1\">\u003C/a>[1] Google Research. (2024). AI in Software Engineering at Google: Progress and the Path Ahead. Retrieved from https://research.google/blog/ai-in-software-engineering-at-google-progress-and-the-path-ahead/\n\n\u003Ca id=\"ref-2\">\u003C/a>[2] Asimov. (2024). Reflection AI - Asimov. Retrieved from https://reflection.ai/asimov/\n\n\u003Ca id=\"ref-3\">\u003C/a>[3] Cline. (2024). Cline - AI Assistant for VS Code. Retrieved from https://cline.bot/\n\n\u003Ca id=\"ref-4\">\u003C/a>[4] Supabase MCP Server. (2024). Security Risks and Mitigations. GitHub. Retrieved from https://github.com/ampcome-mcps/supabase-mcp?tab=readme-ov-file#security-risks","src/content/blog/context-engineering-ai-ides.mdx","ce9bd398c327b463","context-engineering-ai-ides.mdx","context-engineering-cheat-sheet",{"id":193,"data":195,"body":199,"filePath":200,"digest":201,"legacyId":202,"deferredRender":23},{"title":196,"date":197,"authors":187,"description":198},"Context Engineering Navigator","2025-09-19","An interactive cheat sheet covering context engineering techniques for LLMs including retrieval, processing, management, and dynamic assembly strategies.","import ContextNavigator from \"../../components/blog/context-engineering-cheat-sheet/ContextNavigator.astro\";\n\n### Introducing the Context Engineering Navigator\n\nInspired by the foundational paper, \"A Survey of Context Engineering for Large Language Models,\" we developed the **Context Engineering Navigator**. \u003Ca href=\"#context-navigator\">This interactive tool below\u003C/a> allows you to search and explore the core methods from the paper's framework, helping you put theory into practice.\n\nOur Navigator breaks down the key components of context engineering, including:\n* **Context Retrieval & Generation:** Strategies for finding and creating relevant information.\n* **Context Processing:** Methodologies for cleaning, condensing, and structuring that information.\n* **Context Management:** Systems for efficiently storing and handling context.\n\n---\n\n### Find the Right Methods for Your Project\n\nReady to dive in? Use our tool to pinpoint the context engineering methods that best fit your needs.\n\nYou can filter by a category (**All Categories**, **Context Retrieval & Generation**, **Context Processing**, or **Context Management**) and click on any method to learn how it works and when to use it. You can even search for your specific use case to see which techniques we recommend.\n\n\u003Cdiv id=\"context-navigator\">\n  \u003CContextNavigator/>\n\u003C/div>\n\n### A More Effective Way to Build Smarter Language Models\n\nIf you've built production machine learning (ML) systems, you've likely run into these challenges: your model struggles with domain-specific tasks, can't access real-time information, or fails to deliver personalized responses. The traditional solution of creating massive, specialized datasets is often incredibly expensive and time-consuming.\n\n**Context engineering** offers a powerful alternative. Itâ€™s the process of finding, preparing, and managing the right information to give your model at the exact moment it's needed (at inference time). This approach provides your model with targeted, relevant knowledge, drastically reducing the need for costly retraining.\n\nThe world of AI will continue to evolve, but the principle of supplying models with relevant, external information will always be critical. Instead of waiting years for the perfect dataset, you can use context engineering to get immediate performance boosts for your specialized and dynamic applications.\n\nWe've also included a list of additional resources that we've found most effective for deploying these techniques in real-world systems below.\n\n\n# Resources\n\n[1] Original paper guiding this post: https://arxiv.org/abs/2507.13334 \n\nGithub: https://github.com/Meirtz/Awesome-Context-Engineering\n\n\n[2] Context Engineering Guide: https://docs.google.com/document/d/1JU8w-E4LlseFZm-ag22GSBU5A2rp2nb7iFGBNAbFL7k/edit?usp=sharing\n\n\n[3] Manus Context Engineering for AI Agents: [https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus?utm_source=tldrai)\n\n\n[4] 12 Factor Agents Factors 2 & 3: https://github.com/humanlayer/12-factor-agents\n\n\n[5] Ultimate Context Engineering Cheat Sheet: https://x.com/lenadroid/status/1943685060785524824","src/content/blog/context-engineering-cheat-sheet.mdx","88bee91516c5dae8","context-engineering-cheat-sheet.mdx","memorization-generalization-and-reasoning",{"id":203,"data":205,"body":209,"filePath":210,"digest":211,"legacyId":212,"deferredRender":23},{"title":206,"date":207,"authors":187,"description":208},"Memorization, Generalization, and Reasoning","2025-06-23","A deep dive into the concepts of memorization, generalization, and reasoning in large language models.","Not understanding how AI models actually memorize, generalize, and reason is costing us. We're chasing the wrong problems instead of building real solutions.\n\n## How much do models actually memorize?\n\nMeta's research \"How much do language models memorize?\" \u003Ca href=\"#ref-3\">[3]\u003C/a> provides crucial insights. They distinguished between **unintended memorization** (sample-specific storage) and **intended memorization** (generalization).\n\n### Intuition\n\nNeural networks possess a fundamental information storage capacity that governs the trade-off between memorization (storing specific training examples) and generalization (learning compressible patterns). They found each parameter in a neural network can store approximately 3.6 bits of information \u003Ca href=\"#ref-3\">[3]\u003C/a>, creating a hard constraint on what the network can remember versus what it must compress. This capacity limitation forces networks to choose between perfect recall of training data and the discovery of generalizable patterns--a trade-off that explains numerous phenomena in deep learning, from double descent to emergent abilities.\n\n### Information Theory Foundations\n\nNeural network memorization is fundamentally grounded in information theory. Understanding memorization requires precise definitions of information, compression, and capacity:\n\n**Shannon Entropy** quantifies the average information content of a random variable. For a discrete random variable $X$ with possible values $\\{x_1, x_2, \\ldots, x_n\\}$ and probability mass function $p(x)$:\n\n$$\n\\begin{align*}\nH(X) = -\\sum_{x \\in \\mathcal{X}} p(x) \\log_2 p(x)\n\\end{align*}\n$$\n\nThis measures the expected number of bits needed to encode messages from the distribution. Higher entropy indicates more randomness and less predictability.\n\n**Conditional Entropy** measures the average information needed to describe $X$ given knowledge of $Y$:\n\n$$\n\\begin{align*}\nH(X|Y) = -\\sum_{x,y} p(x,y) \\log_2 p(x|y)\n\\end{align*}\n$$\n\n**Mutual Information** quantifies the reduction in uncertainty about one variable when observing another:\n\n$$\n\\begin{align*}\nI(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) = \\sum_{x,y} p(x,y) \\log_2 \\frac{p(x,y)}{p(x)p(y)}\n\\end{align*}\n$$\n\nIn the context of memorization, $I(X; \\Theta)$ measures how much information the model parameters $\\Theta$ contain about the training data $X$.\n\n**Kolmogorov Complexity** $K(x)$ defines the shortest possible description length of string $x$ using a universal Turing machine. While uncomputable in general, it provides the theoretical foundation for understanding compression limits:\n\n$$\n\\begin{align*}\nK(x) = \\min_p \\{|p| : U(p) = x\\}\n\\end{align*}\n$$\n\nwhere $U$ is a universal Turing machine and $|p|$ is the length of program $p$.\n\n**Arithmetic Coding** provides a practical bridge between theory and implementation. Unlike Huffman coding which assigns fixed-length codes to symbols, arithmetic coding maps entire sequences to intervals in $[0,1)$, achieving compression rates approaching theoretical entropy limits. For a sequence $x_1, x_2, \\ldots, x_n$:\n\n$$\n\\begin{align*}\n\\text{Code length} \\approx -\\log_2 \\prod_{i=1}^n p(x_i | x_1, \\ldots, x_{i-1}) = \\sum_{i=1}^n -\\log_2 p(x_i | \\text{context})\n\\end{align*}\n$$\n\nThis connection allows us to estimate Kolmogorov complexity using language model probabilities, forming the basis for measuring memorization.\n\n### Formal Definition of Memorization\n\nThe memorization capacity of neural networks can be formalized through information-theoretic principles. Total memorization represents the mutual information between the training dataset $X$ and the trained model parameters $\\hat{\\Theta}$:\n\n$$\n\\begin{align*}\n\\text{mem}(X, \\hat{\\Theta}) = I(X; \\hat{\\Theta}) = H(X) - H(X|\\hat{\\Theta})\n\\end{align*}\n$$\n\nThis total memorization decomposes into two fundamental and distinct components that capture different aspects of learning:\n\n**Unintended Memorization** (sample-specific storage): This represents information about individual training examples that cannot be inferred from the general data distribution. It measures how much the model has \"overfit\" to specific samples:\n\n$$\n\\begin{align*}\n\\text{mem}_U(X, \\hat{\\Theta}, \\Theta) = H(X|\\Theta) - H(X|(\\Theta, \\hat{\\Theta}))\n\\end{align*}\n$$\n\nThis captures the reduction in uncertainty about specific training examples when given both the true distribution parameters $\\Theta$ and the learned parameters $\\hat{\\Theta}$, compared to knowing only $\\Theta$.\n\n**Intended Memorization** (generalization): This represents the portion of memorization that corresponds to learning the underlying data distribution - essentially \"good\" memorization that enables generalization:\n\n$$\n\\begin{align*}\n\\text{mem}_I(X, \\hat{\\Theta}, \\Theta) = \\text{mem}(X, \\hat{\\Theta}) - \\text{mem}_U(X, \\hat{\\Theta}, \\Theta)\n\\end{align*}\n$$\n\nThe key insight is that total memorization splits into:\n- **Useful patterns** that generalize to new data (intended memorization)  \n- **Spurious details** specific to training examples (unintended memorization)\n\nwhere:\n- $X$: Training dataset drawn from the true data distribution $p(x|\\Theta)$\n- $\\Theta$: Parameters of the true underlying data distribution  \n- $\\hat{\\Theta}$: Learned model parameters after training\n- $H(\\cdot)$: Shannon entropy measuring uncertainty\n- $I(\\cdot;\\cdot)$: Mutual information measuring shared information\n\nThis formulation provides theoretical clarity but requires practical estimation methods for real-world application.\n\n### Computing Memorization in Practice\n\nThe theoretical formulation requires practical estimation methods since we observe single instances of models and datasets. We transition from Shannon entropy to Kolmogorov complexity for computational tractability.\n\n**Kolmogorov Complexity Approximation**: For unintended memorization of a specific text $x$:\n\n$$\n\\begin{align*}\n\\text{mem}_K^U(x, \\theta, \\hat{\\theta}) = K(x|\\theta) - K(x|(\\theta, \\hat{\\theta}))\n\\end{align*}\n$$\n\nwhere $K(x|y)$ represents the Kolmogorov complexity of $x$ given $y$ - the length of the shortest program that generates $x$ when given access to $y$.\n\n**Arithmetic Coding Bridge**: Since Kolmogorov complexity is uncomputable, we use the fundamental connection between compression and probability. For any computable probability distribution $p$, arithmetic coding achieves compression length approximately equal to the negative log-probability:\n\n$$\n\\begin{align*}\nK(x) \\approx -\\log_2 p(x) + O(\\log |x|)\n\\end{align*}\n$$\n\n**Reference Model Method**: To estimate the true data distribution $\\theta$, we use a larger reference model $\\theta_{\\text{ref}}$ trained on a superset of the data. This gives us practical estimators:\n\n- $K(x|\\theta) \\approx -\\log_2 p(x|\\theta_{\\text{ref}})$: Compression cost using reference model\n- $K(x|(\\theta, \\hat{\\theta})) \\approx -\\log_2 \\max\\{p(x|\\theta_{\\text{ref}}), p(x|\\hat{\\theta})\\}$: Best compression using either model\n\n**Final Computable Measure**: Combining these approximations yields the practical unintended memorization measure:\n\n$$\n\\begin{align*}\n\\text{mem}_K^U(x, \\theta_{\\text{ref}}, \\hat{\\theta}) = \\max\\{0, -\\log_2 p(x|\\theta_{\\text{ref}}) + \\log_2 p(x|\\hat{\\theta})\\}\n\\end{align*}\n$$\n\nThis formulation captures the key intuition: if the target model assigns much higher probability to a text than the reference model, it has memorized sample-specific information not present in the general distribution.\n\n**Interpretation**: \n- When $\\text{mem}_K^U = 0$: Target model shows no unintended memorization\n- When $\\text{mem}_K^U > 0$: Target model has memorized $\\text{mem}_K^U$ bits of sample-specific information\n- Larger values indicate stronger memorization of training-specific details\n\n**Validation Requirements**: The reference model must be trained on a proper superset of the target model's training data to ensure it captures the true underlying distribution without the specific memorization artifacts we wish to measure.\n\n\n\u003Cdiv style={{ marginTop: '2rem', marginBottom: '2rem' }}>\n  \u003Ciframe\n    srcDoc={`\u003C!DOCTYPE html>\n\u003Chtml lang=\"en\">\n\u003Chead>\n    \u003Cmeta charset=\"UTF-8\">\n    \u003Cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \u003Ctitle>Memorization Measurement Visualization\u003C/title>\n    \u003Cstyle>\n        body {\n            font-family: Arial, sans-serif;\n            margin: 20px;\n            background-color: #f5f5f5;\n        }\n        .container {\n            max-width: 1200px;\n            margin: 0 auto;\n        }\n        .section {\n            background: white;\n            margin: 20px 0;\n            padding: 20px;\n            border-radius: 8px;\n            box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n        }\n        .model-box {\n            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n            color: white;\n            padding: 15px;\n            border-radius: 8px;\n            margin: 10px;\n            text-align: center;\n            min-width: 200px;\n            display: inline-block;\n        }\n        .text-sample {\n            background: #fff3cd;\n            border: 2px solid #ffeaa7;\n            padding: 15px;\n            border-radius: 8px;\n            margin: 10px 0;\n            font-family: monospace;\n        }\n        .compression-bar {\n            width: 100%;\n            height: 30px;\n            background: #e0e0e0;\n            border-radius: 15px;\n            position: relative;\n            margin: 10px 0;\n        }\n        .compression-fill {\n            height: 100%;\n            border-radius: 15px;\n            display: flex;\n            align-items: center;\n            justify-content: center;\n            color: white;\n            font-weight: bold;\n            min-width: 140px;\n        }\n        .high-compression { background: linear-gradient(90deg, #ff6b6b, #ee5a52); }\n        .medium-compression { background: linear-gradient(90deg, #feca57, #ff9ff3); }\n        .low-compression { background: linear-gradient(90deg, #48dbfb, #0abde3); }\n        \n        .flow-diagram {\n            display: flex;\n            align-items: center;\n            justify-content: space-around;\n            flex-wrap: wrap;\n            margin: 20px 0;\n        }\n        .arrow {\n            font-size: 24px;\n            color: #666;\n            margin: 0 10px;\n        }\n        .measurement-box {\n            background: #f8f9fa;\n            border: 2px solid #dee2e6;\n            padding: 15px;\n            border-radius: 8px;\n            margin: 10px;\n            text-align: center;\n        }\n        .memorization-result {\n            background: linear-gradient(135deg, #ff6b6b, #ee5a52);\n            color: white;\n            padding: 20px;\n            border-radius: 8px;\n            text-align: center;\n            font-size: 18px;\n            font-weight: bold;\n        }\n        .interactive-demo {\n            background: #e8f5e8;\n            border: 2px solid #4CAF50;\n            padding: 20px;\n            border-radius: 8px;\n            margin: 20px 0;\n        }\n        .slider-container {\n            margin: 15px 0;\n        }\n        .slider {\n            width: 100%;\n            margin: 10px 0;\n        }\n        .probability-display {\n            font-family: monospace;\n            font-size: 16px;\n            background: #fff;\n            padding: 10px;\n            border-radius: 4px;\n            margin: 10px 0;\n        }\n        h1, h2 {\n            color: #333;\n        }\n        .concept-box {\n            background: #f0f8ff;\n            border-left: 4px solid #4a90e2;\n            padding: 15px;\n            margin: 15px 0;\n        }\n\n    \u003C/style>\n\u003C/head>\n\u003Cbody>\n    \u003Cdiv class=\"container\">\n        \u003Ch1>ðŸ“Š How They Measure Language Model Memorization\u003C/h1>\n        \n\n        \n        \u003Cdiv class=\"section\">\n            \u003Ch2>1. The Basic Setup\u003C/h2>\n            \u003Cdiv class=\"concept-box\">\n                \u003Cstrong>Key Idea:\u003C/strong> Compare how well two different models can compress the same text. Better compression = more memorization.\n            \u003C/div>\n            \n            \u003Cdiv class=\"flow-diagram\">\n                \u003Cdiv class=\"model-box\">\n                    \u003Ch3>Reference Model\u003C/h3>\n                    \u003Cp>Trained on diverse, large dataset\u003C/p>\n                    \u003Cp>\u003Cem>Represents \"general knowledge\"\u003C/em>\u003C/p>\n                \u003C/div>\n                \u003Cdiv class=\"arrow\">vs\u003C/div>\n                \u003Cdiv class=\"model-box\">\n                    \u003Ch3>Target Model\u003C/h3>\n                    \u003Cp>Trained on specific dataset\u003C/p>\n                    \u003Cp>\u003Cem>Has general knowledge + memorization\u003C/em>\u003C/p>\n                \u003C/div>\n            \u003C/div>\n        \u003C/div>\n\n        \u003Cdiv class=\"section\">\n            \u003Ch2>2. Step-by-Step Measurement Process\u003C/h2>\n            \n            \u003Cdiv class=\"text-sample\">\n                \u003Cstrong>Test Text:\u003C/strong> \"The quick brown fox jumps over the lazy dog\"\n            \u003C/div>\n            \n            \u003Ch3>Step 1: Test Reference Model\u003C/h3>\n            \u003Cdiv class=\"measurement-box\">\n                \u003Cp>\u003Cstrong>Reference Model Probability:\u003C/strong> 0.01 (1%)\u003C/p>\n                \u003Cp>\u003Cstrong>Compression Cost:\u003C/strong> -logâ‚‚(0.01) = 6.64 bits\u003C/p>\n            \u003C/div>\n            \n            \u003Cdiv class=\"compression-bar\">\n                \u003Cdiv class=\"compression-fill low-compression\" style=\"width: 66%;\">\n                    6.64 bits needed\n                \u003C/div>\n            \u003C/div>\n            \n            \u003Ch3>Step 2: Test Target Model\u003C/h3>\n            \u003Cdiv class=\"measurement-box\">\n                \u003Cp>\u003Cstrong>Target Model Probability:\u003C/strong> 0.25 (25%)\u003C/p>\n                \u003Cp>\u003Cstrong>Compression Cost:\u003C/strong> -logâ‚‚(0.25) = 2.00 bits\u003C/p>\n            \u003C/div>\n            \n            \u003Cdiv class=\"compression-bar\">\n                \u003Cdiv class=\"compression-fill high-compression\" style=\"width: 20%;\">\n                    2.00 bits needed\n                \u003C/div>\n            \u003C/div>\n            \n            \u003Ch3>Step 3: Calculate Memorization\u003C/h3>\n            \u003Cdiv class=\"memorization-result\">\n                Memorization = 6.64 - 2.00 = 4.64 bits\n                \u003Cbr>\u003Csmall>The target model \"knows\" 4.64 bits more about this text!\u003C/small>\n            \u003C/div>\n        \u003C/div>\n\n        \u003Cdiv class=\"section\">\n            \u003Ch2>3. Interactive Demo: See How Probability Affects Compression\u003C/h2>\n            \u003Cdiv class=\"interactive-demo\">\n                \u003Cp>Adjust the model's probability for a text sequence and see how it affects compression:\u003C/p>\n                \n                \u003Cdiv class=\"slider-container\">\n                    \u003Clabel>Model's Probability for the text: \u003Cspan id=\"prob-value\">10%\u003C/span>\u003C/label>\n                    \u003Cinput type=\"range\" id=\"prob-slider\" class=\"slider\" min=\"1\" max=\"100\" value=\"10\">\n                \u003C/div>\n                \n                \u003Cdiv class=\"probability-display\">\n                    \u003Cdiv>Probability: \u003Cspan id=\"prob-decimal\">0.10\u003C/span>\u003C/div>\n                    \u003Cdiv>Compression Cost: \u003Cspan id=\"compression-cost\">3.32\u003C/span> bits\u003C/div>\n                    \u003Cdiv>Interpretation: \u003Cspan id=\"interpretation\">Moderate compression\u003C/span>\u003C/div>\n                \u003C/div>\n                \n                \u003Cdiv class=\"compression-bar\">\n                    \u003Cdiv id=\"compression-visual\" class=\"compression-fill medium-compression\" style=\"width: 33%;\">\n                        \u003Cspan id=\"compression-text\">3.32 bits\u003C/span>\n                    \u003C/div>\n                \u003C/div>\n                \n                \u003Cp>\u003Csmall>\u003Cstrong>Remember:\u003C/strong> Higher probability â†’ Better compression â†’ More memorization\u003C/small>\u003C/p>\n            \u003C/div>\n        \u003C/div>\n\n        \u003Cdiv class=\"section\">\n            \u003Ch2>4. Real Results from the Paper\u003C/h2>\n            \n            \u003Ch3>Different Types of Memorization:\u003C/h3>\n            \n            \u003Cdiv style=\"display: flex; justify-content: space-around; flex-wrap: wrap;\">\n                \u003Cdiv class=\"measurement-box\" style=\"background: #ffebee;\">\n                    \u003Ch4>ðŸ”´ High Memorization\u003C/h4>\n                    \u003Cp>\u003Cstrong>Example:\u003C/strong> Rare sequences, foreign text\u003C/p>\n                    \u003Cdiv class=\"compression-bar\">\n                        \u003Cdiv class=\"compression-fill high-compression\" style=\"width: 15%;\">\n                            1-2 bits\n                        \u003C/div>\n                    \u003C/div>\n                    \u003Cp>\u003Csmall>Model assigns very high probability\u003C/small>\u003C/p>\n                \u003C/div>\n                \n                \u003Cdiv class=\"measurement-box\" style=\"background: #fff3e0;\">\n                    \u003Ch4>ðŸŸ¡ Medium Memorization\u003C/h4>\n                    \u003Cp>\u003Cstrong>Example:\u003C/strong> Common phrases, typical sentences\u003C/p>\n                    \u003Cdiv class=\"compression-bar\">\n                        \u003Cdiv class=\"compression-fill medium-compression\" style=\"width: 40%;\">\n                            3-4 bits\n                        \u003C/div>\n                    \u003C/div>\n                    \u003Cp>\u003Csmall>Model has some specific knowledge\u003C/small>\u003C/p>\n                \u003C/div>\n                \n                \u003Cdiv class=\"measurement-box\" style=\"background: #e8f5e8;\">\n                    \u003Ch4>ðŸŸ¢ Low/No Memorization\u003C/h4>\n                    \u003Cp>\u003Cstrong>Example:\u003C/strong> Truly random sequences\u003C/p>\n                    \u003Cdiv class=\"compression-bar\">\n                        \u003Cdiv class=\"compression-fill low-compression\" style=\"width: 70%;\">\n                            6+ bits\n                        \u003C/div>\n                    \u003C/div>\n                    \u003Cp>\u003Csmall>Model treats it like any other text\u003C/small>\u003C/p>\n                \u003C/div>\n            \u003C/div>\n        \u003C/div>\n\n        \u003Cdiv class=\"section\">\n            \u003Ch2>5. Why This Method Works\u003C/h2>\n            \u003Cdiv class=\"concept-box\">\n                \u003Ch3>ðŸŽ¯ The Intuition:\u003C/h3>\n                \u003Cul>\n                    \u003Cli>\u003Cstrong>If a model can compress text really well\u003C/strong> â†’ it has \"seen\" or \"learned\" something specific about that text\u003C/li>\n                    \u003Cli>\u003Cstrong>If two models compress differently\u003C/strong> â†’ the difference shows what one model memorized that the other didn't\u003C/li>\n                    \u003Cli>\u003Cstrong>Better compression = higher probability = more memorization\u003C/strong>\u003C/li>\n                \u003C/ul>\n            \u003C/div>\n            \n            \u003Cdiv class=\"concept-box\">\n                \u003Ch3>ðŸ“ What the Numbers Mean:\u003C/h3>\n                \u003Cul>\n                    \u003Cli>\u003Cstrong>0-1 bits difference:\u003C/strong> Minimal memorization (normal generalization)\u003C/li>\n                    \u003Cli>\u003Cstrong>2-4 bits difference:\u003C/strong> Moderate memorization (model has some specific knowledge)\u003C/li>\n                    \u003Cli>\u003Cstrong>5+ bits difference:\u003C/strong> High memorization (model has strong specific knowledge)\u003C/li>\n                \u003C/ul>\n            \u003C/div>\n        \u003C/div>\n    \u003C/div>\n\n    \u003Cscript>\n        const probSlider = document.getElementById('prob-slider');\n        const probValue = document.getElementById('prob-value');\n        const probDecimal = document.getElementById('prob-decimal');\n        const compressionCost = document.getElementById('compression-cost');\n        const interpretation = document.getElementById('interpretation');\n        const compressionVisual = document.getElementById('compression-visual');\n        const compressionText = document.getElementById('compression-text');\n\n        function updateCompression() {\n            const prob = parseInt(probSlider.value);\n            const decimal = prob / 100;\n            const cost = -Math.log2(decimal);\n            const width = Math.min(cost * 10, 100);\n            \n            probValue.textContent = prob + '%';\n            probDecimal.textContent = decimal.toFixed(2);\n            compressionCost.textContent = cost.toFixed(2);\n            compressionText.textContent = cost.toFixed(2) + ' bits';\n            \n            // Update interpretation\n            if (cost \u003C 2) {\n                interpretation.textContent = 'Excellent compression (high memorization!)';\n                compressionVisual.className = 'compression-fill high-compression';\n            } else if (cost \u003C 4) {\n                interpretation.textContent = 'Good compression (moderate memorization)';\n                compressionVisual.className = 'compression-fill medium-compression';\n            } else {\n                interpretation.textContent = 'Poor compression (little memorization)';\n                compressionVisual.className = 'compression-fill low-compression';\n            }\n            \n            compressionVisual.style.width = width + '%';\n        }\n\n        probSlider.addEventListener('input', updateCompression);\n        updateCompression(); // Initialize\n        \n\n    \u003C/script>\n\u003C/body>\n\u003C/html>`}\n    width=\"100%\"\n    height=\"3450px\"\n    style={{ border: 'none', borderRadius: '8px', boxShadow: '0 4px 6px rgba(0, 0, 0, 0.1)' }}\n    title=\"Shannon Information Theory Visualization\"\n  />\n\u003C/div>\n\n### The 3.6 Bits-Per-Parameter Hypothesis\n\n**Fundamental Capacity Theorem**: Extensive empirical analysis reveals that neural networks in the GPT family exhibit a remarkably consistent memorization capacity governed by a universal constant \u003Ca href=\"#ref-3\">[3]\u003C/a>:\n\n$$\n\\begin{align*}\n\\text{Capacity}(\\theta) = \\alpha \\cdot |\\theta| \\text{ where } \\alpha = 3.6 \\text{ bits/parameter}\n\\end{align*}\n$$\n\nThis represents a fundamental constraint on the maximum mutual information between training data and model parameters:\n\n$$\n\\begin{align*}\nI(X; \\hat{\\Theta}) \\leq 3.6 \\cdot |\\hat{\\Theta}|\n\\end{align*}\n$$\n\n**Physical Interpretation**: Each parameter can store approximately 3.6 bits of information about the training data. This is significantly less than the theoretical maximum for floating-point parameters (which could theoretically store unlimited information), indicating that practical training dynamics impose strict limits.\n\n**Mathematical Foundation**: The capacity constraint emerges from the fundamental trade-off between fitting the training distribution and maintaining generalization capability. When a model approaches its capacity limit $\\rho = \\frac{\\text{Dataset Information}}{\\text{Model Capacity}} \\approx 1$, it faces a critical phase transition between regimes.\n\n**Universality Across Scales**: This 3.6 bits/parameter constant has been empirically validated across \u003Ca href=\"#ref-3\">[3]\u003C/a>:\n\n- **Model architectures**: GPT-2 family with standard transformer architectures\n- **Parameter counts**: From 80K to 1.5B parameters (spanning nearly 4 orders of magnitude)  \n- **Precision formats**: Both bf16 and fp32 training show identical capacity (3.64 bits/param)\n- **Training procedures**: Standard gradient descent with various optimizers\n- **Dataset types**: Multiple natural language corpora with different characteristics\n\n**Lower Bound Nature**: This measurement represents a practical lower bound on theoretical information storage capacity, not an upper limit. Several factors contribute to this gap:\n\n1. **Optimization Limitations**: Gradient descent may converge to local minima rather than globally optimal parameter configurations that maximize information storage\n2. **Continuous Parameter Spaces**: Theoretically, real-valued parameters have infinite precision and could store unbounded information\n3. **Training Dynamics**: The constraint reflects practical SGD training behavior, not information-theoretic limits\n4. **Regularization Effects**: Implicit regularization from finite precision arithmetic and finite training time\n\n**Connection to Parameter Efficiency**: The 3.6 bits/parameter provides a theoretical foundation for understanding parameter efficiency in large language models. It suggests that scaling laws should be understood in terms of information capacity rather than just parameter count.\n\n### Predicting Double Descent and Phase Transitions\n\nThe 3.6 bits/parameter capacity enables precise prediction of the double descent phenomenon through a critical capacity ratio that governs the memorization-generalization trade-off \u003Ca href=\"#ref-3\">[3]\u003C/a>.\n\n**Critical Capacity Ratio**: Double descent occurs precisely when the ratio of dataset information content to model memorization capacity approaches unity:\n\n$$\n\\begin{align*}\n\\rho = \\frac{H(X)}{\\text{Capacity}(\\theta)} = \\frac{|X| \\cdot \\log_2(V^S)}{3.6 \\cdot |\\theta|}\n\\end{align*}\n$$\n\nwhere:\n- $|X|$: Number of training sequences  \n- $V$: Vocabulary size\n- $S$: Sequence length\n- $|\\theta|$: Number of model parameters\n\n**Information Content Estimation**: The dataset entropy $H(X) = |X| \\cdot \\log_2(V^S)$ assumes uniform distribution over token sequences. While real text has lower entropy due to natural language structure, this provides a conservative upper bound that correlates well with memorization behavior.\n\n**Three Distinct Learning Regimes**:\n\n**1. Under-parameterized Regime** ($\\rho \u003C 1$):\n- **Characteristics**: Model capacity exceeds dataset information content\n- **Memorization Behavior**: Can memorize entire dataset with capacity to spare\n- **Training Dynamics**: Zero training loss achievable through perfect memorization\n- **Generalization**: Poor due to lack of compression pressure\n- **Mathematical Description**: $\\text{Capacity} > H(\\text{Data}) \\Rightarrow$ Pure memorization is optimal\n\n**2. Critical Regime** ($\\rho \\approx 1$):\n- **Characteristics**: Capacity exactly matches dataset information content  \n- **Memorization Behavior**: Forced to choose between different training examples\n- **Training Dynamics**: High variance, sensitivity to initialization\n- **Generalization**: Worst performance due to instability at phase boundary\n- **Mathematical Description**: Competition between memorization and compression\n\n**3. Over-parameterized Regime** ($\\rho > 1$):\n- **Characteristics**: Dataset information exceeds model capacity\n- **Memorization Behavior**: Must compress data to fit within capacity constraints\n- **Training Dynamics**: Forced compression leads to pattern discovery\n- **Generalization**: Improved through implicit regularization\n- **Mathematical Description**: $H(\\text{Data}) > \\text{Capacity} \\Rightarrow$ Compression necessary\n\n**Mathematical Characterization of Test Loss**: The test loss exhibits the characteristic double descent curve:\n\n$$\n\\begin{align*}\n\\mathcal{L}_{\\text{test}}(\\rho) = \\begin{cases}\n\\mathcal{L}_{\\text{interpolation}}(\\rho) & \\text{if } \\rho \u003C 1 \\\\\n\\mathcal{L}_{\\text{critical}} + \\epsilon(\\rho) & \\text{if } \\rho \\approx 1 \\\\\n\\mathcal{L}_{\\text{generalization}}(\\rho) & \\text{if } \\rho > 1\n\\end{cases}\n\\end{align*}\n$$\n\nwhere:\n- $\\mathcal{L}_{\\text{interpolation}}(\\rho)$: Decreasing loss in memorization regime\n- $\\mathcal{L}_{\\text{critical}}$: Peak loss at the critical transition point\n- $\\epsilon(\\rho)$: Sharp divergence term near $\\rho = 1$\n- $\\mathcal{L}_{\\text{generalization}}(\\rho)$: Decreasing loss in compression regime\n\n**Practical Implications**:\n\n1. **Model Sizing**: Target $\\rho > 1.5$ to avoid critical regime instabilities\n2. **Training Stability**: Expect higher variance when $\\rho \\approx 1$  \n3. **Scaling Laws**: Information capacity, not just parameter count, determines performance\n4. **Architecture Design**: Optimize for bits-per-parameter efficiency rather than raw size\n\n## Memorization, Privacy, and Security Implications\n\nUnderstanding memorization has profound implications for privacy and security in language model deployment. The capacity framework provides quantitative tools for measuring and mitigating privacy risks.\n\n### Scaling Law for Membership Inference Attacks\n\nMembership inference attacks attempt to determine whether a specific text was included in the training dataset. The success rate of such attacks follows a predictable scaling law based on the capacity ratio \u003Ca href=\"#ref-3\">[3]\u003C/a>:\n\n$$\n\\begin{align*}\n\\text{F1}_{\\text{MI}}(\\theta, \\mathcal{D}) = \\frac{1}{2}\\left(1 + 1.34\\sigma\\left(-0.034\\left(\\frac{\\text{Capacity}(\\theta)}{|\\mathcal{D}|} - 33.14\\right)\\right)\\right)\n\\end{align*}\n$$\n\nwhere:\n- $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the sigmoid function\n- $\\text{Capacity}(\\theta) = 3.6 \\cdot |\\theta|$ is the model's information capacity\n- $|\\mathcal{D}|$ represents the total information content of the dataset\n\n**Mathematical Interpretation**: This function exhibits a sharp transition around the critical point where $\\frac{\\text{Capacity}(\\theta)}{|\\mathcal{D}|} \\approx 33.14$, indicating a phase transition in attack vulnerability.\n\n**Attack Success Regimes**:\n- **High Capacity**: When models have excess capacity relative to data, they memorize verbatim, making membership inference easy\n- **Critical Point**: Sharp transition region with maximum attack vulnerability\n- **Low Capacity**: Over-parameterized models compress data, reducing attack success\n\n### Privacy Threshold and Safe Operating Regions\n\nModels achieve practical privacy protection when the capacity-to-data ratio falls below a critical threshold:\n\n$$\n\\begin{align*}\n\\frac{3.6 \\cdot |\\theta|}{|\\mathcal{D}|} \u003C 0.001\n\\end{align*}\n$$\n\nThis constraint translates to requiring more than 1000 bits of training data per model parameter, or equivalently:\n\n$$\n\\begin{align*}\n\\frac{\\text{Training tokens}}{|\\theta|} > \\frac{1000}{\\log_2(\\text{vocab size}) \\cdot \\text{sequence length}}\n\\end{align*}\n$$\n\nFor typical language models with vocabulary size $V = 50,000$ and sequence length $S = 2048$:\n\n$$\n\\begin{align*}\n\\frac{\\text{Training tokens}}{|\\theta|} > \\frac{1000}{15.6 \\cdot 2048} \\approx 0.031 \\text{ tokens/parameter}\n\\end{align*}\n$$\n\n**Practical Guidelines**:\n- **Minimum Data Requirement**: >100 tokens per parameter for membership inference resistance\n- **Safe Operating Zone**: >1000 tokens per parameter for strong privacy guarantees\n- **Critical Monitoring**: Track capacity utilization during training to avoid privacy-vulnerable regimes\n\n### The Extraction vs Memorization Paradox\n\nA counterintuitive finding emerges when examining extractable memorized content:\n\n**Key Observation**: When the probability of extracting training examples equals the probability of extracting test examples:\n$$\n\\begin{align*}\np_{\\text{extract}}(x \\in \\mathcal{D}_{\\text{train}}) \\approx p_{\\text{extract}}(x \\in \\mathcal{D}_{\\text{test}})\n\\end{align*}\n$$\n\nthen all successful extraction can be attributed to generalization capabilities rather than memorization.\n\n**Implications**:\n1. **True Memorization**: Only extractable content that appears more frequently from training data indicates genuine memorization\n2. **Generalization Masquerading**: Much \"extraction\" actually demonstrates the model's ability to generate realistic text from learned patterns\n3. **Privacy Assessment**: Simple extraction tests may overestimate privacy risks by conflating generation with memorization\n\n**Quantitative Measurement**: The memorization-specific extraction rate:\n$$\n\\begin{align*}\n\\text{True Memorization} = \\max\\{0, p_{\\text{extract}}(\\text{train}) - p_{\\text{extract}}(\\text{test})\\}\n\\end{align*}\n$$\n\nThis provides a more accurate measure of privacy-relevant memorization than raw extraction rates.\n\n## Memorization Patterns and Selective Learning\n\nNeural networks don't memorize randomly - they exhibit systematic preferences for certain types of content based on information-theoretic properties.\n\n### TF-IDF and Rarity-Based Selection\n\nDocuments with higher TF-IDF (Term Frequency-Inverse Document Frequency) scores are preferentially memorized due to their distinctiveness \u003Ca href=\"#ref-3\">[3]\u003C/a>:\n\n$$\n\\begin{align*}\n\\text{TF-IDF}(d, \\mathcal{D}) = \\frac{1}{|d|} \\sum_{w \\in d} \\text{tf}(w,d) \\cdot \\log \\frac{|\\mathcal{D}|}{\\text{df}(w)}\n\\end{align*}\n$$\n\nwhere:\n- $\\text{tf}(w,d)$: Term frequency of word $w$ in document $d$\n- $\\text{df}(w)$: Document frequency of word $w$ across corpus $\\mathcal{D}$\n- $|d|$: Length of document $d$\n- $|\\mathcal{D}|$: Total number of documents\n\n**Empirical Correlation**: Strong positive correlation ($r = 0.73$) between TF-IDF scores and memorization likelihood indicates that models preferentially store distinctive, rare content \u003Ca href=\"#ref-3\">[3]\u003C/a>.\n\n**Information-Theoretic Explanation**: High TF-IDF content has lower compressibility because:\n1. **Rare terms** have high information content ($-\\log p(\\text{term})$)\n2. **Unique combinations** resist compression through pattern matching\n3. **Low redundancy** prevents amortization across multiple examples\n\n### Memorization Priority Function\n\nThe probability that a model memorizes document $d$ follows a competitive allocation process:\n\n$$\n\\begin{align*}\nP(\\text{memorize}|d) \\propto \\exp\\left(\\beta \\cdot \\text{TF-IDF}(d) - \\gamma \\cdot \\text{compressibility}(d) + \\delta \\cdot \\text{frequency}(d)\\right)\n\\end{align*}\n$$\n\n**Parameter Interpretation**:\n- $\\beta > 0$: Rarity weight - higher values prioritize unique content\n- $\\gamma > 0$: Compressibility penalty - resist memorizing incompressible noise\n- $\\delta > 0$: Frequency boost - common patterns receive priority\n\n**Compressibility Estimation**: Using lossless compression ratio as a proxy:\n$$\n\\begin{align*}\n\\text{compressibility}(d) = 1 - \\frac{\\text{compressed\\_size}(d)}{\\text{original\\_size}(d)}\n\\end{align*}\n$$\n\n**Frequency-Based Competition**: When capacity is limited, documents compete for memorization slots:\n$$\n\\begin{align*}\n\\text{frequency}(d) = \\log\\left(1 + \\text{count}(d)\\right)\n\\end{align*}\n$$\n\n### Strategic Memorization Behavior\n\nModels exhibit intelligent allocation strategies that maximize information efficiency:\n\n**1. Outlier Prioritization**: Memorize samples that deviate significantly from learnable patterns\n**2. Frequency Balancing**: Store enough examples of rare patterns to enable generalization\n**3. Compression Optimization**: Prefer content that can be stored efficiently within parameter constraints\n\n**Mathematical Framework**: The optimal memorization strategy solves:\n$$\n\\begin{align*}\n\\max_{\\text{mem\\_set}} \\sum_{d \\in \\text{mem\\_set}} \\text{value}(d) \\text{ subject to } \\sum_{d \\in \\text{mem\\_set}} \\text{cost}(d) \\leq \\text{Capacity}\n\\end{align*}\n$$\n\nwhere:\n- $\\text{value}(d)$: Information value of memorizing document $d$\n- $\\text{cost}(d)$: Parameter capacity required to store $d$\n\nThis resembles a knapsack optimization problem, explaining the observed systematic memorization patterns.\n\n## Connections to Broader Deep Learning Phenomena\n\nThe memorization framework provides unified explanations for several mysterious phenomena in deep learning, revealing common underlying mechanisms.\n\n### Grokking: Sudden Generalization Through Capacity Reallocation\n\nGrokking - the phenomenon where models suddenly transition from memorization to generalization after extended training - can be understood as a dynamic capacity reallocation process.\n\n**Phase 1: Memorization Dominance** ($t \u003C t_{\\text{grok}}$):\n- **Capacity Allocation**: $C_{\\text{mem}}(t) \\approx \\text{Capacity}(\\theta)$, $C_{\\text{gen}}(t) \\approx 0$\n- **Learning Strategy**: Direct memorization of input-output pairs\n- **Loss Behavior**: Low training loss, high test loss (overfitting)\n- **Parameter Usage**: Most parameters encode specific training examples\n\n**Phase 2: Critical Transition** ($t = t_{\\text{grok}}$):\n- **Discovery Event**: Model discovers compressible algorithmic structure\n- **Capacity Reallocation**: Rapid shift from memorization to pattern encoding\n- **Mathematical Signature**: $\\frac{dC_{\\text{gen}}}{dt} \\gg 0$, $\\frac{dC_{\\text{mem}}}{dt} \\ll 0$\n- **Loss Dynamics**: Sharp improvement in test performance\n\n**Phase 3: Generalization Dominance** ($t > t_{\\text{grok}}$):\n- **Stable Allocation**: $C_{\\text{gen}} > C_{\\text{mem}}$, efficient pattern representation\n- **Algorithmic Behavior**: Model executes learned algorithms rather than table lookup\n- **Performance**: Both training and test loss remain low\n\n**Mathematical Model of Grokking**:\n$$\n\\begin{align*}\nC_{\\text{gen}}(t) = C_{\\text{total}} \\cdot \\sigma\\left(\\frac{t - t_{\\text{grok}}}{\\tau}\\right)\n\\end{align*}\n$$\n$$\n\\begin{align*}\nC_{\\text{mem}}(t) = C_{\\text{total}} - C_{\\text{gen}}(t)\n\\end{align*}\n$$\n\nwhere $\\tau$ controls the transition sharpness and $t_{\\text{grok}}$ is the critical time point.\n\n### Lottery Ticket Hypothesis: Optimal Capacity Utilization\n\nThe lottery ticket hypothesis states that dense networks contain sparse subnetworks that achieve comparable performance when trained in isolation. This connects directly to capacity efficiency.\n\n**Winning Ticket Characterization**: A winning lottery ticket is a subnetwork that achieves optimal capacity allocation:\n\n$$\n\\begin{align*}\n\\text{Score}(\\theta_{\\text{sub}}) = \\frac{\\text{Generalization Performance}(\\theta_{\\text{sub}})}{\\text{Capacity Used}} \\cdot \\text{Sparsity Bonus}\n\\end{align*}\n$$\n\nMore precisely:\n$$\n\\begin{align*}\n\\text{Score}(\\theta_{\\text{sub}}) = \\frac{\\mathcal{L}_{\\text{test}}^{-1}(\\theta_{\\text{sub}})}{3.6 \\cdot |\\theta_{\\text{sub}}|} \\cdot \\frac{|\\theta_{\\text{full}}|}{|\\theta_{\\text{sub}}|}\n\\end{align*}\n$$\n\n**Capacity Efficiency Interpretation**:\n- **Bad tickets**: Waste capacity on irrelevant memorization\n- **Good tickets**: Efficiently allocate capacity to generalizable patterns  \n- **Winning tickets**: Optimal balance of capacity utilization and performance\n\n**Connection to Memorization**: Pruning removes parameters that store unimportant memorized content, leaving those that encode useful patterns.\n\n### Emergent Abilities: Capacity Threshold Effects\n\nEmergent abilities appear suddenly when models exceed critical size thresholds. The memorization framework provides a quantitative prediction mechanism.\n\n**Emergence Threshold**: New capabilities emerge when model capacity exceeds task complexity:\n\n$$\n\\begin{align*}\n|\\theta|_{\\text{emergence}} = \\frac{H(\\text{Task})}{3.6} + \\epsilon_{\\text{compression}}\n\\end{align*}\n$$\n\nwhere:\n- $H(\\text{Task})$: Information content required for task competence\n- $\\epsilon_{\\text{compression}}$: Additional capacity needed for pattern discovery\n\n**Task Complexity Estimation**: For reasoning tasks requiring $n$ sequential steps with branching factor $b$:\n$$\n\\begin{align*}\nH(\\text{Task}) \\approx n \\cdot \\log_2(b) + \\log_2(\\text{solution space})\n\\end{align*}\n$$\n\n**Phase Transition Dynamics**: Emergence occurs when:\n1. **Insufficient Capacity**: Model can only memorize specific examples\n2. **Critical Capacity**: Model discovers algorithmic patterns that generalize\n3. **Abundant Capacity**: Reliable execution of learned algorithms\n\n**Examples**:\n- **Arithmetic**: Emerges when capacity exceeds the information needed to encode calculation procedures\n- **Chain-of-thought**: Appears when models can allocate capacity to intermediate reasoning steps\n- **In-context learning**: Develops when models can store pattern matching algorithms\n\n### Scaling Laws: Information-Theoretic Foundations\n\nTraditional scaling laws focus on parameter count, but the memorization framework suggests information capacity as the fundamental quantity.\n\n**Revised Scaling Law**:\n$$\n\\begin{align*}\n\\text{Performance} \\propto \\left(\\frac{\\text{Effective Capacity}}{\\text{Task Complexity}}\\right)^{\\alpha}\n\\end{align*}\n$$\n\nwhere $\\text{Effective Capacity} = 3.6 \\cdot |\\theta| \\cdot \\text{efficiency}(\\text{architecture})$\n\nThis explains why architectural improvements can achieve better scaling than simple parameter increases.\n\n## Practical Implementation\n\n### Capacity-Aware Model Design\n\n```python\nfrom typing import Dict\n\ndef compute_model_size(dataset_bits: int, safety_factor: float = 1.5, tokens_per_param: int = 1000) -> Dict[str, int]:\n    \"\"\"\n    Compute required model size given dataset and safety requirements\n\n    Args:\n        dataset_bits: Total information in dataset\n        safety_factor: Multiplicative safety margin\n        tokens_per_param: Target ratio for good generalization\n\n    Returns:\n        Dictionary with model specifications\n    \"\"\"\n    bits_per_param = 3.6\n\n    # Memorization-based lower bound\n    min_params_memorization = dataset_bits / bits_per_param\n\n    # Generalization-based sizing\n    min_params_generalization = dataset_bits / (tokens_per_param * np.log2(vocab_size))\n\n    # Take maximum and apply safety factor\n    recommended_params = max(min_params_memorization, min_params_generalization) * safety_factor\n\n    return {\n        'parameters': int(recommended_params),\n        'capacity_bits': recommended_params * bits_per_param,\n        'memorization_ratio': dataset_bits / (recommended_params * bits_per_param)\n    }\n```\n\n### Measuring Unintended Memorization\n\n```python\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom typing import List\n\ndef measure_unintended_memorization(model: nn.Module, reference_model: nn.Module, dataset: DataLoader) -> float:\n    \"\"\"\n    Quantify unintended memorization using reference model method\n\n    Args:\n        model: Target model being evaluated\n        reference_model: Larger model trained on superset\n        dataset: Evaluation dataset\n\n    Returns:\n        Unintended memorization in bits\n    \"\"\"\n    mem_U_total = 0\n\n    for x in dataset:\n        # Compute negative log-likelihoods\n        nll_target = -torch.log(model.forward(x))\n        nll_reference = -torch.log(reference_model.forward(x))\n\n        # Kolmogorov complexity approximation\n        H_K_given_reference = nll_reference\n        H_K_given_both = torch.min(nll_target, nll_reference)\n\n        # Unintended memorization for this sample\n        mem_U = torch.clamp(H_K_given_reference - H_K_given_both, min=0)\n        mem_U_total += mem_U.item()\n\n    return mem_U_total\n```\n\n## Theoretical Foundations and Future Directions\n\n### Statistical Learning Theory Connections\n\nThe memorization framework bridges empirical observations with established theoretical foundations in statistical learning theory.\n\n**PAC-Bayes Bounds**: The generalization gap can be bounded using the memorization capacity:\n$$\n\\begin{align*}\n\\mathbb{E}[\\mathcal{L}_{\\text{test}} - \\mathcal{L}_{\\text{train}}] \\leq \\sqrt{\\frac{\\text{mem}(X, \\hat{\\Theta}) + \\log(1/\\delta)}{2m}}\n\\end{align*}\n$$\n\nwhere $m$ is the training set size and $\\delta$ is the confidence parameter.\n\n**Rademacher Complexity**: Memorization capacity provides an upper bound on the Rademacher complexity of the hypothesis class:\n$$\n\\begin{align*}\n\\mathfrak{R}_m(\\mathcal{H}) \\leq \\sqrt{\\frac{3.6 \\cdot |\\theta|}{m}}\n\\end{align*}\n$$\n\n**Minimum Description Length (MDL)**: The optimal model balances fit quality with description length:\n$$\n\\begin{align*}\n\\text{Score}(\\theta) = \\mathcal{L}_{\\text{train}}(\\theta) + \\frac{3.6 \\cdot |\\theta|}{m}\n\\end{align*}\n$$\n\n### Multi-Modal and Cross-Domain Extensions\n\nThe capacity framework extends beyond language models to other domains and modalities.\n\n**Multi-Modal Capacity Sharing**: For models processing multiple modalities (vision, language, audio):\n$$\n\\begin{align*}\n\\text{Capacity}_{\\text{total}} = \\sum_{m \\in \\text{modalities}} \\alpha_m \\cdot |\\theta_m| + \\alpha_{\\text{shared}} \\cdot |\\theta_{\\text{shared}}|\n\\end{align*}\n$$\n\nwhere:\n- $\\alpha_m$: Modality-specific capacity coefficient (may differ from 3.6)\n- $|\\theta_m|$: Parameters dedicated to modality $m$\n- $\\alpha_{\\text{shared}}$: Capacity coefficient for shared representations\n\n**Cross-Domain Transfer**: When fine-tuning across domains:\n$$\n\\begin{align*}\n\\text{Transfer Efficiency} = \\frac{\\Delta \\text{Performance}}{\\Delta \\text{Capacity Used}}\n\\end{align*}\n$$\n\n**Continual Learning Dynamics**: Capacity allocation evolves over sequential tasks:\n$$\n\\begin{align*}\nC_{\\text{task}_i}(t) = C_{\\text{total}} \\cdot \\exp\\left(-\\lambda(t - t_i)\\right) \\cdot \\frac{I(\\text{task}_i)}{\\sum_j I(\\text{task}_j)}\n\\end{align*}\n$$\n\nwhere $\\lambda$ represents the forgetting rate and $I(\\text{task}_i)$ is task information content.\n\n### Quantum and Neuromorphic Extensions\n\n**Quantum Neural Networks**: Theoretical capacity for quantum parameters exploiting superposition:\n$$\n\\begin{align*}\n\\text{Capacity}_{\\text{quantum}} = \\alpha_{\\text{classical}} \\cdot \\log_2(d) \\cdot |\\theta_{\\text{quantum}}|\n\\end{align*}\n$$\n\nwhere $d$ is the Hilbert space dimension, potentially providing exponential capacity advantages.\n\n**Neuromorphic Computing**: Spike-based neural networks may exhibit different capacity constraints:\n$$\n\\begin{align*}\n\\text{Capacity}_{\\text{spike}} = \\beta \\cdot |\\text{synapses}| \\cdot \\log_2(\\text{spike rates})\n\\end{align*}\n$$\n\n### Open Research Questions and Future Work\n\n**1. Architecture Universality**: \n- Does $\\alpha = 3.6$ hold for Transformers beyond GPT-2?\n- How do architectural innovations (attention mechanisms, normalization) affect capacity?\n- What is the capacity coefficient for other architectures (CNNs, RNNs, graph networks)?\n\n**2. Optimization Dependency**:\n- Can alternative optimization methods (evolutionary algorithms, second-order methods) exceed the 3.6 bits/parameter bound?\n- How do different learning rate schedules affect capacity utilization?\n- What role does the optimization landscape geometry play?\n\n**3. Task-Specific Capacity**:\n- How does task structure affect effective memorization capacity?\n- Can we predict task-specific capacity requirements?\n- How do compositional tasks scale with capacity?\n\n**4. Biological Parallels**:\n- Do biological neural networks exhibit similar capacity constraints?\n- How does synaptic plasticity relate to artificial parameter updates?\n- Can neuroscience inform better capacity utilization strategies?\n\n**5. Reversibility and Unlearning**:\n- Can memorized information be selectively removed without affecting generalization?\n- How can we design \"forgettable\" training procedures?\n- What are the fundamental limits of machine unlearning?\n\n**6. Efficiency Optimization**:\n- How can we maximize the effective capacity per parameter?\n- What architectural modifications improve capacity efficiency?\n- Can we predict optimal model sizes for given datasets?\n\n### Practical Implementation Framework\n\n**Capacity-Aware Training Pipeline**:\n1. **Pre-training Analysis**: Estimate dataset information content\n2. **Model Sizing**: Use capacity formula to determine optimal architecture\n3. **Training Monitoring**: Track capacity utilization throughout training\n4. **Post-training Audit**: Measure unintended memorization for privacy assessment\n5. **Deployment Optimization**: Prune parameters storing irrelevant memorized content\n\n**Capacity Optimization Strategies**:\n- **Dynamic Capacity Allocation**: Adjust parameter allocation during training based on task requirements\n- **Hierarchical Memorization**: Structure models to separate general patterns from specific memorization\n- **Federated Capacity**: Distribute memorization across multiple model instances to enhance privacy\n\n\n\n## How does reasoning come into play?\n\nApple's recent paper \"The Illusion of Thinking\" \u003Ca href=\"#ref-1\">[1]\u003C/a> claimed models \"collapse\" on complex reasoning tasks, with non-reasoning models outperforming reasoning ones on simple tasks. However, \"The Illusion of Illusion of Thinking\" paper \u003Ca href=\"#ref-2\">[2]\u003C/a> revealed critical experimental design flaws:\n\n### Prompt Design\nApple's prompts asked models to enumerate every Tower of Hanoi move, causing output token limits where models explicitly stated \"The pattern continues, but to avoid making this too long, I'll stop here.\" \u003Ca href=\"#ref-1\">[1]\u003C/a> When prompted to generate functions instead (\"Output a Lua function that prints the solution when called\"), Claude Opus, Sonnet, and Gemini correctly produced recursive algorithms for 15-disk solutions \u003Ca href=\"#ref-2\">[2]\u003C/a>.\n\n### Unsolvable Problems\nApple marked models as failures for not solving mathematically impossible River Crossing puzzles (N â‰¥ 6 with boat capacity of 3) \u003Ca href=\"#ref-1\">[1]\u003C/a>.\n\nBoth sides acknowledge that current models have real output limitations, but experimental design can make those limits appear more fundamental than they are.\n\nHere's an example of successfully solving the Tower of Hanoi algorithmically using AI:\n\n\u003Cdiv style={{ marginTop: '2rem', marginBottom: '2rem' }}>\n  \u003Ciframe\n    srcDoc={`\u003C!DOCTYPE html>\n\u003Chtml lang=\"en\">\n\u003Chead>\n    \u003Cmeta charset=\"UTF-8\">\n    \u003Cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \u003Ctitle>Tower of Hanoi Solver\u003C/title>\n    \u003Cstyle>\n        body {\n            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n            max-width: 1200px;\n            margin: 0 auto;\n            padding: 20px;\n            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n            color: white;\n            min-height: 100vh;\n            overflow: hidden;\n        }\n\n        .container {\n            background: rgba(255, 255, 255, 0.1);\n            backdrop-filter: blur(10px);\n            border-radius: 20px;\n            padding: 30px;\n            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);\n        }\n\n        h1 {\n            text-align: center;\n            margin-bottom: 30px;\n            font-size: 2.5em;\n            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);\n        }\n\n        .controls {\n            text-align: center;\n            margin-bottom: 30px;\n            display: flex;\n            gap: 15px;\n            justify-content: center;\n            align-items: center;\n            flex-wrap: wrap;\n        }\n\n        .control-group {\n            display: flex;\n            align-items: center;\n            gap: 10px;\n        }\n\n        label {\n            font-weight: 600;\n            font-size: 1.1em;\n        }\n\n        input, button, select {\n            padding: 10px 15px;\n            border: none;\n            border-radius: 8px;\n            font-size: 16px;\n            font-weight: 600;\n        }\n\n        input, select {\n            background: rgba(255, 255, 255, 0.9);\n            color: #333;\n            width: 35px;\n        }\n\n        select {\n            width: 132px;\n        }\n\n        button {\n            background: linear-gradient(45deg, #ff6b6b, #ee5a24);\n            color: white;\n            cursor: pointer;\n            transition: all 0.3s ease;\n            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);\n        }\n\n        button:hover {\n            transform: translateY(-2px);\n            box-shadow: 0 6px 20px rgba(0, 0, 0, 0.3);\n        }\n\n        button:disabled {\n            background: #666;\n            cursor: not-allowed;\n            transform: none;\n        }\n\n        .game-area {\n            display: flex;\n            justify-content: space-around;\n            align-items: flex-end;\n            height: 400px;\n            margin: 40px 0;\n            background: rgba(0, 0, 0, 0.1);\n            border-radius: 15px;\n            padding: 20px;\n        }\n\n        .tower {\n            display: flex;\n            flex-direction: column;\n            align-items: center;\n            position: relative;\n            width: 200px;\n        }\n\n        .tower-base {\n            width: 180px;\n            height: 20px;\n            background: linear-gradient(45deg, #8B4513, #A0522D);\n            border-radius: 10px;\n            margin-top: 10px;\n            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);\n            position: relative;\n            z-index: 1;\n        }\n\n        .tower-pole {\n            width: 8px;\n            min-height: 250px;\n            height: calc(100% - 80px);\n            background: linear-gradient(to bottom, #654321, #8B4513);\n            border-radius: 4px;\n            position: absolute;\n            top: 60px;\n            box-shadow: 2px 0 4px rgba(0, 0, 0, 0.3);\n            z-index: 2;\n        }\n\n        .tower-label {\n            font-size: 1.5em;\n            font-weight: bold;\n            margin-bottom: 20px;\n            text-shadow: 1px 1px 2px rgba(0, 0, 0, 0.5);\n            position: relative;\n            z-index: 10;\n            text-align: center;\n        }\n\n        .disk {\n            height: 20px;\n            border-radius: 10px;\n            margin: 1px 0;\n            display: flex;\n            align-items: center;\n            justify-content: center;\n            font-weight: bold;\n            color: white;\n            text-shadow: 1px 1px 2px rgba(0, 0, 0, 0.5);\n            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.3);\n            transition: all 0.3s ease;\n            position: relative;\n            z-index: 20;\n            font-size: 0.8em;\n        }\n\n        .tower-content {\n            display: flex;\n            flex-direction: column-reverse;\n            align-items: center;\n            position: relative;\n            height: 300px;\n            justify-content: flex-end;\n        }\n\n        .stats {\n            display: grid;\n            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\n            gap: 20px;\n            margin: 30px 0;\n        }\n\n        .stat-card {\n            background: rgba(255, 255, 255, 0.1);\n            padding: 20px;\n            border-radius: 15px;\n            text-align: center;\n            backdrop-filter: blur(5px);\n        }\n\n        .stat-value {\n            font-size: 2em;\n            font-weight: bold;\n            color: #ffd700;\n            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);\n        }\n\n\n\n        @keyframes pulse {\n            0%, 100% { opacity: 1; }\n            50% { opacity: 0.7; }\n        }\n\n        .moving {\n            transform: translateY(-50px);\n            z-index: 100;\n        }\n\n\n    \u003C/style>\n\u003C/head>\n\u003Cbody>\n    \u003Cdiv class=\"container\">\n        \u003Ch1>ðŸ—¼ Tower of Hanoi Solver\u003C/h1>\n        \n        \u003Cdiv class=\"controls\">\n            \u003Cdiv class=\"control-group\">\n                \u003Clabel for=\"diskCount\">Number of Disks:\u003C/label>\n                \u003Cinput type=\"number\" id=\"diskCount\" min=\"1\" max=\"15\" value=\"3\">\n            \u003C/div>\n            \u003Cdiv class=\"control-group\">\n                \u003Clabel for=\"speed\">Animation Speed:\u003C/label>\n                \u003Cselect id=\"speed\">\n                    \u003Coption value=\"2000\">Slow\u003C/option>\n                    \u003Coption value=\"1000\">Medium\u003C/option>\n                    \u003Coption value=\"500\">Fast\u003C/option>\n                    \u003Coption value=\"100\">Very Fast\u003C/option>\n                    \u003Coption value=\"10\" selected>Ultra Fast\u003C/option>\n                    \u003Coption value=\"1\">Lightning\u003C/option>\n                \u003C/select>\n            \u003C/div>\n            \u003Cbutton id=\"solveBtn\">Solve Automatically\u003C/button>\n            \u003Cbutton id=\"resetBtn\">Reset\u003C/button>\n            \u003Cbutton id=\"stepBtn\" disabled>Next Step\u003C/button>\n        \u003C/div>\n\n\n\n        \u003Cdiv class=\"game-area\">\n            \u003Cdiv class=\"tower\" id=\"tower-A\">\n                \u003Cdiv class=\"tower-label\">Tower A (Source)\u003C/div>\n                \u003Cdiv class=\"tower-content\">\n                    \u003Cdiv class=\"tower-pole\">\u003C/div>\n                    \u003Cdiv class=\"tower-base\">\u003C/div>\n                \u003C/div>\n            \u003C/div>\n            \u003Cdiv class=\"tower\" id=\"tower-B\">\n                \u003Cdiv class=\"tower-label\">Tower B (Auxiliary)\u003C/div>\n                \u003Cdiv class=\"tower-content\">\n                    \u003Cdiv class=\"tower-pole\">\u003C/div>\n                    \u003Cdiv class=\"tower-base\">\u003C/div>\n                \u003C/div>\n            \u003C/div>\n            \u003Cdiv class=\"tower\" id=\"tower-C\">\n                \u003Cdiv class=\"tower-label\">Tower C (Destination)\u003C/div>\n                \u003Cdiv class=\"tower-content\">\n                    \u003Cdiv class=\"tower-pole\">\u003C/div>\n                    \u003Cdiv class=\"tower-base\">\u003C/div>\n                \u003C/div>\n            \u003C/div>\n        \u003C/div>\n\n        \u003Cdiv class=\"stats\">\n            \u003Cdiv class=\"stat-card\">\n                \u003Cdiv class=\"stat-value\" id=\"moveCount\">0\u003C/div>\n                \u003Cdiv>Moves Made\u003C/div>\n            \u003C/div>\n            \u003Cdiv class=\"stat-card\">\n                \u003Cdiv class=\"stat-value\" id=\"minMoves\">7\u003C/div>\n                \u003Cdiv>Minimum Moves\u003C/div>\n            \u003C/div>\n            \u003Cdiv class=\"stat-card\">\n                \u003Cdiv class=\"stat-value\" id=\"totalTime\">0\u003C/div>\n                \u003Cdiv>Time (seconds)\u003C/div>\n            \u003C/div>\n        \u003C/div>\n\n\n    \u003C/div>\n\n    \u003Cscript>\n        class TowerOfHanoi {\n            constructor() {\n                this.towers = { A: [], B: [], C: [] };\n                this.moves = [];\n                this.currentMoveIndex = 0;\n                this.moveCount = 0;\n                this.isAnimating = false;\n                this.startTime = null;\n                this.timerInterval = null;\n                this.diskColors = [\n                    'linear-gradient(45deg, #FF6B6B, #FF8E8E)',\n                    'linear-gradient(45deg, #4ECDC4, #7FDBDA)',\n                    'linear-gradient(45deg, #45B7D1, #96CEB4)',\n                    'linear-gradient(45deg, #FFA07A, #FFB347)',\n                    'linear-gradient(45deg, #98D8C8, #F7DC6F)',\n                    'linear-gradient(45deg, #BB8FCE, #D7BDE2)',\n                    'linear-gradient(45deg, #F8C471, #F9E79F)',\n                    'linear-gradient(45deg, #85C1E9, #AED6F1)',\n                    'linear-gradient(45deg, #F1948A, #F8BBD9)',\n                    'linear-gradient(45deg, #82E0AA, #A9DFBF)',\n                    'linear-gradient(45deg, #F7DC6F, #F4D03F)',\n                    'linear-gradient(45deg, #AED6F1, #85C1E9)',\n                    'linear-gradient(45deg, #D7BDE2, #BB8FCE)',\n                    'linear-gradient(45deg, #F9E79F, #F8C471)',\n                    'linear-gradient(45deg, #7FDBDA, #4ECDC4)'\n                ];\n                \n                this.initializeEventListeners();\n                this.reset();\n            }\n\n            initializeEventListeners() {\n                document.getElementById('solveBtn').addEventListener('click', () => this.solve());\n                document.getElementById('resetBtn').addEventListener('click', () => this.reset());\n                document.getElementById('stepBtn').addEventListener('click', () => this.nextStep());\n                document.getElementById('diskCount').addEventListener('change', () => this.reset());\n\n            }\n\n\n\n            reset() {\n                this.stopTimer();\n                this.towers = { A: [], B: [], C: [] };\n                this.moves = [];\n                this.currentMoveIndex = 0;\n                this.moveCount = 0;\n                this.isAnimating = false;\n                \n                const diskCount = parseInt(document.getElementById('diskCount').value);\n                \n                // Initialize tower A with disks (largest to smallest from bottom to top)\n                for (let i = diskCount; i >= 1; i--) {\n                    this.towers.A.push(i);\n                }\n                \n                this.updateDisplay();\n                this.updateStats();\n                \n                // Enable/disable buttons\n                document.getElementById('solveBtn').disabled = false;\n                document.getElementById('stepBtn').disabled = true;\n            }\n\n            solve() {\n                if (this.isAnimating) return;\n                \n                const diskCount = parseInt(document.getElementById('diskCount').value);\n                this.moves = [];\n                this.solveTowerOfHanoi(diskCount, 'A', 'C', 'B');\n                \n                // Start automatic solving\n                document.getElementById('solveBtn').disabled = true;\n                document.getElementById('stepBtn').disabled = false;\n                this.startTimer();\n                this.animateSolution();\n            }\n\n            solveTowerOfHanoi(n, source, destination, auxiliary) {\n                if (n === 1) {\n                    this.moves.push({ from: source, to: destination, disk: 1 });\n                } else {\n                    // Move n-1 disks from source to auxiliary\n                    this.solveTowerOfHanoi(n - 1, source, auxiliary, destination);\n                    \n                    // Move the largest disk from source to destination\n                    this.moves.push({ from: source, to: destination, disk: n });\n                    \n                    // Move n-1 disks from auxiliary to destination\n                    this.solveTowerOfHanoi(n - 1, auxiliary, destination, source);\n                }\n            }\n\n            async animateSolution() {\n                const speed = parseInt(document.getElementById('speed').value);\n                this.isAnimating = true;\n                \n                for (let i = 0; i \u003C this.moves.length; i++) {\n                    // Check if animation was interrupted (e.g., by reset)\n                    if (!this.isAnimating) {\n                        break;\n                    }\n                    \n                    await this.animateMove(this.moves[i], i);\n                    await this.delay(speed);\n                }\n                \n                // Only disable solve button if we completed the animation\n                if (this.isAnimating) {\n                    this.isAnimating = false;\n                    this.stopTimer();\n                    document.getElementById('solveBtn').disabled = false;\n                }\n            }\n\n            async animateMove(move, moveIndex) {\n                const { from, to } = move;\n                \n                // Get the disk to move\n                const disk = this.towers[from].pop();\n                const diskElement = this.getDiskElement(from, this.towers[from].length);\n                \n                // Animate the move\n                if (diskElement) {\n                    diskElement.classList.add('moving');\n                    await this.delay(250);\n                    diskElement.classList.remove('moving');\n                }\n                \n                // Move the disk to destination\n                this.towers[to].push(disk);\n                this.moveCount++;\n                \n                this.updateDisplay();\n                this.updateStats();\n            }\n\n            nextStep() {\n                if (this.currentMoveIndex \u003C this.moves.length) {\n                    const move = this.moves[this.currentMoveIndex];\n                    this.animateMove(move, this.currentMoveIndex);\n                    this.currentMoveIndex++;\n                    \n                    if (this.currentMoveIndex >= this.moves.length) {\n                        document.getElementById('stepBtn').disabled = true;\n                        this.stopTimer();\n                    }\n                }\n            }\n\n            getDiskElement(tower, index) {\n                const towerElement = document.getElementById(\\`tower-\\${tower}\\`);\n                const towerContent = towerElement.querySelector('.tower-content');\n                const diskElements = towerContent.querySelectorAll('.disk');\n                return diskElements[index];\n            }\n\n            updateDisplay() {\n                // Clear all towers\n                ['A', 'B', 'C'].forEach(towerName => {\n                    const tower = document.getElementById(\\`tower-\\${towerName}\\`);\n                    const existingDisks = tower.querySelectorAll('.disk');\n                    existingDisks.forEach(disk => disk.remove());\n                });\n\n                // Render disks on each tower\n                ['A', 'B', 'C'].forEach(towerName => {\n                    const tower = document.getElementById(\\`tower-\\${towerName}\\`);\n                    const towerContent = tower.querySelector('.tower-content');\n                    const towerDisks = this.towers[towerName];\n                    \n                    towerDisks.forEach((diskSize, index) => {\n                        const disk = document.createElement('div');\n                        disk.className = 'disk';\n                        disk.style.width = \\`\\${40 + diskSize * 10}px\\`;\n                        disk.style.background = this.diskColors[diskSize - 1];\n                        disk.textContent = diskSize;\n                        \n                        // Insert disk into tower content (before the base)\n                        const base = towerContent.querySelector('.tower-base');\n                        towerContent.insertBefore(disk, base);\n                    });\n                });\n            }\n\n            updateStats() {\n                document.getElementById('moveCount').textContent = this.moveCount;\n                const diskCount = parseInt(document.getElementById('diskCount').value);\n                document.getElementById('minMoves').textContent = Math.pow(2, diskCount) - 1;\n            }\n\n\n\n\n\n\n\n            startTimer() {\n                this.startTime = Date.now();\n                this.timerInterval = setInterval(() => {\n                    const elapsed = Math.floor((Date.now() - this.startTime) / 1000);\n                    document.getElementById('totalTime').textContent = elapsed;\n                }, 1000);\n            }\n\n            stopTimer() {\n                if (this.timerInterval) {\n                    clearInterval(this.timerInterval);\n                    this.timerInterval = null;\n                }\n            }\n\n            delay(ms) {\n                return new Promise(resolve => setTimeout(resolve, ms));\n            }\n        }\n\n        // Initialize the game when the page loads\n        window.addEventListener('DOMContentLoaded', () => {\n            new TowerOfHanoi();\n        });\n    \u003C/script>\n\u003C/body>\n\u003C/html>`}\n    width=\"100%\"\n    height=\"1075px\"\n    style={{ border: 'none', borderRadius: '8px', boxShadow: '0 4px 6px rgba(0, 0, 0, 0.1)' }}\n    title=\"Tower of Hanoi Algorithmic Solver\"\n  />\n\u003C/div>\n\n**Tower of Hanoi Recursive Algorithm:**\n\n```python\nfrom typing import List, Tuple, Optional\n\ndef solve_tower_of_hanoi(\n    n: int,\n    source: str = \"A\",\n    destination: str = \"C\",\n    auxiliary: str = \"B\",\n    moves: Optional[List[Tuple[int, str, str]]] = None,\n) -> Optional[List[Tuple[int, str, str]]]:\n    \"\"\"\n    Solve the Towers of Hanoi problem recursively.\n\n    Parameters\n    ----------\n    n : int\n        Number of disks to move\n    source : str, optional\n        Name of the source peg (default is 'A')\n    destination : str, optional\n        Name of the destination peg (default is 'C')\n    auxiliary : str, optional\n        Name of the auxiliary peg (default is 'B')\n    moves : list, optional\n        List to store the sequence of moves (default is None)\n\n    Returns\n    -------\n    list\n        A list of tuples representing the moves, where each tuple is\n        (disk_number, source_peg, destination_peg)\n\n    Notes\n    -----\n    The algorithm follows the recursive pattern:\n    1. Move n-1 disks from source to auxiliary (using destination as temporary)\n    2. Move the largest disk from source to destination\n    3. Move n-1 disks from auxiliary to destination (using source as temporary)\n\n    Examples\n    --------\n    >>> solve_tower_of_hanoi(3)\n    [(1, 'A', 'C'), (2, 'A', 'B'), (1, 'C', 'B'), (3, 'A', 'C'),\n     (1, 'B', 'A'), (2, 'B', 'C'), (1, 'A', 'C')]\n    \"\"\"\n    # Initialize moves list on first call\n    if moves is None:\n        moves = []\n\n    # Base case: no disks to move\n    if n == 0:\n        return moves\n\n    # Recursive case: move n disks\n    # Step 1: Move n-1 disks from source to auxiliary (using destination as temporary)\n    solve_tower_of_hanoi(n - 1, source, auxiliary, destination, moves)\n\n    # Step 2: Move the largest disk from source to destination\n    moves.append((n, source, destination))\n\n    # Step 3: Move n-1 disks from auxiliary to destination (using source as temporary)\n    solve_tower_of_hanoi(n - 1, auxiliary, destination, source, moves)\n\n    return moves\n```\n\n## Understanding the relationship\n\nUnderstanding the relationship between memorization, generalization and reasoning is critical for:\n\n### 1. Evaluation Design\nTests that expect memorized responses miss models that have progressed to generalization and reasoning. When evaluations reward exhaustive approaches (like move lists), they penalize models that have learned to think abstractly and generate practical engineering solutions.\n\n### 2. Capability Unlocking\nReasoning \"failures\" often stem from poor evaluation design. Tweaking task setup or prompts can reveal genuine capabilities valuable for workflow automation, coding, and information extraction. Additionally, understanding when models move from memorization to generalization can help us in areas of efficiency and intelligence.\n\n### 3. Tool Integration\nLLMs should be paired with tools to augment reasoning. In our experiments, Claude Opus solved Tower of Hanoi through recursive algorithms, not exhaustive move enumeration.\n\n### 4. Privacy and Security Implications\nUnderstanding memorization carries significant implications for privacy and security in language model deployment. The capacity framework offers quantitative tools for assessing and reducing privacy risks.\n\n## Conclusion\n\nModels are generalizing effectively and knowing at what points helps us iterate on intelligence. With proper prompts, tools, and evaluation frameworks, models demonstrate stronger reasoning than flawed tests suggest. While not yet human-level reasoners, the future isn't as bleak as some evaluations indicate. The \"collapse\" often reflects our testing limitations, not fundamental model constraints.\n\nUnderstanding these distinctions is crucial for determining where innovation is truly needed versus where better implementation can unlock existing capabilities.\n\n## References\n\n\u003Ca id=\"ref-1\">\u003C/a>[1] Shojaee, P., Mirzadeh, I., Alizadeh, K., Horton, M., Bengio, S., & Farajtabar, M. (2025). The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity. Apple. Retrieved from https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf\n\n\u003Ca id=\"ref-2\">\u003C/a>[2] Opus, C., & Lawsen, A. (2025). The Illusion of the Illusion of Thinking: A Comment on Shojaee et al. (2025). arXiv preprint arXiv:2506.09250. Retrieved from https://arxiv.org/pdf/2506.09250\n\n\u003Ca id=\"ref-3\">\u003C/a>[3] Morris, J. X., Sitawarin, C., Guo, C., Kokhlikyan, N., Suh, G. E., Rush, A. M., Chaudhuri, K., & Mahloujifar, S. (2025). How much do language models memorize? arXiv preprint arXiv:2505.24832. Retrieved from https://arxiv.org/pdf/2505.24832","src/content/blog/memorization-generalization-and-reasoning.mdx","46c2348006038ffa","memorization-generalization-and-reasoning.mdx","self-evolving-llm-agents",{"id":213,"data":215,"body":219,"filePath":220,"digest":221,"legacyId":222,"deferredRender":23},{"title":216,"date":217,"authors":187,"description":218},"Continuous Self-Learning in AI Agents","2025-11-10","An overview of two frameworks for memory and context management to enable continous self-learning systems","## The Challenge: Learning Without Retraining\n\nModern LLM agents face a fundamental constraint: they can't continuously improve from real-world experience without expensive retraining cycles. When deployed, they encounter edge cases, domain-specific patterns, and novel scenarios that expose gaps in their base capabilities. Current approaches either ignore this drift or require periodic fine-tuning, both suboptimal for production systems.\n\nTwo recent frameworks address this challenge from complementary angles: **ReasoningBank with Memory-aware Test-Time Scaling (MaTTS)** learns from structured reasoning patterns, while **Agentic Context Engineering (ACE)** incrementally refines operational context. This post synthesizes these frameworks and demonstrates how their integration creates a powerful architecture for continuous learning systems.\n\n## Two Complementary Frameworks\n\n### ReasoningBank + MaTTS: Learning from Structured Experience\n\nReasoningBank transforms agent trajectories into structured memory itemsâ€”JSON objects containing titles, descriptions, and reasoning patterns. Rather than storing raw trajectories, it distills them into retrievable strategies:\n\n```json\n{\n  \"title\": \"Navigate nested menu via hover interaction\",\n  \"description\": \"Hover over parent menu to reveal submenu options\",\n  \"content\": \"When target is in nested menu: (1) identify parent, (2) hover to trigger submenu, (3) click target\",\n  \"embedding\": [...]\n}\n```\n\n**Memory-aware Test-Time Scaling (MaTTS)** amplifies this through two modes:\n- **Parallel scaling**: Generate k diverse trajectories (k=3 to k=5), use self-contrast to identify robust patterns, select best via LLM-as-judge\n- **Sequential scaling**: Iteratively refine single trajectory over k passes\n\nRetrieval uses cosine similarity on embedded queries. Retrieved memories augment the LLM prompt as additional system instructions, creating a feedback loop: better experiences â†’ better memories â†’ better retrieval â†’ better decisions.\n\n**Results**: +8.3% success rate on WebArena, +7.2% on Mind2Web, 34.2% relative improvement via MaTTS scaling, 16% fewer interaction steps on SWE-Bench-Verified.\n\n### Agentic Context Engineering: Incremental Adaptation\n\nACE maintains context as bullet-point heuristics, updating incrementally rather than rewriting completely. A three-stage pipeline refines context:\n\n1. **Generator**: Proposes 5-10 delta context bullets based on execution feedback\n2. **Reflector**: Iteratively refines over 3-5 epochs (clarity â†’ actionability â†’ conciseness)\n3. **Curator**: Merges refined deltas with semantic de-duplication (cosine similarity > 0.85)\n\nThe key insight: delta updates (Î”C\u003Csub>t\u003C/sub>) are dramatically cheaper than full rewrites. Context evolves as C\u003Csub>t+1\u003C/sub> = C\u003Csub>t\u003C/sub> + Î”C\u003Csub>t\u003C/sub>, maintaining compactness through embedding-based pruning.\n\n**Results**: +17.1% accuracy on agent benchmarks, 86.9% latency reduction in context adaptation, 83.6% reduction in token costs.\n\n## Real-World Applications\n\n### ReasoningBank + MaTTS: Pattern-Heavy Domains\n\n**Web navigation (WebArena, Mind2Web)**:\n- Repeated UI patterns: nested menus, multi-step forms, pagination\n- ReasoningBank captures interaction strategies (\"hover parent menu to reveal submenu\")\n- MaTTS parallel mode (k=3) discovers robust navigation across layout variations\n- After 50 interactions: 30+ reusable DOM traversal patterns\n\n**Code repository tasks (SWE-Bench)**:\n- Recurring patterns: type mismatches, import errors, test failures\n- ReasoningBank stores diagnostic strategies (\"check function signature compatibility\")\n- Sequential MaTTS (k=5) refines multi-file reasoning\n- Example memory: \"verify dependency injection before debugging runtime errors\"\n\n### ACE Alone: Incremental Domain Learning\n\n**Project-specific coding assistants**: Learn codebase conventions during development sessions.\n- Accumulates rules: \"Use CustomError base class,\" \"Database queries in repositories/\"\n- Delta updates add conventions without regenerating existing rules\n- Reflection refines clarity: \"Use CustomError\" â†’ \"Extend CustomError base class for domain exceptions\"\n- Semantic de-duplication prevents redundant rules\n- 86.9% latency reduction critical for frequent context updates\n\n**Generalizes to**: Customer support (product policies), content moderation (guideline updates), personal assistants (user preferences), any scenario requiring incremental knowledge accumulation with low-latency evolution.\n\n### Unified Framework: Production Automation\n\n**Enterprise SaaS automation (Salesforce, HubSpot)**:\n- ReasoningBank: UI interaction patterns (click sequences, field navigation)\n- ACE: Client-specific rules (\"Client X requires 'Enterprise' tag on leads >$50k\")\n- MaTTS parallel (k=3): Handles platform updates and custom layouts via multi-trajectory exploration\n- Synergy: Memories provide \"how to navigate,\" context provides \"what rules apply per client\"\n\n## Unified Architecture: Closed-Loop Learning\n\nIntegrating these frameworks creates synergistic learning loops: \u003Cbr/>\n**1. Memory-Informed Context**: Retrieved reasoning patterns from ReasoningBank guide ACE's delta context generation \u003Cbr/>\n**2. Context-Guided Trajectories**: Evolved context shapes MaTTS trajectory exploration \u003Cbr/>\n**3. Trajectory-Enriched Memory**: New experiences expand ReasoningBank continuously \u003Cbr/>\n**4. Feedback-Driven Refinement**: Execution outcomes refine both memory and context \u003Cbr/>\n\n### System Architecture\n\n```mermaid\n%%{init: {\n  'theme': 'default',\n  'flowchart': {\n    'htmlLabels': false,\n    'wrap': true\n  },\n  'themeVariables': {\n    'primaryColor': '#E8F4F8',\n    'primaryTextColor': '#1a1a1a',\n    'primaryBorderColor': '#4A90A4',\n    'secondaryColor': '#F0E8F4',\n    'secondaryTextColor': '#1a1a1a',\n    'secondaryBorderColor': '#8B7AA8',\n    'tertiaryColor': '#F4F0E8',\n    'tertiaryTextColor': '#1a1a1a',\n    'tertiaryBorderColor': '#A89B7A',\n    'noteBkgColor': '#E8F8E8',\n    'noteTextColor': '#1a1a1a',\n    'noteBorderColor': '#6FA86F'\n  }\n}}%%\nflowchart TD\n    Start[\"Query q\"]:::inputStyle --> Embed[\"Embed Query\\n(embedding_model)\"]:::memoryStyle\n    Embed --> Retrieve[\"Retrieve Top-K Memories\\n(cosine similarity)\"]:::memoryStyle\n\n    Retrieve --> Generator[\"Generator\\nGenerate Î” Context Candidates\\n(from query + memories + context)\"]:::aceStyle\n    Generator --> Reflector[\"Reflector\\nRefine Î” Context\\n(N reflection epochs)\"]:::aceStyle\n    Reflector --> Curator[\"Curator\\nSynthesize & Deduplicate\\n(semantic merge)\"]:::aceStyle\n\n    Curator --> Combine[\"Combine Contexts\\n(base + memories + curated Î”)\"]:::combineStyle\n\n    Combine --> ScalingMode{\"Scaling Mode?\"}:::decisionStyle\n\n    ScalingMode -->|Parallel| ParallelGen[\"Generate N Trajectories\\n(parallel sampling)\"]:::mattsStyle\n    ParallelGen --> SelfContrast[\"Self-Contrast Filter\\n(LLM-as-Judge scoring)\"]:::mattsStyle\n    SelfContrast --> BestTraj[\"Select Best Trajectory\"]:::mattsStyle\n\n    ScalingMode -->|Sequential| SeqGen[\"Generate Single Trajectory\"]:::mattsStyle\n    SeqGen --> SelfRefine[\"Self-Refine\\n(N refinement passes)\"]:::mattsStyle\n    SelfRefine --> BestTraj\n\n    BestTraj --> Execute[\"Execute Trajectory\\n(return response)\"]:::executeStyle\n    Execute --> ExtractMem[\"Extract New Memories\\n(LLM extraction pipeline)\"]:::extractStyle\n    ExtractMem --> UpdatePool[\"Update Memory Pool\\n(additive, no pruning)\"]:::memoryStyle\n\n    UpdatePool -.->|Next Query: \u003Cbr/>Use Updated Memory Pool| Retrieve\n    Curator -.->|Next Query: \u003Cbr/>Use Updated Context| Start\n\n    classDef inputStyle fill:#BBE5ED,stroke:#2E86AB,stroke-width:2px,color:#1a1a1a\n    classDef memoryStyle fill:#C5DBF7,stroke:#4A7BA7,stroke-width:2px,color:#1a1a1a\n    classDef aceStyle fill:#E4C9F5,stroke:#9B59B6,stroke-width:2px,color:#1a1a1a\n    classDef combineStyle fill:#BEF0CB,stroke:#27AE60,stroke-width:2px,color:#1a1a1a\n    classDef decisionStyle fill:#FFD6A5,stroke:#F39C12,stroke-width:2px,color:#1a1a1a\n    classDef mattsStyle fill:#FFCCCB,stroke:#E74C3C,stroke-width:2px,color:#1a1a1a\n    classDef executeStyle fill:#B2E6D4,stroke:#16A085,stroke-width:2px,color:#1a1a1a\n    classDef extractStyle fill:#F5C6E8,stroke:#D35D9A,stroke-width:2px,color:#1a1a1a\n```\n\n## Key Implementation Insights\n\n### 1. Memory Quality Over Quantity\n\nStructure matters. ReasoningBank's JSON format (title, description, content, embedding) enables precise retrieval. Store reasoning patterns, not raw trajectories.\n\n```python\n# Good: Structured extraction\nmemory = {\n    'title': 'Navigate nested menu via hover',\n    'description': 'Hover parent to reveal submenu',\n    'content': '(1) identify parent, (2) hover, (3) click target',\n    'embedding': embed(content)\n}\n```\n\n### 2. Incremental Beats Rewrite\n\nACE's delta updates reduced latency by **86.9%** and costs by **83.6%**. Generate 5-10 new bullets, not 50+ complete rewrites. Use semantic de-duplication (cosine similarity > 0.85) to prevent bloat.\n\n### 3. Multi-Trajectory Scaling Works\n\nMaTTS parallel mode (k=3 to k=5) delivered **34.2% relative improvement**. Self-contrast filtering identifies robust patterns across diverse attempts. For production: k=3 for routine tasks, k=5 for critical ones.\n\n### 4. Learn from Failures\n\nLabel and store failed trajectories: \"Failed approach (avoid): \\{reasoning\\}\". This negative learning prevents repeated mistakes and accelerates convergence.\n\n### 5. Embedding Cache Is Critical\n\nPre-compute and cache embeddings for both memory retrieval and context de-duplication. This single optimization enables production latency for both frameworks.\n\n## Production Considerations\n\n**Scaling Trade-offs:**\n- k=1: Baseline, no additional compute\n- k=3: 2x improvement, 3x compute (recommended for production)\n- k=5: 2.5x improvement, 5x compute (critical tasks only)\n\n**Memory Management:**\n- Format: JSON with pre-computed embeddings\n- Storage: Append-only (isolates quality impact)\n- Retrieval: Top-k=5 cosine similarity\n- Future: Implement intelligent pruning for unbounded growth\n\n**Context Management:**\n- Representation: Bullet points for interpretability\n- Updates: Incremental deltas with semantic merge\n- Size: Bounded by curator when exceeding limits\n- Epochs: 3 reflection passes (balance quality vs. latency)\n\n**Model Configuration:**\n- LLMs: Gemini-2.5, Claude-3.7, DeepSeek-V3.1\n- Embeddings: gemini-embedding-001\n- Temperature: 0.7 for generation, 0.0 for judging\n\n## Conclusion\n\nThe synthesis of ReasoningBank and ACE demonstrates a new paradigm for production LLM agents: continuous learning without retraining. By combining structured memory distillation with incremental context adaptation, systems can:\n\n- Learn continuously without manual labels via LLM-as-judge evaluation\n- Adapt efficiently through delta updates (86.9% latency reduction)\n- Scale intelligently via multi-trajectory exploration (34.2% improvement)\n- Maintain interpretability through structured memory and bullet-point contexts\n\nFor practitioners, this integrated approach offers concrete blueprintsâ€”complete with implementation specs, hyperparameters, and trade-off analysisâ€”for building agents that evolve with experience. The key shift: emphasize memory quality over quantity, incremental updates over rewrites, and closed-loop learning over manual curation.\n\nAs LLM agents transition from research prototypes to production systems, frameworks like these provide the architectural foundation for continuous adaptation, efficient scaling, and robust performance gains across diverse domains.\n\n---\n\n## Key Takeaways\n\n- **ReasoningBank + Memory-aware Test-Time Scaling (MaTTS)** distills structured reasoning patterns from agent trajectories, improving success rates by up to 8.3% through parallel trajectory generation (k=3 to k=5)\n- **Agentic Context Engineering (ACE)** incrementally adapts context via delta updates, reducing adaptation latency by 86.9% and improving accuracy by 17.1%\n- **Combined architecture** creates closed-loop learning: retrieved memories inform context updates, evolved contexts guide trajectory generation, trajectories enrich both memory and context\n- **Production-ready**: Full implementation specs provided below for rapid POC development without retraining infrastructure\n\n---\n\n## Implementation Resources\n\nComplete implementation specifications are provided below. Copy these to your coding agent for rapid POC development:\n\n\u003Cdetails>\n\u003Csummary>\u003Cstrong>ACE Implementation Spec\u003C/strong> - Incremental context adaptation pipeline\u003C/summary>\n\n```markdown\n# Agentic Context Engineering (ACE) Implementation Specification\n\n## Overview\n\nImplement an incremental context adaptation system that maintains an evolving context of domain-specific heuristics represented as bullet points. The framework uses a three-stage pipeline (Generator, Reflector, Curator) to produce compact, high-quality context updates.\n\n## Core Objectives\n\n1. Maintain compact, interpretable context as itemized bullet points\n2. Generate incremental delta context updates rather than full rewrites\n3. Iteratively refine context updates through multi-epoch reflection\n4. Merge updates with semantic de-duplication to prevent redundancy\n5. Achieve low-latency, low-cost context adaptation\n\n## Implementation Instructions\n\n### Context Representation\n\nModel the context as a set of itemized bullet points representing domain-specific heuristics and strategies. The context should be:\n- **Interpretable**: Clear, actionable guidance\n- **Compact**: Bounded by token limits\n- **Incremental**: Updated via delta additions, not full rewrites\n\n### Three-Stage Pipeline\n\n#### 1. Generator\n**Purpose**: Propose candidate delta context updates based on current context and execution feedback\n\n**Input**:\n- Current context C_t\n- Task query q\n- Retrieved memories (if integrated with ReasoningBank)\n- Execution feedback E_t (optional, includes environment signals and task outcomes)\n\n**Output**:\n- Candidate delta context Î” C_t (5-10 bullet points)\n\n**Implementation Notes**:\n- Generate compact knowledge bullets, not complete context rewrites\n- Focus on new insights from current task/feedback\n- Each bullet should be a specific, actionable heuristic\n\n#### 2. Reflector\n**Purpose**: Iteratively critique and refine delta context candidates\n\n**Input**:\n- Candidate delta context from Generator\n- Task query q\n- Execution feedback (optional)\n\n**Output**:\n- Refined delta context after N reflection epochs\n\n**Process**:\n- Epoch 1: Generate initial delta from raw feedback\n- Epoch 2: Critique for clarity and actionability\n- Epoch 3: Refine for non-redundancy and conciseness\n- Continue for NUM_REFLECTION_EPOCHS total\n\n**Implementation Notes**:\n- Perform multi-pass refinement to improve quality\n- Focus on error diagnosis and credit assignment\n- Ensure bullets are clear, concise, and non-redundant\n\n#### 3. Curator\n**Purpose**: Merge refined delta contexts with existing context and remove redundancies\n\n**Input**:\n- Existing context C_t\n- Refined delta context from Reflector\n\n**Output**:\n- Updated context C\u003Csub>t+1\u003C/sub>\n\n**Process**:\n1. Merge refined delta context with existing context\n2. Perform semantic embedding-based de-duplication\n3. Remove bullets with cosine similarity > threshold (0.85)\n4. Prune low-value items if context exceeds size limits\n\n**Implementation Notes**:\n- Use pre-computed embeddings for efficiency\n- Maintain context compactness to prevent bloat\n- Preserve high-value, non-redundant items\n\n### Execution Loop\n\nFor each task:\n1. Generate candidate delta contexts using current context and task\n2. Refine candidates through multi-epoch reflection\n3. Curate and merge refined updates into existing context\n4. Use updated context as system prompt for LLM inference\n5. Collect execution feedback for next iteration\n\n## Pseudocode Reference\n\n# Initialize empty context C as a set of knowledge bullets\nContext = set()\n\n# For each new task/query q:\nfor q in TaskQueries:\n\n    # Step 1: Generator produces candidate context updates (knowledge bullets)\n    candidate_updates = Generator.generate(Context, q)\n\n    # Step 2: Reflector iteratively refines candidate updates over multiple epochs\n    max_epochs = 5\n    for epoch in range(max_epochs):\n        candidate_updates = Reflector.refine(candidate_updates, execution_feedback=None)\n\n    # Step 3: Curator merges refined updates into existing context\n    Context = Curator.merge(Context, candidate_updates)\n\n    # Step 4: Curator performs semantic embedding-based de-duplication to prune redundancy\n    Context = Curator.de_duplicate(Context)\n\n    # Step 5: Use updated context C as system prompt for LLM inference on task q\n    output = LLM.infer(q, context=Context)\n\n    # Step 6: Collect execution feedback (success/failure signals, environment observations)\n    execution_feedback = CollectFeedback(output, environment)\n\n    # Optionally, use feedback to guide next iteration's reflection and curation\n\n## Hyperparameters\n\n### Model Configuration\n- **Embedding Model**: gemini-embedding-001 for semantic similarity\n- **LLM Models**: Gemini-2.5 variants, Claude-3.7, or DeepSeek-V3.1 via Vertex AI API\n- **Generation Temperature**: Context-dependent (typically 0.7-1.0)\n\n### Pipeline Configuration\n- **NUM_REFLECTION_EPOCHS**: 3 to 5 (typically 3 for balance)\n  - Epoch 1: Initial generation from feedback\n  - Epoch 2: Critique for clarity and actionability\n  - Epoch 3+: Refine for non-redundancy and conciseness\n- **Delta Context Size**: 5-10 bullet points per update\n- **De-duplication Threshold**: Cosine similarity > 0.85 to identify redundant bullets\n\n### Context Management\n- **Representation**: Itemized bullet points\n- **Update Strategy**: Incremental delta contexts, not full rewrites\n- **Size Management**: Bounded by token limits; curator removes low-value items when needed\n- **Storage**: Maintain as set or list of bullet strings with cached embeddings\n\n### Optimization Settings\n- **KV Cache Reuse**: Maintain cache across delta context updates\n- **Embedding Caching**: Pre-compute and store embeddings for context items\n- **Lazy Evaluation**: Only compute embeddings when needed for de-duplication\n\n## Key Implementation Notes\n\n1. **Incremental Updates Reduce Latency**: Delta contexts (5-10 bullets) vs. full rewrites (50+ bullets)\n2. **Multi-Epoch Refinement**: 3 epochs significantly improves context quality\n3. **Semantic De-duplication Critical**: Prevents context bloat and redundancy\n4. **Embedding Cache Essential**: Pre-compute and cache for production latency\n5. **Compact Context**: Maintain interpretability and stay within token limits\n6. **Feedback Integration**: Use execution signals to guide refinement (optional but recommended)\n\n## Practical Trade-offs\n\n### Context Update Frequency\n- **High-frequency updates**: Better adaptation, higher compute cost\n- **Low-frequency updates**: Lower cost, slower adaptation\n- **Recommendation**: Update after each task for rapid learning, batch for cost optimization\n\n### Reflection Epochs\n- **1 epoch**: Fast but lower quality\n- **3 epochs**: Good balance (recommended)\n- **5+ epochs**: Diminishing returns, higher latency\n\n### Delta Context Size\n- **5 bullets**: Fast, focused updates\n- **10 bullets**: More comprehensive, slightly higher cost\n- **15+ bullets**: Approaching full rewrite territory, loses benefits\n```\n\n\u003C/details>\n\n\u003Cdetails>\n\u003Csummary>\u003Cstrong>ReasoningBank + MaTTS Implementation Spec\u003C/strong> - Structured memory distillation with test-time scaling\u003C/summary>\n\n```markdown\n# ReasoningBank + MaTTS Implementation Specification\n\n## Overview\n\nImplement a structured memory system that captures reasoning patterns from both successful and failed agent trajectories. The system uses Memory-aware Test-Time Scaling (MaTTS) to generate richer experiences through parallel or sequential trajectory generation.\n\n## Core Objectives\n\n1. Build a growing repository of reusable reasoning strategies\n2. Enable memory retrieval via semantic similarity\n3. Support both parallel and sequential scaling modes for test-time trajectory generation\n4. Implement closed-loop self-supervised learning using LLM-as-judge\n5. Extract and store structured memory items from trajectories\n\n## Implementation Instructions\n\n### Memory Representation\n\nCreate a structured memory system where each memory item is stored as JSON with the following schema:\n\n{\n  \"title\": \"string\",\n  \"description\": \"string\",\n  \"content\": \"string\",\n  \"embedding\": \"vector\"\n}\n\n**Good Example:**\n{\n  \"title\": \"Navigate to nested menu via hover interaction\",\n  \"description\": \"Hover over parent menu to reveal submenu options\",\n  \"content\": \"When target element is in nested menu: (1) identify parent menu, (2) hover to trigger submenu, (3) click target\",\n  \"embedding\": [0.123, 0.456, ...]\n}\n\n**Bad Example:**\nStore entire trajectory text without structure.\n\n### Memory Retrieval\n\nImplement retrieval using cosine similarity:\n- Embed the query using the embedding model\n- Compute cosine similarity between query embedding and all memory embeddings\n- Return top-k most similar memory items\n- Inject retrieved memories as additional system instructions to the LLM\n\n### MaTTS Implementation\n\nImplement two scaling modes:\n\n**Parallel Scaling:**\n- Generate k diverse trajectories simultaneously\n- Use LLM-as-judge to label each trajectory as Success or Failure (temperature=0.0)\n- Apply self-contrast reasoning to identify consistent patterns across trajectories\n- Filter out spurious or low-quality trajectories\n- Select best trajectory using best-of-n selection\n\n**Sequential Scaling:**\n- Initialize a single trajectory\n- Iteratively generate or refine reasoning steps over k passes\n- Each pass is conditioned on previous steps and retrieved memory\n- Optionally update memory with intermediate reasoning signals\n- Label final trajectory as Success or Failure\n\n### Memory Extraction and Update\n\nAfter trajectory execution:\n1. Use LLM to extract structured memory items from the trajectory\n2. Include both successful and failed trajectories (label them appropriately)\n3. For failures, format as: \"Failed approach (avoid): \\{reasoning\\}\"\n4. For successes, format as: \"Successful approach: \\{reasoning\\}\"\n5. Append new memory items to the memory bank (additive, no pruning)\n6. Pre-compute and cache embeddings for new memories\n\n### Agent Policy\n\nThe LLM agent policy should be conditioned on:\n- Current observation\n- Past trajectory\n- Retrieved memory (injected as system instructions)\n\nConcatenate these components to form the full prompt for the LLM.\n\n## Pseudocode Reference\n\n# Initialize empty ReasoningBank memory repository\nMemoryRepository = []\n\n# For each incoming task query q:\nfor q in TaskQueries:\n\n    # Step 1: Memory Retrieval\n    # Embed the current query using gemini-embedding-001\n    query_embedding = Embed(q, model=\"gemini-embedding-001\")\n\n    # Retrieve top-k relevant memory items by cosine similarity\n    relevant_memories = RetrieveTopK(MemoryRepository, query_embedding, k=1)\n\n    # Step 2: Build agent context by injecting retrieved memories as system instructions\n    context = BuildContext(q, relevant_memories)\n\n    # Step 3: Memory-aware Test-Time Scaling (MaTTS)\n    # Choose scaling mode: parallel or sequential\n    if scaling_mode == \"parallel\":\n        # Generate k independent trajectories in parallel\n        trajectories = []\n        for i in range(k):\n            traj = AgentRun(context, temperature=0.7)\n            trajectories.append(traj)\n\n        # Label each trajectory as Success or Failure using LLM-as-a-judge (temperature=0.0)\n        labeled_trajectories = []\n        for traj in trajectories:\n            label = JudgeTrajectory(traj, temperature=0.0)\n            labeled_trajectories.append((traj, label))\n\n        # Perform self-contrast reasoning to identify consistent reasoning patterns\n        consistent_patterns = SelfContrast(labeled_trajectories)\n\n        # Filter out spurious or low-quality trajectories\n        filtered_trajectories = FilterTrajectories(consistent_patterns)\n\n    elif scaling_mode == \"sequential\":\n        # Initialize a single trajectory\n        traj = []\n        for iteration in range(k):\n            # Generate or refine reasoning step conditioned on previous steps and memory\n            step = AgentGenerateStep(context, previous_steps=traj)\n            traj.append(step)\n\n            # Optionally update memory with intermediate reasoning signals\n            UpdateMemoryWithIntermediateSteps(MemoryRepository, traj)\n\n        # Label final trajectory\n        label = JudgeTrajectory(traj, temperature=0.0)\n        filtered_trajectories = [(traj, label)]\n\n    # Step 4: Extract structured memory items from filtered trajectories\n    new_memory_items = []\n    for traj, label in filtered_trajectories:\n        items = ExtractMemoryItems(traj, label, temperature=1.0)\n        new_memory_items.extend(items)\n\n    # Step 5: Consolidate new memory items into ReasoningBank repository (additive)\n    MemoryRepository.extend(new_memory_items)\n\n    # Step 6: Use LLM to select best trajectory answer (best-of-n selection)\n    best_answer = SelectBestAnswer(filtered_trajectories)\n\n    # Step 7: Output final answer\n    Output(best_answer)\n\n## Hyperparameters\n\n### Model Configuration\n- **Embedding Model**: gemini-embedding-001\n- **LLM Models**: Gemini-2.5 variants, Claude-3.7, or DeepSeek-V3.1 via Vertex AI API\n- **Trajectory Generation Temperature**: 0.7 (exploration-exploitation balance)\n- **Judge Temperature**: 0.0 (deterministic classification)\n- **Memory Extraction Temperature**: 1.0\n\n### Scaling Configuration\n- **TOP_K_MEMORIES**: 5 (number of memories to retrieve)\n- **SCALING_MODE**: 'parallel' or 'sequential'\n- **NUM_TRAJECTORIES**:\n  - k=1: Baseline (no additional compute)\n  - k=3: Good balance (~2x improvement with 3x compute) - **Recommended for production**\n  - k=5: Maximum tested (~2.5x improvement with 5x compute) - **For critical tasks**\n- **NUM_REFINEMENT_PASSES**: 5 (for sequential scaling mode)\n\n### Memory Management\n- **Storage Strategy**: Append-only without pruning\n- **Retrieval Method**: Top-k cosine similarity search\n- **Format**: JSON with fields \\{title, description, content\\} plus pre-computed embeddings\n- **Indexing**: Pre-compute and cache embeddings for fast retrieval\n\n### Agent Architecture\n- **Framework**: ReAct (Reasoning + Acting) interleaved steps\n- **Trajectory Structure**: Sequence of (thought, action, observation) tuples\n- **Memory Injection**: Retrieved memories added as system instructions\n\n## Performance Expectations\n\nBased on benchmark results:\n- **WebArena**: +8.3% success rate improvement\n- **Mind2Web**: +7.2% improvement in element accuracy and action F1\n- **SWE-Bench-Verified**: +4.6% improvement with 16% fewer interaction steps\n- **MaTTS Relative Improvement**: 34.2% over baseline\n\n## Key Implementation Notes\n\n1. **Memory Quality Over Quantity**: Focus on structured extraction rather than raw storage\n2. **Learn from Failures**: Explicitly include failed trajectories with labels\n3. **Embedding Cache**: Pre-compute and cache embeddings to reduce latency\n4. **Deterministic Judging**: Use temperature=0.0 for reproducible success/failure labels\n5. **Parallel Scaling Advantage**: Parallel mode outperforms sequential at larger k values\n6. **Cost-Performance Trade-off**: Start with k=3, scale to k=5 only for critical tasks\n```\n\n\u003C/details>\n\n\u003Cdetails>\n\u003Csummary>\u003Cstrong>Unified Framework Implementation Spec\u003C/strong> - Combined ReasoningBank + ACE for closed-loop learning\u003C/summary>\n\n```markdown\n# Unified ReasoningBank + ACE Implementation Specification\n\n## Overview\n\nImplement a unified framework that combines ReasoningBank's structured memory distillation with ACE's incremental context adaptation. This integration creates a closed-loop learning system where memory-informed contexts guide trajectory generation, and trajectories enrich both memory and context.\n\n## Core Objectives\n\n1. Retrieve relevant reasoning patterns from memory bank to inform context updates\n2. Generate and refine incremental delta contexts using ACE's three-stage pipeline\n3. Combine base context, retrieved memories, and curated delta contexts into full agent prompt\n4. Generate multiple trajectories using MaTTS (parallel or sequential scaling)\n5. Extract structured memories from trajectories and update memory pool\n6. Create virtuous learning cycle: better memories â†’ better contexts â†’ better trajectories â†’ better memories\n\n## Implementation Instructions\n\n### System Components\n\n#### 1. Memory Pool (ReasoningBank)\n- Structured memory items in JSON format with embeddings\n- Supports cosine similarity retrieval\n- Additive updates (no pruning)\n- Pre-computed and cached embeddings\n\n#### 2. Context Manager (ACE)\n- Base prompt template\n- Delta context updates (bullet points)\n- Semantic de-duplication\n- Incremental merge operations\n\n#### 3. Pipeline Components\n- **Generator**: Creates delta contexts from query, memories, and current context\n- **Reflector**: Iteratively refines delta contexts\n- **Curator**: Merges and de-duplicates contexts\n- **Trajectory Generator**: Produces agent trajectories using combined prompts\n- **Memory Extractor**: Distills memories from trajectories\n\n### Unified Algorithm Flow\n\nFor each incoming query:\n\n1. **Memory Retrieval**\n   - Embed query using embedding model\n   - Retrieve top-k relevant memories by cosine similarity\n   - Format memories for injection into prompts\n\n2. **Context Evolution (ACE Pipeline)**\n   - Generator: Create delta context candidates using query, memories, and current context\n   - Reflector: Refine candidates over N reflection epochs\n   - Curator: Merge refined deltas with existing context and de-duplicate\n\n3. **Prompt Assembly**\n   - Combine base context + formatted memories + curated delta context\n   - Result is full context for agent trajectory generation\n\n4. **Trajectory Generation (MaTTS)**\n\n   **Parallel Mode:**\n   - Generate k trajectories in parallel using full context\n   - Label each trajectory (Success/Failure) using LLM-as-judge at temperature=0.0\n   - Apply self-contrast filtering to identify robust patterns\n   - Select best trajectory via best-of-n selection\n\n   **Sequential Mode:**\n   - Generate single trajectory\n   - Iteratively refine over k passes\n   - Each pass conditioned on previous steps and full context\n\n5. **Memory Extraction and Update**\n   - Extract structured memory items from best trajectory\n   - Include success/failure labels\n   - Compute embeddings for new memories\n   - Append to memory pool (additive)\n\n6. **Context Update for Next Iteration**\n   - Optionally update base context with curated delta\n   - Maintains evolved context for subsequent queries\n\n### Closed-Loop Learning\n\nThe system creates four synergistic learning loops:\n\n1. **Memory-Informed Context**: Retrieved reasoning patterns guide delta context generation\n2. **Context-Guided Trajectories**: Evolved context shapes multi-trajectory exploration\n3. **Trajectory-Enriched Memory**: New experiences continuously expand memory bank\n4. **Feedback-Driven Refinement**: Execution outcomes refine both memory and context\n\n## Pseudocode Reference\n\n# Initialization\nmemory_pool = load_reasoningbank_memory()  # Structured memory items with embeddings\ncontext = initial_prompt_template            # Base prompt template for the agent\nembedding_model = load_embedding_model()     # e.g., gemini-embedding-001\nbackbone_llm = load_backbone_llm()           # e.g., Gemini-2.5, Claude-3.7\n\n# Hyperparameters\nTOP_K_MEMORIES = 5\nNUM_TRAJECTORIES = 10\nNUM_REFLECTION_EPOCHS = 3\nSCALING_MODE = 'parallel'  # or 'sequential'\nNUM_REFINEMENT_PASSES = 5  # for sequential scaling\n\ndef embed(text):\n    return embedding_model.encode(text)\n\ndef retrieve_top_k(memory_pool, query_embedding, k=TOP_K_MEMORIES):\n    # Retrieve top-k memories by cosine similarity\n    return memory_pool.similarity_search(query_embedding, top_k=k)\n\ndef generator_generate(query, retrieved_memories, current_context):\n    # Generate candidate delta contexts (small knowledge bullets)\n    prompt = compose_generator_prompt(query, retrieved_memories, current_context)\n    delta_context_candidates = backbone_llm.generate(prompt)\n    return delta_context_candidates\n\ndef reflector_refine(delta_context_candidates, query, num_epochs=NUM_REFLECTION_EPOCHS):\n    # Iteratively critique and refine delta contexts\n    for _ in range(num_epochs):\n        reflection_prompt = compose_reflector_prompt(delta_context_candidates, query)\n        delta_context_candidates = backbone_llm.generate(reflection_prompt)\n    return delta_context_candidates\n\ndef curator_synthesize(delta_context_candidates, existing_context):\n    # Merge, prune, and de-duplicate delta contexts semantically\n    combined_context = merge_contexts(existing_context, delta_context_candidates)\n    curated_context = semantic_deduplication(combined_context)\n    return curated_context\n\ndef combine_contexts(base_context, retrieved_memories, curated_context):\n    # Combine original prompt, memories, and curated delta context\n    combined_prompt = base_context + format_memories(retrieved_memories) + curated_context\n    return combined_prompt\n\ndef generate_multiple_trajectories(prompt, num_trajectories=NUM_TRAJECTORIES):\n    trajectories = []\n    for _ in range(num_trajectories):\n        traj = backbone_llm.generate(prompt)\n        trajectories.append(traj)\n    return trajectories\n\ndef self_contrast_filter(trajectories):\n    # Use LLM-as-a-judge to score and filter trajectories\n    scores = []\n    judge_prompt = compose_judge_prompt(trajectories)\n    for traj in trajectories:\n        score = backbone_llm.evaluate(judge_prompt, traj)\n        scores.append(score)\n    best_idx = scores.index(max(scores))\n    return trajectories[best_idx]\n\ndef generate_single_trajectory(prompt):\n    return backbone_llm.generate(prompt)\n\ndef self_refine(trajectory, context):\n    # Iteratively refine a single trajectory using intermediate feedback\n    refinement_prompt = compose_refinement_prompt(trajectory, context)\n    refined_trajectory = backbone_llm.generate(refinement_prompt)\n    return refined_trajectory\n\ndef extract_memories(trajectory, query):\n    # Extract structured memory items from trajectory using LLM extraction pipeline\n    extraction_prompt = compose_memory_extraction_prompt(trajectory, query)\n    new_memories = backbone_llm.generate(extraction_prompt)\n    return parse_memory_items(new_memories)\n\ndef update_memory_storage(memory_pool, new_memories):\n    # Append new memories to memory pool without pruning\n    memory_pool.extend(new_memories)\n\ndef reasoning_pipeline(query_stream):\n    global context, memory_pool\n    for query in query_stream:\n        # Step 1: Retrieve relevant memories\n        query_embedding = embed(query)\n        retrieved_memories = retrieve_top_k(memory_pool, query_embedding)\n\n        # Step 2: Update context using ACE flow\n        delta_context_candidates = generator_generate(query, retrieved_memories, context)\n        refined_deltas = reflector_refine(delta_context_candidates, query)\n        curated_context = curator_synthesize(refined_deltas, context)\n\n        # Step 3: Combine contexts into full prompt\n        full_context = combine_contexts(context, retrieved_memories, curated_context)\n\n        # Step 4: Generate trajectories with MaTTS\n        if SCALING_MODE == 'parallel':\n            trajectories = generate_multiple_trajectories(full_context, NUM_TRAJECTORIES)\n            best_trajectory = self_contrast_filter(trajectories)\n        else:  # sequential scaling\n            trajectory = generate_single_trajectory(full_context)\n            for _ in range(NUM_REFINEMENT_PASSES):\n                trajectory = self_refine(trajectory, full_context)\n            best_trajectory = trajectory\n\n        # Step 5: Execute or return best trajectory\n        response = execute_trajectory(best_trajectory)\n\n        # Step 6: Extract new memories and update memory pool\n        new_memories = extract_memories(best_trajectory, query)\n        update_memory_storage(memory_pool, new_memories)\n\n        # Optionally update context for next query\n        context = curated_context\n\n        yield response\n\n## Hyperparameters\n\n### Model Configuration\n- **Embedding Model**: gemini-embedding-001\n- **LLM Models**: Gemini-2.5 variants, Claude-3.7, or DeepSeek-V3.1 via Vertex AI API\n- **Trajectory Generation Temperature**: 0.7 (exploration-exploitation balance)\n- **Judge Temperature**: 0.0 (deterministic classification)\n- **Memory Extraction Temperature**: 1.0\n\n### Memory Configuration (ReasoningBank)\n- **TOP_K_MEMORIES**: 5 (number of memories to retrieve per query)\n- **Memory Format**: JSON with \\{title, description, content\\} plus embeddings\n- **Storage Strategy**: Append-only without pruning\n- **Retrieval Method**: Top-k cosine similarity search\n- **Indexing**: Pre-compute and cache embeddings\n\n### Context Configuration (ACE)\n- **NUM_REFLECTION_EPOCHS**: 3 (iterative refinement passes)\n- **Delta Context Size**: 5-10 bullet points\n- **De-duplication Threshold**: Cosine similarity > 0.85\n- **Representation**: Itemized bullet points\n- **Update Strategy**: Incremental delta merging\n\n### Trajectory Scaling Configuration (MaTTS)\n- **SCALING_MODE**: 'parallel' or 'sequential'\n- **NUM_TRAJECTORIES**:\n  - k=1: Baseline (no scaling)\n  - k=3: Good balance (~2x improvement, 3x compute) - **Recommended for production**\n  - k=5: Maximum tested (~2.5x improvement, 5x compute) - **For critical tasks**\n  - k=10: Higher exploration (for research/testing)\n- **NUM_REFINEMENT_PASSES**: 5 (for sequential scaling mode)\n\n### Compute Optimization\n- **KV Cache Reuse**: Maintain cache across delta context updates\n- **Parallel Batch Generation**: Batch k trajectories in parallel mode\n- **Embedding Caching**: Pre-compute and store all embeddings\n- **Lazy Loading**: Load memories on-demand rather than full bank\n\n## Architecture Benefits\n\n### Multi-Scale Adaptation\n- **Episode-level**: ReasoningBank learns from complete trajectories\n- **Token-level**: ACE performs incremental context updates\n- **Multi-trajectory**: MaTTS explores diverse solution paths\n- **Multi-epoch**: ACE reflector refines context quality\n\n### Knowledge Synergy\n- **Concrete + Abstract**: Specific examples (memory) + general principles (context)\n- **Memory Quality**: Structured memories guide higher-quality context evolution\n- **Context Quality**: Evolved contexts shape better trajectory generation\n- **Feedback Loop**: Trajectories enrich both memory and context\n\n### Efficiency Gains\n- **Memory Cost**: Bounded context size despite growing memory pool\n- **Latency**: 86.9% reduction via delta updates + KV cache reuse\n- **Compute**: Intelligent scaling (k=3) balances cost and performance\n- **Storage**: Additive memory growth offset by compact context representation\n\n## Key Implementation Notes\n\n1. **Memory-Informed Context Generation**: Always pass retrieved memories to Generator\n2. **Dual Storage**: Maintain separate memory pool and context state\n3. **Embedding Cache**: Critical for both retrieval and de-duplication performance\n4. **Deterministic Judging**: Use temperature=0.0 for reproducible labels\n5. **Incremental Updates**: ACE delta contexts (5-10 bullets) for efficiency\n6. **Learn from Failures**: Include failed trajectories in memory with labels\n7. **Parallel Scaling**: Preferred for kâ‰¥3, better exploration than sequential\n8. **Multi-Epoch Reflection**: 3 epochs significantly improves context quality\n9. **Semantic De-duplication**: Prevents both context and memory redundancy\n10. **Closed-Loop Design**: Each component feeds into others for continuous improvement\n\n## Practical Trade-offs\n\n### Task Criticality\n- **Critical tasks**: k=5 parallel scaling, 5 reflection epochs, top-10 memories\n- **Standard tasks**: k=3 parallel scaling, 3 reflection epochs, top-5 memories\n- **Routine tasks**: k=1 (no scaling), 3 reflection epochs, top-3 memories\n\n### Resource Constraints\n- **High latency tolerance**: Use parallel scaling with higher k\n- **Low latency requirement**: Use sequential scaling or k=1\n- **Memory limited**: Reduce TOP_K_MEMORIES, implement memory pruning\n- **Compute limited**: Reduce NUM_TRAJECTORIES and NUM_REFLECTION_EPOCHS\n\n### Domain Characteristics\n- **Novel domains**: Higher k, more reflection epochs for exploration\n- **Familiar domains**: Lower k, fewer epochs, rely more on memory\n- **Dynamic environments**: More frequent context updates\n- **Stable environments**: Less frequent updates, accumulate more memories\n\n## Monitoring and Observability\n\nTrack these metrics for system health:\n\n1. **Memory Growth**: Monitor pool size and retrieval relevance over time\n2. **Context Drift**: Track context size and semantic coherence across updates\n3. **Performance Metrics**: Success rate, interaction steps, latency per trajectory\n4. **Quality Signals**: Self-contrast agreement rate, judge label consistency\n5. **Efficiency Metrics**: Token cost per task, latency breakdown by component\n6. **Learning Curves**: Performance improvement over time per domain\n```\n\n\u003C/details>\n\n---\n\n## References\n\n- Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models: https://arxiv.org/html/2510.04618v1\n- ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory: https://arxiv.org/html/2509.25140v1","src/content/blog/self-evolving-llm-agents.mdx","09cb70539e49cdd8","self-evolving-llm-agents.mdx","atomic-workflow",{"id":223,"data":225,"body":229,"filePath":230,"digest":231,"legacyId":232,"deferredRender":23},{"title":226,"date":227,"authors":187,"description":228},"Atomic: Automated Procedures and Memory for AI Coding Agents","2025-12-08","Building on AI Coding Infrastructure, Atomic introduces a research-to-execution flywheel where specifications become lasting memory. Here's what we learned scaling multi-agent workflows.","import { Image } from \"astro:assets\";\nimport InteractiveMermaid from \"../../components/blog/InteractiveMermaid.astro\";\nimport atomicLogo from \"../../assets/images/blog/atomic/atomic.png\";\nimport architectureSvg from \"../../assets/images/blog/atomic/architecture.svg\";\n\n## Building on AI Coding Infrastructure\n\n\u003Cdiv class=\"flex justify-center\">\n  \u003CImage src={atomicLogo} alt=\"Atomic logo\" width={256} height={256} />\n\u003C/div>\n\nIn our previous post on [AI Coding Infrastructure](/blog/ai-coding-infrastructure), we introduced a foundational layer for AI coding tools: skills, specialized subagents, ExecPlans, and autonomous execution with Ralph.\n\nBut we learned two critical lessons as we continued to scale it:\n\n**Lesson 1: Less is more.** Having 114+ subagents and dozens of skills sounds powerful, but in practice it created confusion. Agents didn't know which specialist to call. Skills overlapped and contradicted each other. We learned to curate a focused set of agents and skills that work cohesively in a single workflow.\n\n**Lesson 2: Specs were the main memory layer.** Each coding session produced valuable insights that evaporated when the context window reset. Research findings, architectural decisions, debugging discoveriesâ€”all lost. We needed a system where work compounds rather than restarts and that maintains active working memory to keep the agent on track.\n\n**Atomic** is that system. It's an automated procedure with focused agents, commands/prompts, and skills designed to work together. The procedure for Atomic follows Software Development Lifecycle best practices and enables engineers to effectively steer agents, providing alignment.\n\n## The Core Insight: Lasting Memory Through Specs & Active Memory Through Progress.txt\n\nIn Atomic, specifications live in `specs/` and survive across sessions. When a new session starts, agents read the existing specs to understand context, decisions, and progress. Research happens within a session, but the **spec is what persists**.\n\nThe workflow follows a clear progression:\n\n1. **Research** â†’ Multiple agents analyze the codebase in parallel\n2. **Spec Creation** â†’ Synthesize research into a specification (this is what persists)\n3. **Feature Decomposition** â†’ Break the spec into discrete, implementable features (provides agent with actual status so it doesn't erroneously declare a feature complete)\n4. **Implementation** â†’ Build each feature with tests and validation (this is where active memory is tracked through progress.txt)\n5. **Pull Request** â†’ Package changes for review\n\nThe features and progress.txt are what maintain active memory during implementation. The spec is the source of truth that carries context forward. Future sessions read existing specs to understand decisions, progress, and architectural choices.\n\n## Architecture: Three Primitives\n\nAtomic operates through three interconnected primitives:\n\n| Primitive | Role | Examples |\n|-----------|------|---------|\n| **Commands** | Orchestrate workflows | `/research-codebase`, `/creat-spec`, `/create-feature-list`, `/implement-feature`, `/compact`, `/commit`, `/create-debug-report`, `create-pr`, `explain-code` |\n| **subagents** | Execute specialized tasks | codebase-analyzer, codebase-locator, codebase-online-researcher, codebase-pattern-finder, codebase-research-analyzer, codebase-research-locater |\n| **Skills** | Inject domain knowledge | prompt-engineer, testing-anti-patterns |\n| **MCP Servers** | Research online and debug | playwright, deepwiki |\n\n**Nine commands** automate our procedure, leveraging **six subagents**. **Two skills** available for prompt enhancement and testing best practices. **Playwright** is leveraged for debugging and documentation look up where we've seen Search APIs and webFetch fall short. While, **deepwiki** is leveraged for critical information on open source code relevant to the project being developed. To be able to effectively steer the agents, engineer is always in the loop for every step as a core principle.\n\n## The Research-to-Execution Flywheel\n\nHere's the complete workflow. Notice how research outputs feed into specs, specs decompose into features, and implementation failures loop back through debugging, all while accumulating documentation as memory.\n\n\u003CInteractiveMermaid height=\"700px\" maxZoom={4}>\n  \u003CImage src={architectureSvg} alt=\"Atomic workflow architecture diagram showing the research-to-execution flywheel\" />\n\u003C/InteractiveMermaid>\n\n### Phase 1: Parallel Research\n\nSix agents work simultaneously:\n\n| Agent | Output | Persists To |\n|-------|--------|-------------|\n| **Pattern Finder** | Structural patterns to follow | `research/notes/` |\n| **Locator** | Relevant code sections | `research/tickets/` |\n| **Analyzer** | Patterns & architecture | `research/docs/` |\n| **Research Locator** | Deeper code location analysis | `research/tickets/` |\n| **Research Analyzer** | Extended analysis findings | `research/docs/` |\n| **Online Researcher** | External docs & best practices | `research/docs/` |\n\n**Why parallel?** Research tasks are independent. One agent finding authentication patterns doesn't block another analyzing database schemas. Review the spec and make sure it's not missing any key information.\n\n### Phase 2: Specification\n\nThe create-spec command synthesizes all research into a single specification. Critically, this spec references the research documents so anyone (engineer or agent) reading the spec can trace decisions back to their source.\n\nSpecs include: problem statement, proposed solution, architectural decisions, acceptance criteria, and links to relevant research files. Review the spec and make sure it is in line with what you're looking to implement.\n\n### Phase 3: Feature Decomposition\n\nThe create-feature-list command breaks specs into discrete features, each with:\n```json\n{\n    \"category\": \"functional\",\n    \"description\": \"New chat button creates a fresh conversation\",\n    \"steps\": [\n      \"Navigate to main interface\",\n      \"Click the 'New Chat' button\",\n      \"Verify a new conversation is created\",\n      \"Check that chat area shows welcome state\",\n      \"Verify conversation appears in sidebar\"\n    ],\n    \"passes\": false\n}\n```\n\n\u003Cbr>\u003C/br>\n\n**Learning**: Features must be small enough to implement in one session. If a feature requires multiple context switches, it's too big, decompose further. Adjust depending on model you're using and whether or not task falls within common training distribution set of models. Review all features and make any adjustments as necessary before proceeding.\n\n### Phase 4: Atomic Implementation\n\nFeatures implement **one at a time**, each following this loop:\n\n1. Implement the change\n2. Create tests\n3. Validate all tests pass (including previous features)\n4. Review all code and make any adjustments as necessary\n5. Mark complete & commit\n\n**Why atomic?** Parallel implementation creates merge conflicts and tangled dependencies making it harder to debug depending on the features. Additionally, as your codebase grows, it becomes more difficult for coding agents to follow instructions and make correct changes. Atomic implementation with validation gates catches issues immediately and prevents cascading failures. We only recommend parallel implementation when you're sure that you're working on areas that do not conflict with each other. In the future, as models and scaffolding/harnesses around these agents improve, we anticipate being able to increase the scope and greater parallelization for features.\n\nIn case of bugs, the create-debug-report command handles failures, producing diagnostic reports that naturally feed back into the implementation loop.\n\n### Phase 5: Quality Gates\n\nDeterministic hooks enforce standards on every commit:\n- Pre-commit validation\n- Linting and formatting\n- Full test suite\n\nThese aren't optional. Broken code doesn't progress.\n\n## What We Learned\n\n### 1. Agents Deliver 40-60%, Not 100%\n\nSet expectations correctly. Atomic gets you to 40-60% completion on complex features. The remaining work requires human judgment: edge cases, performance optimization, integration testing, architectural refinement.\n\nThis isn't a failure, it's the right division of labor and realistic for today's systems. Agents handle the mechanical work while you handle the judgment calls and code review.\n\n### 2. Research Quality Determines Everything\n\nGarbage research produces garbage specs which produce garbage implementations. We spent significant time refining the Research Agents because their output quality cascades through the entire pipeline.\n\n### 3. Feature Size Matters More Than You Think\n\nInitially we let the Feature List Agent produce features of any size. Large features failed constantly they exceeded context windows, created merge conflicts, made it harder to review, and led to more cascading failures.\n\nThe fix: enforce small features. If implementing a feature requires more than one session, decompose it. This constraint improved completion rates dramatically. This is something you'll need to test and get a feel for through trial and error and is also dependent on the model you're using.\n\n### 4. Built-In Debug Loops Within Active Memory\n\nAs engineers are developing, they naturally run into bugs. During implementation, if the agent runs into a bug, this is added back into the feature list to be addressed in the next implementation loop. This mirrors just how an engineer works and resolves bugs as they go along with their implementation plan.\n### 5. Documentation Compounds\n\nThe `specs/` directories become genuinely valuable. New features reference old research. Debugging sessions read past notes. The system gets smarter over time.\n\nThis is the real payoff: lasting memory that compounds.\n\n## You Own Decisions, Agents Own Execution\n\nAtomic doesn't replace engineering judgment. It amplifies it.\n\n- **You** approve specifications before implementation begins\n- **You** review the feature list decomposition\n- **You** decide when to accept agent output vs. iterate\n\nAgents handle the mechanical work: researching codebases, writing boilerplate, running tests, creating PRs. They're excellent at execution. They're not (yet) excellent at decisions.\n\nThat division is the key to making this work.\n\n## Getting Started\n\n**Repository**: [github.com/flora131/atomic](https://github.com/flora131/atomic/)\n\n## Key Takeaways\n\n- **Specs as lasting memory**: Specifications aren't throwaway promptsâ€” they persist across sessions and inform all future work\n- **Three primitives**: Commands orchestrate, Agents execute specialized tasks, Skills improve how tasks are performed\n- **Research-to-execution flywheel**: A five-step procedure where research feeds specs, specs feed features, and documentation accumulates\n- **Parallel research, atomic implementation**: Multiple agents analyze simultaneously, but features implement one at a time with quality gates\n- **You own decisions and steer, agents own execution**: Expect 40-60% completion requiring engineer polish and refinement\n\n## What's Next\n\nAtomic is actively evolving.\n\nIf you're building with AI coding agents and want workflows that compound, Atomic is the next step.\n\nPRs welcome.\n\n---\n\n## References\n\n\u003Cdiv id=\"ref-1\">[1] AI Coding Infrastructure - Previous post on foundational agent setup - https://alexlavaee.me/blog/ai-coding-infrastructure/\u003C/div>\n\u003Cdiv id=\"ref-2\">[2] Atomic Repository - https://github.com/flora131/agent-instructions\u003C/div>\n\u003Cdiv id=\"ref-3\">[3] Superpowers Framework - https://github.com/obra/superpowers\u003C/div>\n\u003Cdiv id=\"ref-4\">[4] Anthropic Skills - https://github.com/anthropics/skills\u003C/div>","src/content/blog/atomic-workflow.mdx","85c83d1e821d1dec","atomic-workflow.mdx"]